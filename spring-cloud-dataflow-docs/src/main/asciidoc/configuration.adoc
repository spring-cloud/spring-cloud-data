[[configuration]]
= Configuration

[partintro]
--
This section covers how to configure Spring Cloud Data Flow Server's features, such as which relational database to use and security.
It also covers how to configure Spring Cloud Data Flow's shell features.
--

[[enable-disable-specific-features]]
== Feature Toggles

Sprig Cloud Data Flow Server offers specific set of features that can be enabled/disabled when launching. These features include all the lifecycle operations and REST endpoints (server and client implementations, including the shell and the UI) for:

* Streams
* Tasks
* Analytics
* Skipper

One can enable and disable these features by setting the following boolean properties when launching the Data Flow server:

* `spring.cloud.dataflow.features.streams-enabled`
* `spring.cloud.dataflow.features.tasks-enabled`
* `spring.cloud.dataflow.features.analytics-enabled`
* `spring.cloud.dataflow.features.skipper-enabled`

By default, stream, tasks, and analytics are enabled and Skipper is disabled by default.

NOTE: Since the analytics feature is enabled by default, the Data Flow Server expects to have a valid Redis store available to server as the analytic repository, because Spring Cloud Data Flow provides a default implementation of analytics based on Redis.
This also means that the Data Flow Server's `health` depends on the redis store availability as well.
If you do not want Data Flow's endpoints to read analytics data written to Redis, then disable the analytics feature by setting the property mentioned above.

The REST `/about` endpoint provides information on the features that have been enabled and disabled.

[[configuration-rdbms]]
== Database

A relational database is used to store stream and task definitions as well as the state of executed tasks.
Spring Cloud Data Flow provides schemas for H2, HSQLDB, MySQL, Oracle, Postgresql, DB2, and SqlServer. The schema is automatically created when the server starts.


By default, Spring Cloud Data Flow offers an embedded instance of the H2 database.
The H2 database is good for development purposes but is not recommended for production use.

The JDBC drivers for *MySQL* (through the MariaDB driver), *HSQLDB*, *PostgreSQL*, and embedded *H2* are available without additional configuration.
If you are using any other database, then you need to put the corresponding JDBC driver jar on the classpath of the server.

The database properties can be passed as environment variables or command-line arguments to the Data Flow Server.

The following example shows how to define a database connection with environment variables:

[source,bash]
----
export spring_datasource_url=jdbc:postgresql://localhost:5432/mydb
export spring_datasource_username=myuser
export spring_datasource_password=mypass
export spring_datasource_driver-class-name="org.postgresql.Driver"
----

The following example shows how to define a MySQL database connection with command Line arguments

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:mysql:<db-info> \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver &
----

The following example shows how to define a PostgreSQL database connection with command line arguments:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:postgresql:<db-info> \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.postgresql.Driver &
----

The following example shows how to define a HSQLDB database connection with command line arguments:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:hsqldb:<db-info> \
    --spring.datasource.username=SA \
    --spring.datasource.driver-class-name=org.hsqldb.jdbc.JDBCDriver &
----

NOTE: If you wish to use an external H2 database instance instead of the one
embedded with Spring Cloud Data Flow, set the
`spring.dataflow.embedded.database.enabled` property to false.  If
`spring.dataflow.embedded.database.enabled` is set to false or a database
other than h2 is specified as the datasource, the embedded database does not
start.

=== Disabling database schema creation and migration strategies

You can control whether or not Data Flow bootstraps your database on startup. On most production environments chances are you will not have enough privileges to do so.
If that's the case you can disable it by setting the property `spring.cloud.dataflow.rdbms.initialize.enable` to `false`.
The scripts that the server uses to bootstrap the database can be found under `spring-cloud-dataflow-core/src/main/resources/` folder.

For new installations run the corresponding database script located under `/schemas` and `/migrations.1.x.x`, for upgrades from version `1.2.0` you only need to run the `/migrations.1.x.x` scripts.


=== Adding a Custom JDBC Driver
To add a custom driver for the database (for example, Oracle), you should rebuild the Data Flow Server and add the dependency to the Maven `pom.xml` file.
Since there is a Spring Cloud Data Flow Server for each target platform, you need to modify the appropriate maven `pom.xml` for each platform.  There are tags in each GitHub repository for each server version.

To add a custom JDBC driver dependency for the local server implementation:

. Select the tag that corresponds to the version of the server you want to rebuild and clone the github repository.
. Edit the spring-cloud-dataflow-server-local/pom.xml and, in the `dependencies` section, add the dependency for the database driver required.  In the following example , an Oracle driver has been chosen:

[source, xml]
----
<dependencies>
...
  <dependency>
    <groupId>com.oracle.jdbc</groupId>
    <artifactId>ojdbc8</artifactId>
    <version>12.2.0.1</version>
  </dependency>
...
</dependencies>
----

[start=3]
. Build the application as described in <<appendix-building.adoc#building, Building Spring Cloud Data Flow>>

You can also provide default values when rebuilding the server by adding the necessary properties to the dataflow-server.yml file,
as shown in the following example for PostgreSQL:

[source]
----
spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypass
    driver-class-name:org.postgresql.Driver
----

[[configuration-deployer]]
== Local Deployer
You can use the following configuration properties of the Data Flow Local server's deployer to customize how applications are deployed:

[source,properties,indent=0,subs="verbatim,attributes,macros"]
----
spring.cloud.deployer.local.workingDirectoriesRoot=java.io.tmpdir # Directory in which all created processes will run and create log files.

spring.cloud.deployer.local.deleteFilesOnExit=true # Whether to delete created files and directories on JVM exit.

spring.cloud.deployer.local.envVarsToInherit=TMP,LANG,LANGUAGE,"LC_.*. # Array of regular expression patterns for environment variables that are passed to launched applications.

spring.cloud.deployer.local.javaCmd=java # Command to run java.

spring.cloud.deployer.local.shutdownTimeout=30 # Max number of seconds to wait for app shutdown.

spring.cloud.deployer.local.javaOpts= # The Java options to pass to the JVM

spring.cloud.deployer.local.freeDiskSpacePercentage=5 # The target percentage of free disk space to always aim for when cleaning downloaded resources (typically via the local maven repository). Specify as an integer greater than zero and less than 100. Default is 5.
----

[NOTE]
====
Data Flow Local server itself overrides
`spring.cloud.deployer.local.freeDiskSpacePercentage` to `0` from its
default value.
====

When deploying the application, you can also set deployer properties prefixed with `deployer.<name of application>`. For example, to set Java options for the time application in the `ticktock` stream, use the following stream deployment properties.
[source,bash]
----
dataflow:> stream create --name ticktock --definition "time --server.port=9000 | log"
dataflow:> stream deploy --name ticktock --properties "deployer.time.local.javaOpts=-Xmx2048m -Dtest=foo"
----

As a convenience, you can set the `deployer.memory` property to set the Java option `-Xmx`, as shown in the following example:

[source,bash]
----
dataflow:> stream deploy --name ticktock --properties "deployer.time.memory=2048m"
----

At deployment time, if you specify an `-Xmx` option in the `deployer.<app>.local.javaOpts` property in addition to a value of the `deployer.<app>.local.memory` option, the value in the `javaOpts` property has precedence.  Also, the `javaOpts` property set when deploying the application has precedence over the Data Flow Server's `spring.cloud.deployer.local.javaOpts` property.

[[configuration-maven]]
== Maven
If you want to override specific maven configuration properties (remote repositories, proxies, and others) or run the Data Flow Server behind a proxy,
you need to specify those properties as command line arguments when starting the Data Flow Server, as shown in the following example:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar --maven.localRepository=mylocal
--maven.remote-repositories.repo1.url=https://repo1
--maven.remote-repositories.repo1.auth.username=user1
--maven.remote-repositories.repo1.auth.password=pass1
--maven.remote-repositories.repo1.snapshot-policy.update-policy=daily
--maven.remote-repositories.repo1.snapshot-policy.checksum-policy=warn
--maven.remote-repositories.repo1.release-policy.update-policy=never
--maven.remote-repositories.repo1.release-policy.checksum-policy=fail
--maven.remote-repositories.repo2.url=https://repo2
--maven.remote-repositories.repo2.policy.update-policy=always
--maven.remote-repositories.repo2.policy.checksum-policy=fail
--maven.proxy.host=proxy1
--maven.proxy.port=9010 --maven.proxy.auth.username=proxyuser1
--maven.proxy.auth.password=proxypass1
----

By default, the protocol is set to `http`. You can omit the auth properties if the proxy does not need a username and password. Also, the maven `localRepository` is set to `${user.home}/.m2/repository/` by default.
As shown in the preceding example, the remote repositories can be specified along with their authentication (if needed). If the remote repositories are behind a proxy, then the proxy properties can be specified as shown in the preceding example.

The repository policies can be specified for each remote repository configuration as shown in the preceding example.
The key `policy` is applicable to both `snapshot` and the `release` repository policies.

You can refer to https://github.com/eclipse/aether-core/blob/4cf5f7a406b516a45d8bf15e7dfe3fb3849cb87b/aether-api/src/main/java/org/eclipse/aether/repository/RepositoryPolicy.java#L16[Repository Policies] for the list of
supported repository policies.

As these are Spring Boot `@ConfigurationProperties`, you can also specify them as environment variables, such as `MAVEN_REMOTE_REPOSITORIES_REPO1_URL`.
Another common option is to set the properties by setting the `SPRING_APPLICATION_JSON` environment variable.
The following example shows how the JSON is structured:

[source,bash,subs=attributes]
----
$ SPRING_APPLICATION_JSON='{ "maven": { "local-repository": null,
"remote-repositories": { "repo1": { "url": "https://repo1", "auth": { "username": "repo1user", "password": "repo1pass" } }, "repo2": { "url": "https://repo2" } },
"proxy": { "host": "proxyhost", "port": 9018, "auth": { "username": "proxyuser", "password": "proxypass" } } } }' java -jar spring-cloud-dataflow-server-local-{project-version}.jar
----

[[configuration-skipper]]
== Skipper
To use features such as Stream update and rollback, the Data Flow Server delegates to the Skipper server to manage the Stream's lifecycle.  Set the configuration property `spring.cloud.skipper.client.serverUri` to the location of Skipper, e.g.
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar --spring.cloud.skipper.client.serverUri=http://192.51.100.1:7577/api --spring.cloud.dataflow.features.skipper-enabled=true
----

[[configuration-security]]
== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection.
You can secure your REST endpoints as well as the Data Flow Dashboard by enabling HTTPS
and requiring clients to authenticate using either:

* https://oauth.net/2/[OAuth 2.0]
* Traditional Authentication (including Basic Authentication)

The following image shows the authentication options for Spring Cloud Data Flow Server:

.Authentication Options
image::{dataflow-asciidoc}/images/dataflow-authentication-options.png[Authentication Options, scaledwidth="80%"]

When choosing traditional authentication, the Spring Cloud Data Flow server
is the main authentication point, using Spring Security under the covers. When
selecting this option, users then need to further define their preferred authentication
mechanism by selecting the desired authentication backing store, which can be one of the
following options:

* <<configuration-security-single-user-authentication,Single User Authentication>>
* <<configuration-security-ldap-authentication,LDAP Authentication>>
* <<configuration-security-file-based-authentication,File-based authentication>>

When choosing between traditional authentication or OAuth2, keep in mind that
both options are mutually exclusive. Please refer to the sections below for
a more detailed discussion.

[NOTE]
====
By default, the REST endpoints (administration, management, and health) as well as the Dashboard UI do not require authenticated access.
====

[[configuration-security-enabling-https]]
=== Enabling HTTPS

By default, the dashboard, management, and health endpoints use HTTP as a transport.
You can switch to HTTPS by adding a certificate to your configuration in
`application.yml`, as shown in the following example:

[source,yaml]
----
server:
  port: 8443                                         # <1>
  ssl:
    key-alias: yourKeyAlias                          # <2>
    key-store: path/to/keystore                      # <3>
    key-store-password: yourKeyStorePassword         # <4>
    key-password: yourKeyPassword                    # <5>
    trust-store: path/to/trust-store                 # <6>
    trust-store-password: yourTrustStorePassword     # <7>
----

<1> As the default port is `9393`, you may choose to change the port to a more common HTTPs-typical port.
<2> The alias (or name) under which the key is stored in the keystore.
<3> The path to the keystore file. Classpath resources may also be specified, by using the classpath prefix - for example: `classpath:path/to/keystore`.
<4> The password of the keystore.
<5> The password of the key.
<6> The path to the truststore file. Classpath resources may also be specified, by using the classpath prefix - for example: `classpath:path/to/trust-store`
<7> The password of the trust store.

NOTE: If HTTPS is enabled, it completely replaces HTTP as the protocol over
which the REST endpoints and the Data Flow Dashboard interact. Plain HTTP requests
will fail. Therefore, make sure that you configure your Shell accordingly.

[[configuration-security-self-signed-certificates]]
==== Using Self-Signed Certificates

For testing purposes or during development, it might be convenient to create self-signed certificates.
To get started, execute the following command to create a certificate:

[source,bash]
----
$ keytool -genkey -alias dataflow -keyalg RSA -keystore dataflow.keystore \
          -validity 3650 -storetype JKS \
          -dname "CN=localhost, OU=Spring, O=Pivotal, L=Kailua-Kona, ST=HI, C=US"  # <1>
          -keypass dataflow -storepass dataflow
----

<1> `CN` is the important parameter here. It should match the domain you are trying to access - for example, `localhost`.

Then add the following lines to your `application.yml` file:

[source,yaml]
----
server:
  port: 8443
  ssl:
    enabled: true
    key-alias: dataflow
    key-store: "/your/path/to/dataflow.keystore"
    key-store-type: jks
    key-store-password: dataflow
    key-password: dataflow
----

This is all that is needed for the Data Flow Server. Once you start the server,
you should be able to access it at `https://localhost:8443/`.
As this is a self-signed certificate, you should hit a warning in your browser, which
you need to ignore.

[[configuration-security-self-signed-certificates-shell]]
==== Self-Signed Certificates and the Shell

By default, self-signed certificates are an issue for the shell, and additional steps
are necessary to make the shell work with self-signed certificates. Two options
are available:

* Add the self-signed certificate to the JVM truststore.
* Skip certificate validation.

===== Adding the Self-signed Certificate to the JVM Truststore

In order to use the JVM truststore option, we need to
export the previously created certificate from the keystore, as follows:

[source,bash]
----
$ keytool -export -alias dataflow -keystore dataflow.keystore -file dataflow_cert -storepass dataflow
----

Next, we need to create a truststore which the shell can use, as follows:

[source,bash]
----
$ keytool -importcert -keystore dataflow.truststore -alias dataflow -storepass dataflow -file dataflow_cert -noprompt
----

Now, you are ready to launch the Data Flow Shell by using the following JVM arguments:

[source,bash,subs=attributes]
----
$ java -Djavax.net.ssl.trustStorePassword=dataflow \
       -Djavax.net.ssl.trustStore=/path/to/dataflow.truststore \
       -Djavax.net.ssl.trustStoreType=jks \
       -jar spring-cloud-dataflow-shell-{project-version}.jar
----

[TIP]
====
In case you run into trouble establishing a connection over SSL, you can enable additional
logging by using and setting the `javax.net.debug` JVM argument to `ssl`.
====

Do not forget to target the Data Flow Server with the following:

[source,bash]
----
dataflow:> dataflow config server https://localhost:8443/
----

===== Skipping Certificate Validation

Alternatively, you can also bypass the certification validation by providing the
optional command-line parameter `--dataflow.skip-ssl-validation=true`.

If you set this command-line parameter, the shell accepts any (self-signed) SSL
certificate.

[WARNING]
====
If possible, you should avoid using this option. Disabling the trust manager
defeats the purpose of SSL and makes you vulnerable to man-in-the-middle attacks.
====

[[configuration-security-basic-authentication]]
=== Traditional Authentication

When using traditional authentication, Spring Cloud Data Flow is the sole
authentication provider. In that case, Data Flow REST API users would use
https://en.wikipedia.org/wiki/Basic_access_authentication[Basic Authentication]
to access the endpoints.

When using that option, users have a choice of three backing stores for authentication
details:

* *Single User Authentication* by setting Spring Boot properties
* *File-based Authentication* for multiple users by using a Yaml file
* *Ldap Authentication*

[[configuration-security-single-user-authentication]]
==== Single User Authentication

This is the simplest option and mimics the behavior of the default Spring Boot user
experience. It can be enabled by setting environment variables or by adding the following to `application.yml`:

[source,yaml]
----
security:
  basic:
    enabled: true                                                     # <1>
    realm: Spring Cloud Data Flow                                     # <2>
----

<1> Enables basic authentication. Must be set to true for security to be enabled.
<2> (Optional) The realm for Basic authentication. Defaults to `Spring` if not explicitly set.

NOTE: Current versions of Chrome do not display the realm. Please see the following
https://bugs.chromium.org/p/chromium/issues/detail?id=544244[Chromium issue ticket] for more information.

In this use case, the underlying Spring Boot auto-creates a user called `user`
with an auto-generated password which is printed out to the console upon startup.

With this setup, the generated user has all main roles assigned, as follows:

* VIEW
* CREATE
* MANAGE

The following image shows the default Spring Boot user credentials as they appear in the console.

.Default Spring Boot user credentials
image::{dataflow-asciidoc}/images/dataflow-security-default-user.png[Default Spring Boot user credentials , scaledwidth="100%"]

You can customize the user by setting the following properties:

```
security.user.name=user # Default user name.
security.user.password= # Password for the default user name. A random password is logged on startup by default.
security.user.role=VIEW,CREATE,MANAGE # Granted roles for the default user name.
```

NOTE: Please be aware of inherent issues of Basic Authentication and logging out: The credentials are cached by the browser and simply browsing back to application pages logs you back in.

Of course, you can also pass in credentials by setting system properties, environment
variables, or command-line arguments, as this is standard Spring Boot behavior. For
instance, in the following example, command-line arguments are used to specify the
user credentials:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar\
    --security.basic.enabled=true \
    --security.user.name=test \
    --security.user.password=pass \
    --security.user.role=VIEW
----

If you need to define more than one file-based user account, please take a look
at <<configuration-security-file-based-authentication,File-based authentication>>.

[[configuration-security-file-based-authentication]]
==== File-based Authentication

By default, Spring Boot lets you specify only one single user. Spring Cloud
Data Flow also supports the listing of more than one user in a configuration file. Each user must be assigned a password and one or more roles.
The following example shows the creation of additional users:

[source,yaml]
----
security:
  basic:
    enabled: true
    realm: Spring Cloud Data Flow
spring:
  cloud:
    dataflow:
      security:
        authentication:
          file:
            enabled: true                                                 # <1>
            users:                                                        # <2>
              bob: bobspassword, ROLE_MANAGE                              # <3>
              alice: alicepwd, ROLE_VIEW, ROLE_CREATE
----

<1> Enables file based authentication.
<2> This is a yaml map of username to password.
<3> Each map `value` is made of a corresponding password and role(s), comma separated.

[[configuration-security-ldap-authentication]]
==== LDAP Authentication

Spring Cloud Data Flow also supports authentication against an LDAP (Lightweight Directory Access Protocol) server, providing support for the following modes:

* Direct bind
* Search and bind

When the LDAP authentication option is activated, the default single user mode is
turned off.

In direct bind mode, a pattern is defined for the user’s distinguished name (DN),
using a placeholder for the username. The authentication process derives the
distinguished name of the user by replacing the placeholder and using it to authenticate
a user against the LDAP server, along with the supplied password. You can set up
LDAP direct bind as follows:

[source,yaml]
----
security:
  basic:
    enabled: true
    realm: Spring Cloud Data Flow
spring:
  cloud:
    dataflow:
      security:
        authentication:
          ldap:
            enabled: true                                                 # <1>
            url: ldap://ldap.example.com:3309                             # <2>
            userDnPattern: uid={0},ou=people,dc=example,dc=com            # <3>
----

<1> Enables LDAP authentication
<2> The URL for the LDAP server
<3> The distinguished name (DN) pattern for authenticating against the server

The search and bind mode involves connecting to an LDAP server, either anonymously
or with a fixed account, searching for the distinguished name of the authenticating
user based on its username, and then using the resulting value and the supplied password
for binding to the LDAP server. This option is configured as follows:

[source,yaml]
----
security:
  basic:
    enabled: true
    realm: Spring Cloud Data Flow
spring:
  cloud:
    dataflow:
      security:
        authentication:
          ldap:
            enabled: true                                                 # <1>
            url: ldap://localhost:10389                                   # <2>
            managerDn: uid=admin,ou=system                                # <3>
            managerPassword: secret                                       # <4>
            userSearchBase: ou=otherpeople,dc=example,dc=com              # <5>
            userSearchFilter: uid={0}                                     # <6>
----

<1> Enables LDAP integration
<2> The URL of the LDAP server
<3> A DN to authenticate to the LDAP server, if anonymous searches are not supported (optional, required together with next option)
<4> A password to authenticate to the LDAP server, if anonymous searches are not supported (optional, required together with previous option)
<5> The base for searching the DN of the authenticating user (serves to restrict the scope of the search)
<6> The search filter for the DN of the authenticating user

TIP: For more information, please also see the
http://docs.spring.io/spring-security/site/docs/current/reference/html/ldap.html[LDAP Authentication]
chapter of the Spring Security reference guide.

===== LDAP Role Mapping

By default, the role name retrieved from Ldap needs to match the name of the
role in Spring Cloud Data Flow. However, it is also possible to explicitly
provide a mapping between LDAP roles and Spring Cloud Data Flow roles.
[source,yaml]
----
security:
  basic:
    enabled: true
    realm: Spring Cloud Data Flow
spring:
  cloud:
    dataflow:
      security:
        authentication:
          ldap:
            enabled: true
            url: ldap://localhost:10389
            managerDn: uid=admin,ou=system
            managerPassword: secret
            userSearchBase: ou=otherpeople,dc=example,dc=com
            userSearchFilter: uid={0}
            roleMappings:                                                 # <1>
              ROLE_MANAGE: foo-manage                                     # <2>
              ROLE_VIEW: bar-view
              ROLE_CREATE: foo-manage
----

<1> Enables explicit role mapping support
<2> When role mapping support is enabled, you must provide a mapping for
all 3 Spring Cloud Data Flow roles *ROLE_MANAGE*, *ROLE_VIEW*, *ROLE_CREATE*.

===== LDAP Transport Security

When connecting to an LDAP server, you typically (in the LDAP world) have two options
to establish a connection to an LDAP server securely:

* LDAP over SSL (LDAPs)
* Start Transport Layer Security (Start TLS is defined in https://www.ietf.org/rfc/rfc2830.txt[RFC2830])

As of Spring Cloud Data Flow 1.1.0, only LDAPs is supported out-of-the-box. When using
official certificates, no special configuration is necessary to connect
to an LDAP Server over LDAPs. You need only change the url format to **ldaps** - for example: `ldaps://localhost:636`.

In the case of self-signed certificates, the setup for your Spring Cloud Data Flow
server becomes slightly more complex. The setup is very similar to
<<configuration-security-self-signed-certificates>> (please read first), and
Spring Cloud Data Flow needs to reference a trustStore in order to work with
your self-signed certificates.

IMPORTANT: While useful during development and testing, never use
self-signed certificates in production!

Ultimately, you have to provide a set of system properties to reference
the trustStore and its credentials when starting the server, as follows:

[source,bash,subs=attributes]
----
$ java -Djavax.net.ssl.trustStorePassword=dataflow \
       -Djavax.net.ssl.trustStore=/path/to/dataflow.truststore \
       -Djavax.net.ssl.trustStoreType=jks \
       -jar spring-cloud-starter-dataflow-server-local-{project-version}.jar
----

As mentioned earlier, another option to connect to an LDAP server securely is over Start TLS.
In the LDAP world, LDAPs is technically even considered deprecated in favor of Start TLS. However,
this option is currently not supported out-of-the-box by Spring Cloud Data Flow.

Please follow the following https://github.com/spring-cloud/spring-cloud-dataflow/issues/963[issue
tracker ticket] to track its implementation. You may also want to look at the
Spring LDAP reference documentation chapter on
http://docs.spring.io/spring-ldap/docs/current/reference/#custom-dircontext-authentication-processing[Custom DirContext Authentication Processing] for further details.

[[configuration-security-authentication-via-shell]]
==== Shell Authentication

When using traditional authentication with the Data Flow Shell, you typically provide
a username and password by using command-line arguments, as shown in the following example:

[source,bash, subs=attributes+]
----
$ java -jar target/spring-cloud-dataflow-shell-{project-version}.jar  \
  --dataflow.username=myuser                                          \   # <1>
  --dataflow.password=mysecret                                            # <2>
----

<1> If authentication is enabled, the username must be provided.
<2> If the password is not provided, the shell prompts for it.

Alternatively, you can target a Data Flow Server also from within the shell, as follows:

[source,bash]
----
server-unknown:>dataflow config server
  --uri  http://localhost:9393                                        \   # <1>
  --username myuser                                                   \   # <2>
  --password mysecret                                                 \   # <3>
  --skip-ssl-validation  true                                         \   # <4>
----

<1> Optional, defaults to http://localhost:9393.
<2> Mandatory if security is enabled.
<3> If security is enabled, and the password is not provided, the user is prompted for it.
<4> Optional, ignores certificate errors (when using self-signed certificates). Use cautiously!

The following image shows a typical shell command to connect to and authenticate a Data
Flow Server:

.Target and Authenticate with the Data Flow Server from within the Shell
image::{dataflow-asciidoc}/images/dataflow-security-shell-target.png[Target and Authenticate with the Data Flow Server from within the Shell, scaledwidth="100%"]

[[customizing-authorization]]
==== Customizing Authorization

The preceding content deals with authentication - that is, how to assess the identity of the user. Irrespective of the option chosen, you can also customize *authorization* - that is,
who can do what.

The default scheme uses three roles to protect the xref:api-guide[REST endpoints]
that Spring Cloud Data Flow exposes:

* *ROLE_VIEW* for anything that relates to retrieving state
* *ROLE_CREATE* for anything that involves creating, deleting, or mutating the state of the system
* *ROLE_MANAGE* for boot management endpoints

All of those defaults are specified in `dataflow-server-defaults.yml`, which is
part of the Spring Cloud Data Flow Core Module. Nonetheless, you can
override those, if desired - for example, in `application.yml`. The configuration takes
the form of a YAML list (as some rules may have precedence over others). Consequently,
you need to copy and paste the whole list and tailor it to your needs (as there is no way to merge lists).

NOTE: Always refer to your version of `application.yml`, as the following snippet may be outdated.

The default rules are as follows:

[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          enabled: true
          rules:
            # Metrics

            - GET    /metrics/streams                => hasRole('ROLE_VIEW')

            # About

            - GET    /about                          => hasRole('ROLE_VIEW')

            # Metrics

            - GET    /metrics/**                     => hasRole('ROLE_VIEW')
            - DELETE /metrics/**                     => hasRole('ROLE_CREATE')

            # Boot Endpoints

            - GET    /management/**                  => hasRole('ROLE_MANAGE')

            # Apps

            - GET    /apps                           => hasRole('ROLE_VIEW')
            - GET    /apps/**                        => hasRole('ROLE_VIEW')
            - DELETE /apps/**                        => hasRole('ROLE_CREATE')
            - POST   /apps                           => hasRole('ROLE_CREATE')
            - POST   /apps/**                        => hasRole('ROLE_CREATE')

            # Completions

            - GET /completions/**                    => hasRole('ROLE_CREATE')

            # Job Executions & Batch Job Execution Steps && Job Step Execution Progress

            - GET    /jobs/executions                => hasRole('ROLE_VIEW')
            - PUT    /jobs/executions/**             => hasRole('ROLE_CREATE')
            - GET    /jobs/executions/**             => hasRole('ROLE_VIEW')

            # Batch Job Instances

            - GET    /jobs/instances                 => hasRole('ROLE_VIEW')
            - GET    /jobs/instances/*               => hasRole('ROLE_VIEW')

            # Running Applications

            - GET    /runtime/apps                   => hasRole('ROLE_VIEW')
            - GET    /runtime/apps/**                => hasRole('ROLE_VIEW')

            # Stream Definitions

            - GET    /streams/definitions            => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*          => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*/related  => hasRole('ROLE_VIEW')
            - POST   /streams/definitions            => hasRole('ROLE_CREATE')
            - DELETE /streams/definitions/*          => hasRole('ROLE_CREATE')
            - DELETE /streams/definitions            => hasRole('ROLE_CREATE')

            # Stream Deployments

            - DELETE /streams/deployments/*          => hasRole('ROLE_CREATE')
            - DELETE /streams/deployments            => hasRole('ROLE_CREATE')
            - POST   /streams/deployments/*          => hasRole('ROLE_CREATE')

            # Task Definitions

            - POST   /tasks/definitions              => hasRole('ROLE_CREATE')
            - DELETE /tasks/definitions/*            => hasRole('ROLE_CREATE')
            - GET    /tasks/definitions              => hasRole('ROLE_VIEW')
            - GET    /tasks/definitions/*            => hasRole('ROLE_VIEW')

            # Task Executions

            - GET    /tasks/executions               => hasRole('ROLE_VIEW')
            - GET    /tasks/executions/*             => hasRole('ROLE_VIEW')
            - POST   /tasks/executions               => hasRole('ROLE_CREATE')
            - DELETE /tasks/executions/*             => hasRole('ROLE_CREATE')
----

The format of each line is the following:
----
HTTP_METHOD URL_PATTERN '=>' SECURITY_ATTRIBUTE
----

where

* HTTP_METHOD is one http method, capital case
* URL_PATTERN is an Ant style URL pattern
* SECURITY_ATTRIBUTE is a SpEL expression.  See http://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#el-access[Expression-Based Access Control].
* Each of those separated by one or several blank characters (spaces, tabs, and so on)

Be mindful that the above is indeed a YAML list, not a map (thus the use of '-' dashes at the start of each line) that lives under the `spring.cloud.dataflow.security.authorization.rules` key.

[TIP]
====
In case you are solely interested in authentication but not authorization
(for instance every user shall have have access to all endpoints), then you can also
set `spring.cloud.dataflow.security.authorization.enabled=false`.
====

If you use basic security configuration by setting security properties, then it is important to set the roles for the users,
as shown in the following example:

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-{project-version}.jar \
    --security.basic.enabled=true \
    --security.user.name=test \
    --security.user.password=pass \
    --security.user.role=VIEW
----

[[authorization-shell-and-dashboard]]
==== Authorization - Shell and Dashboard Behavior

When authorization is enabled, the dashboard and the shell are role-aware,
meaning that, depending on the assigned roles, not all functionality may be visible.

For instance, shell commands for which the user does not have the necessary roles
are marked as unavailable.

[IMPORTANT]
====
Currently, the shell's `help` command lists commands that are unavailable.
Please track the following issue: https://github.com/spring-projects/spring-shell/issues/115
====

Similarly, for the dashboard, the UI does not show pages or page elements for
which the user is not authorized.

[[ldap-authorization-and-roles]]
==== Authorization with LDAP

When configuring LDAP for authentication, you can also specify the `group-role-attribute`
in conjunction with `group-search-base` and `group-search-filter`.

The group role attribute contains the name of the role. If not specified, the
`ROLE_MANAGE` role is populated by default.

For further information, please refer to http://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#loading-authorities[Configuring an LDAP Server] in the Spring Security reference guide.

[[configuration-security-oauth2]]
=== OAuth 2.0

https://oauth.net/2/[OAuth 2.0] lets you integrate Spring Cloud
Data Flow into Single Sign On (SSO) environments. The following OAuth2 Grant Types are used:

* *Authorization Code*: Used for the GUI (browser) integration. Visitors are redirected to your OAuth Service for authentication
* *Password*: Used by the shell (and the REST integration), so visitors can log in with username and password
* *Client Credentials*: Retrieve an access token directly from your OAuth provider and pass it to the Data Flow server by using the Authorization HTTP header

The REST endpoints can be accessed in two ways:

* *Basic authentication*, which uses the Password Grant Type under the covers to authenticate with your OAuth2 service
* *Access token*, which uses the Client Credentials Grant Type under the covers

NOTE: When authentication is set up, it is strongly recommended to enable HTTPS
as well, especially in production environments.

You can turn on OAuth2 authentication by adding the following to `application.yml` or by setting
environment variables:

[source,yaml]
----
security:
  oauth2:
    client:
      client-id: myclient                                             # <1>
      client-secret: mysecret
      access-token-uri: http://127.0.0.1:9999/oauth/token
      user-authorization-uri: http://127.0.0.1:9999/oauth/authorize
    resource:
      user-info-uri: http://127.0.0.1:9999/me
----

<1> Providing the Client ID in the OAuth Configuration Section activates OAuth2 security

You can verify that basic authentication is working properly by using curl, as follows:

[source,bash]
----
$ curl -u myusername:mypassword http://localhost:9393/ -H 'Accept: application/json'
----

As a result, you should see a list of available REST endpoints.

IMPORTANT: Please be aware that when accessing the Root URL with a web browser and
enabled security, you are redirected to the Dashboard UI. In order to see the
list of REST endpoints, specify the `application/json`. Also be sure to add the
Accept header using tools such as Postman (Chrome) or RESTClient (Firefox).

Besides Basic Authentication, you can also provide an Access Token in order to
access the REST Api. In order to make that happen, you would retrieve an
OAuth2 Access Token from your OAuth2 provider first and then pass that Access Token to
the REST Api using the *Authorization* Http header:

```
$ curl -H "Authorization: Bearer <ACCESS_TOKEN>" http://localhost:9393/ -H 'Accept: application/json'
```

[[configuration-security-oauth2-authorization]]
==== OAuth REST Endpoint Authorization

The OAuth2 authentication option uses the same authorization rules as used by the
<<configuration-security-basic-authentication, Traditional Authentication>> option.

[TIP]
====
The authorization rules are defined in `dataflow-server-defaults.yml` (part of
the Spring Cloud Data Flow Core module). Please see the chapter on
<<customizing-authorization, customizing authorization>> for more details.
====

Because the determination of security roles is environment-specific,
Spring Cloud Data Flow, by default, assigns all roles to authenticated OAuth2
users by using the `DefaultDataflowAuthoritiesExtractor` class.

You can customize that behavior by providing your own Spring bean definition that
extends Spring Security OAuth's `AuthoritiesExtractor` interface. In that case,
the custom bean definition takes precedence over the default one provided by
Spring Cloud Data Flow.

[[configuration-security-oauth2-shell]]
==== OAuth Authentication using the Spring Cloud Data Flow Shell

When using the Shell, the credentials can either be provided via username and password
or by specifying a _credentials-provider_ command.

If your OAuth2 provider supports the _Password_ Grant Type you can start the
_Data Flow Shell_ with:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar \
  --dataflow.uri=http://localhost:9393 \
  --dataflow.username=my_username --dataflow.password=my_password
----

NOTE: Keep in mind that when authentication for Spring Cloud Data Flow is enabled,
the underlying OAuth2 provider *must* support the _Password_ OAuth2 Grant Type
if you want to use the Shell via username/password authentication.

From within the Data Flow Shell you can also provide credentials by using the following command:

[source,bash]
----
dataflow config server --uri http://localhost:9393 --username my_username --password my_password
----

Once successfully targeted, you should see the following output:

[source,bash]
----
dataflow:>dataflow config info
dataflow config info

╔═══════════╤═══════════════════════════════════════╗
║Credentials│[username='my_username, password=****']║
╠═══════════╪═══════════════════════════════════════╣
║Result     │                                       ║
║Target     │http://localhost:9393                  ║
╚═══════════╧═══════════════════════════════════════╝
----

Alternatively, you can specify the _credentials-provider_ command in order to
pass-in a bearer token directly, instead of providing a username and password.
This works from within the shell or by providing the
`--dataflow.credentials-provider-command` command-line argument when starting the Shell.

[IMPORTANT]
====
When using the _credentials-provider_ command, please be aware that your
specified command *must* return a _Bearer token_ (Access Token prefixed with _Bearer_).
For instance, in Unix environments the following simplistic command can be used:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar \
  --dataflow.uri=http://localhost:9393 \
  --dataflow.credentials-provider-command="echo Bearer 123456789"
----

====

==== OAuth2 Authentication Examples

This section offers the following authentication examples:

* <<oauth2-examples-local>>
* <<oauth2-examples-github>>

[[oauth2-examples-local]]
===== Local OAuth2 Server

With http://projects.spring.io/spring-security-oauth/[Spring Security OAuth], you
can easily create your own OAuth2 Server with the following simple annotations:

* `@EnableResourceServer`
* `@EnableAuthorizationServer`

A working example application can be found at:

https://github.com/ghillert/oauth-test-server/[https://github.com/ghillert/oauth-test-server/]

Clone the project and configure Spring Cloud
Data Flow with the respective Client ID and Client Secret. Then build and start the project.

[[oauth2-examples-github]]
===== Authentication with GitHub

If you like to use an existing OAuth2 provider, here is an example for GitHub.
First, you need to register a new application under your GitHub account at:

https://github.com/settings/developers[https://github.com/settings/developers]

When running a default version of Spring Cloud Data Flow locally, your GitHub configuration
should look like the following image:

.Register an OAuth Application for GitHub
image::{dataflow-asciidoc}/images/dataflow-security-github.png[Register an OAuth Application for GitHub , scaledwidth="100%"]

NOTE: For the authorization callback URL, enter Spring Cloud Data Flow's Login URL - for example, `http://localhost:9393/login`.

Configure Spring Cloud Data Flow with the GitHub relevant Client ID and Secret, as follows:

[source,yaml]
----
security:
  oauth2:
    client:
      client-id: your-github-client-id
      client-secret: your-github-client-secret
      access-token-uri: https://github.com/login/oauth/access_token
      user-authorization-uri: https://github.com/login/oauth/authorize
    resource:
      user-info-uri: https://api.github.com/user
----

IMPORTANT: GitHub does not support the OAuth2 password grant type. As a result, you cannot use the Spring Cloud Data Flow Shell in conjunction with GitHub.

=== Securing the Spring Boot Management Endpoints

When enabling security, please also make sure that the http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-monitoring.html[Spring Boot HTTP Management Endpoints]
are secured as well. You can enable security for the management endpoints by adding the following to `application.yml`:

[source,yaml]
----
management:
  contextPath: /management
  security:
    enabled: true
----

IMPORTANT: If you do not explicitly enable security for the management endpoints,
you may end up having unsecured REST endpoints, despite `security.basic.enabled`
being set to `true`.

[[configuration-monitoring-management]]
== Monitoring and Management
The Spring Cloud Data Flow server is a Spring Boot 1.5 application that includes the  https://docs.spring.io/spring-boot/docs/1.5.12.RELEASE/reference/htmlsingle/#production-ready[Actuator
library], which adds several production ready features to help you monitor and manage your application.

The Actuator library adds HTTP endpoints under the context path `/management` which is a discovery page for available managerment endpoints.
For example, there is a `health` endpoint that shows application health information and an `env` that lists properties from Spring's `ConfigurableEnvironment`.
By default, only the health and application info endpoints are accessible.  The other endpoints are considered to be sensitive
and need to be  https://docs.spring.io/spring-boot/docs/1.5.12.RELEASE/reference/htmlsingle/#production-ready-customizing-endpoints[enabled explicitly via configuration].
If you enable sensitive endpoints, you should also <<configuration-security,secure the Data Flow server's endpoints>> so that information is not inadvertently exposed to unauthenticated users.
The local Data Flow server has security disabled by default, so all actuator endpoints are available.

The Data Flow server requires a relational database, and, if the feature toggled for analytics is enabled, a Redis server is also required.
The Data Flow server autoconfigures the https://github.com/spring-projects/spring-boot/blob/v1.4.1.RELEASE/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/health/DataSourceHealthIndicator.java[DataSourceHealthIndicator] and https://github.com/spring-projects/spring-boot/blob/v1.4.1.RELEASE/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/health/RedisHealthIndicator.java[RedisHealthIndicator] if needed.
The health of these two services is incorporated to the overall health status of the server through the `health` endpoint.


[[configuration-monitoring-deployed-applications]]
=== Monitoring Deployed Stream Applications

The stream applications deployed by Spring Cloud Data Flow can be based on Spring Boot 1.5 or Spring Boot 2.0.
 Both versions contains several features for monitoring your application in production.
However, Spring Boot 1.x and 2.x as well as Spring Cloud Stream 1.x and 2.x differ in how monitoring is implemented.
Since Spring Cloud Data Flow supports deploying 1.x and 2.x applications, we will cover both cases individually.

What is common across 1.x and 2.x applications is that Spring Cloud Stream apps can be configured to publish metrics to a messaging middleware destination.
The https://github.com/spring-cloud/spring-cloud-dataflow-metrics-collector[Spring Cloud Data Flow Metrics Collector] subscribes to this destination and aggregates metrics into a stream based view.
The Metrics Collector 2.0 server supports collecting metrics from streams that contain only Boot 1.x or 2.x apps as well as streams that contain a mixture of Boot versions.
The Data Flow UI queries the Metrics collector over HTTP to display messages rates next to each deployed application.

The following image shows the architecture when using Spring Cloud Stream's metrics publisher, the Metrics Collector, and the Data Flow server:

.Spring Cloud Data Flow Metrics Architecture
image::{dataflow-asciidoc}/images/dataflow-metrics-arch.png[Spring Cloud Data Flow Metrics Architecture , scaledwidth="100%"]


==== Using the Metrics Collector

There are two versions of the Metrics Collector, a 1.0 version based on Spring Boot 1.0 that understands how to aggregate metrics from Spring Boot 1.x applications and a 2.0 version based on Spring Boot 2.0 that understands how to aggregate metrics from Spring Boot 1.0 and 2.0.
There is a Metrics Collector server for Rabbit and Kafka.
You can find more information on downloading and running the Metrics Collector on its https://github.com/spring-cloud/spring-cloud-dataflow-metrics-collector[project page].


The Data Flow server property: `spring.cloud.dataflow.metrics.collector.uri` references the URI the Metrics Collector.
For example, if you run the Metrics Collector locally on port `8080`, this is how you start local Data Flow server to reference the Metrics Collector.

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar --spring.cloud.dataflow.metrics.collector.uri=http://localhost:8080
----

The Metrics Collector can be secured with 'basic' authentication that requires a username and password.
To set the username and password, use the properties `spring.cloud.dataflow.metrics.collector.username` and `spring.cloud.dataflow.metrics.collector.password` when starting the Data Flow server.

The metrics for each application are published when the property `spring.cloud.stream.bindings.applicationMetrics.destination` is set.
Using a destination name of `metrics` is a good choice as the Metrics Collector subscribes to that name by default.

Since it is quite common to want all stream applications deployed by Data Flow to emit metrics, setting the property:
[source,bash]
----
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.bindings.applicationMetrics.destination=metrics
----
on the Data Flow server will let you configure support for metrics publication in one central location.

The next most common way to configure the metrics destination is to use deployment properties.
The following example shows the `ticktock` stream that uses the App Starters `time` and `log` applications:

[source,bash]
----
app register --name time --type source --uri maven://org.springframework.cloud.stream.app:time-source-rabbit:1.2.0.RELEASE

app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.2.0.RELEASE

stream create --name foostream --definition "time | log"

stream deploy --name foostream --properties "app.*.spring.cloud.stream.bindings.applicationMetrics.destination=metrics"
----

The Metrics Collector exposes aggregated metrics under the HTTP endpoint `/collector/metrics` in JSON format.
The Data Flow server accesses this endpoint in two distinct ways.
The first is by exposing a `/metrics/streams` HTTP endpoint that acts as a proxy to the Metrics Collector endpoint.
This is accessed by the UI when overlaying message rates on the Flow diagrams for each stream.
It is also accessed to enrich the Data Flow `/runtime/apps` endpoint that is exposed in the UI in the `Runtime` tab and in the shell
through the `runtime apps` command with message rates.

The following image shows the message rates as they appear in the Streams tab of the UI:

.Stream Message Rates
image::{dataflow-asciidoc}/images/dataflow-metrics-message-rates.png[Stream Message Rates, scaledwidth="100%"]

When deploying applications, Data Flow sets the `spring.cloud.stream.metrics.properties` property, as shown
in the following example:

[source,bash]
----
spring.cloud.stream.metrics.properties=spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
----
The values of these keys are used as the tags to perform aggregation.
In the case of 2.x applications, these key-values map directly onto tags in the micrometer library.
The property `spring.cloud.application.guid` can be used to track back to the specific application instance that generated the metric.
The `guid` value is platform-dependent.

Data Flow also sets the application property that controls which metric values are exported.
For 1.x applications, the property is `spring.metrics.export.triggers.application.includes` and the default value is is shown below:

[source,bash]
----
spring.metrics.export.triggers.application.includes=integration**
----

For 2.x applications, the property is `spring.cloud.stream.metrics.meter-filter` and it does not have a default value, so all metrics are exported.

Note that the Data Flow UI only displays only instantaneous input and output channel message rates.
Data Flow does not provide its own implementation to store and visualize historical metrics data, instead we integrate with existing monitoring system.
For Boot 1.x, there is support for sending metrics to the application log and DataDog.
For Boot 2.x, metrics is backed by the https://micrometer.io/[Micrometer library] that provides a wide range of https://docs.spring.io/spring-boot/docs/2.0.0.RELEASE/reference/htmlsingle/#production-ready-metrics[monitoring systems].


==== Spring Boot 2.x

We have developed an sample application that show how to modify the `time` and `log` applications and export metrics to InfluxDB using the micrometer library.  A Grafana front end is also provided.
The project can be found in the https://github.com/spring-cloud/spring-cloud-dataflow-samples[Spring Cloud Data Flow Samples Repository].

==== Spring Boot 1.x

Each deployed application contains https://docs.spring.io/spring-boot/docs/1.5.12.RELEASE/reference/htmlsingle/#production-ready-endpoints[web endpoints] for monitoring and interacting with Stream and Task applications.

In particular, the `/metrics` https://docs.spring.io/spring-boot/docs/1.5.12.RELEASE/reference/htmlsingle/#production-ready-metrics[endpoint] contains counters and gauges for HTTP requests, System Metrics such as JVM stats, DataSource Metrics, and Message Channel Metrics.
Spring Boot lets you http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-public-metrics[add your own metrics] to the `/metrics` endpoint either by registering an implementation of the `PublicMetrics` interface or through its integration with http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-dropwizard-metrics[Dropwizard].

The Spring Boot interfaces, `MetricWriter` and `Exporter`, are used to send the metrics data to a place where they can be displayed and analyzed.
There are implementations in Spring Boot to export metrics to Redis, Open TSDB, Statsd, and JMX.

A few additional Spring projects provide support for sending metrics data to external systems:

* https://github.com/spring-cloud/spring-cloud-dataflow-metrics[Spring Cloud Data Flow Metrics] provides `LogMetricWriter` that writes to the log.
* https://github.com/spring-cloud/spring-cloud-dataflow-metrics-datadog[Spring Cloud Data Flow Metrics Datadog Metrics] provides `DatadogMetricWriter` that writes to https://www.datadoghq.com/[Datadog].

To make use of this functionality, you need build the Stream application with the additional pom dependency of the MetricWriter implementation you want to use.
To customize the "`out of the box`" Stream applications, use the http://start-scs.cfapps.io/[Spring Cloud Stream Initializr] to generate a project and then modify the pom.
The documentation on the Data Flow Metrics project pages provides the additional information you need to get started.

== About Configuration
The Spring Cloud Data Flow About Restful API result contains a display name,
version, and, if specified, a URL for each of the major dependencies that
comprise Spring Cloud Data Flow.  The result (if enabled) also contains the
sha1 and or sha256 checksum values for the shell dependency. The information
that is returned for each of the dependencies is configurable by setting the following
properties:

* spring.cloud.dataflow.version-info.spring-cloud-dataflow-core.name: the
name to be used for the core.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-core.version:
the version to be used for the core.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-dashboard.name: the
name to be used for the dashboard.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-dashboard.version:
the version to be used for the dashboard.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-implementation.name: the
name to be used for the implementation.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-implementation.version:
the version to be used for the implementation.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.name: the
name to be used for the shell.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.version:
the version to be used for the shell.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.url:
the URL to be used for downloading the shell dependency.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha1: the sha1
checksum value that is returned with the shell dependency info.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha256:
the sha256 checksum value that is returned with the shell dependency info.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha1-url:
if the `spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha1`
is not specified, SCDF uses the contents of the file specified at this URL for the checksum.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha256-url:
if the `spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha256`
is not specified, SCDF uses the contents of the file specified at this URL for the checksum.

=== Enabling Shell Checksum values
By default, checksum values are not displayed for the shell dependency. If
you need this feature enabled, set the
`spring.cloud.dataflow.version-info.dependency-fetch.enabled` property to true.

=== Reserved Values for URLs
There are reserved values (surrounded by curly braces) that you can insert into
the URL that will make sure that the links are up to date:

* repository: if using a build-snapshot, milestone, or release candidate of
Data Flow, the repository refers to the repo-spring-io repository. Otherwise, it
refers to Maven Central.
* version: Inserts the version of the jar/pom.

For example,
`https://myrepository/org/springframework/cloud/spring-cloud-dataflow-shell/{version}/spring-cloud-dataflow-shell-{version}.jar`
produces
`https://myrepository/org/springframework/cloud/spring-cloud-dataflow-shell/1.2.3.RELEASE/spring-cloud-dataflow-shell-1.2.3.RELEASE.jar`
if you were using the 1.2.3.RELEASE version of the Spring Cloud Data Flow Shell
