[[spring-cloud-dataflow-streams]]
= Streams

[partintro]
--
This section goes into more detail about how you can create Streams, which are collections of
https://cloud.spring.io/spring-cloud-stream/[Spring Cloud Stream] applications. It covers topics such as
creating and deploying Streams.

If you are just starting out with Spring Cloud Data Flow, you should probably read the
<<getting-started.adoc#getting-started, Getting Started>> guide before diving into
this section.
--

[[spring-cloud-dataflow-stream-intro]]
== Introduction
A Stream is are a collection of long-lived https://cloud.spring.io/spring-cloud-stream/[Spring Cloud Stream] applications that communicate with each other over messaging middleware.
A text-based DSL defines the configuration and data flow between the applications.  While many applications are provided for you to implement common use-cases, you typically create a custom Spring Cloud Stream application to implement custom business logic.

The general lifecycle of a Stream is:

. Register applications.
. Create a Stream Definition.
. Deploy the Stream.
. Undeploy or Destroy the Stream.
. Upgrade or Rollback applications in the Stream.

For deploying streams, the Data Flow Server has to be configured to delegate the deployment to a new server in the Spring Cloud ecosystem named https://cloud.spring.io/spring-cloud-skipper/[Skipper].

Furthermore you can configure Skipper to deploy applications to one or more Cloud Foundry orgs and spaces, one or more namespaces on a Kubernetes cluster, or to the local machine.
When deploying a stream in Data Flow, you can specify which platform to use at deployment time.
Skipper also provides Data Flow with the ability to perform updates to deployed streams.
There are many ways the applications in a stream can be updated, but one of the most common examples is to upgrade a processor application with new custom business logic while leaving the existing source and sink applications alone.


[[spring-cloud-dataflow-stream-intro-dsl]]
=== Stream Pipeline DSL

A stream is defined by using a unix-inspired link:https://en.wikipedia.org/wiki/Pipeline_(Unix)[Pipeline syntax].
The syntax uses vertical bars, also known as "`pipes`" to connect multiple commands.
The command `ls -l | grep key | less` in Unix takes the output of the `ls -l` process and pipes it to the input of the `grep key` process.
The output of `grep` in turn is sent to the input of the `less` process.
Each `|` symbol connects the standard output of the command on the left to the standard input of the command on the right.
Data flows through the pipeline from left to right.

In Data Flow, the Unix command is replaced by a https://cloud.spring.io/spring-cloud-stream/[Spring Cloud Stream] application and each pipe symbol represents connecting the input and output of applications over messaging middleware, such as RabbitMQ or Apache Kafka.

Each Spring Cloud Stream application is registered under a simple name.
The registration process specifies where the application can be obtained (for example, in a Maven Repository or a Docker registry).  You can find out more information on how to register Spring Cloud Stream applications in this <<spring-cloud-dataflow-register-stream-apps,section>>.
In Data Flow, we classify the Spring Cloud Stream applications as Sources, Processors, or Sinks.

As a simple example, consider the collection of data from an HTTP Source writing to a File Sink.
Using the DSL, the stream description is:

`http | file`

A stream that involves some processing would be expressed as:

`http | filter | transform | file`

Stream definitions can be created by using the shell's `stream create` command, as shown in the following example:

`dataflow:> stream create --name httpIngest --definition "http | file"`

The Stream DSL is passed in to the `--definition` command option.

The deployment of stream definitions is done through the shell's `stream deploy` command.

`dataflow:> stream deploy --name ticktock`

The xref:getting-started#getting-started[Getting Started] section shows you how to start the server and how to start and use the Spring Cloud Data Flow shell.

Note that the shell calls the Data Flow Servers' REST API. For more information on making HTTP requests directly to the server, consult the <<api-guide, REST API Guide>>.

[[spring-cloud-dataflow-stream-app-dsl]]
=== Stream Application DSL

The Stream Pipeline DSL described in the previous section automatically sets the input and output binding properties of each Spring Cloud Stream application.
This can be done because there is only one input and/or output destination in a Spring Cloud Stream application that uses the provided binding interface of a `Source`, `Processor`, or `Sink`.
However, a Spring Cloud Stream application can define a custom binding interface such as the one shown below

[source,java]
----
public interface Barista {

    @Input
    SubscribableChannel orders();

    @Output
    MessageChannel hotDrinks();

    @Output
    MessageChannel coldDrinks();
}
----

or as is common when creating a Kafka Streams application,

[source,java]
----
interface KStreamKTableBinding {

    @Input
    KStream<?, ?> inputStream();

    @Input
    KTable<?, ?> inputTable();
}
----

In these cases with multiple input and output bindings, Data Flow cannot make any assumptions about the flow of data from one application to another.
Therefore the developer needs to set the binding properties to 'wire up' the application.
The *Stream Application DSL* uses a `double pipe`, instead of the `pipe symbol`, to indicate that Data Flow should not configure the binding properties of the application.  Think of `||` as meaning 'in parallel'.
For example:

`dataflow:> stream create --definition "orderGeneratorApp || baristaApp || hotDrinkDeliveryApp || coldDrinkDeliveryApp" --name myCafeStream`

NOTE: Breaking Change!  Versions of SCDF Local, Cloud Foundry 1.7.0 to 1.7.2 and SCDF Kubernetes 1.7.0 to 1.7.1 used the `comma` character as the separator between applications.  This caused breaking changes in the traditional Stream DSL.  While not ideal, changing the separator character was felt to be the best solution with the least impact on existing users.

There are four applications in this stream.
 The baristaApp has two output destinations, `hotDrinks` and `coldDrinks` intended to be consumed by the `hotDrinkDeliveryApp` and `coldDrinkDeliveryApp` respectively.
When deploying this stream, you need to set the binding properties so that the `baristaApp` sends hot drink messages to the `hotDrinkDeliveryApp` destination and cold drink messages to the `coldDrinkDeliveryApp` destination.
For example

[source,bash,subs=attributes]
----
app.baristaApp.spring.cloud.stream.bindings.hotDrinks.destination=hotDrinksDest
app.baristaApp.spring.cloud.stream.bindings.coldDrinks.destination=coldDrinksDest
app.hotDrinkDeliveryApp.spring.cloud.stream.bindings.input.destination=hotDrinksDest
app.coldDrinkDeliveryApp.spring.cloud.stream.bindings.input.destination=coldDrinksDest
----

If you want to use consumer groups, you will need to set the Spring Cloud Stream application property `spring.cloud.stream.bindings.<channelName>.producer.requiredGroups` and `spring.cloud.stream.bindings.<channelName>.group` on the producer and consumer applications respectively.

Another common use case for the Stream Application DSL is to deploy a http gateway application that sends a synchronous request/reply message to a Kafka or RabbitMQ application.
In this case both the http gateway application and the Kafka or RabbitMQ application can be a Spring Integration application that does not make use of the Spring Cloud Stream library.

It is also possible to deploy just a single application using the Stream application DSL.


=== Application properties

Each application takes properties to customize its behavior.  As an example, the `http` source module exposes a `port` setting that allows the data ingestion port to be changed from the default value.

`dataflow:> stream create --definition "http --port=8090 | log" --name myhttpstream`

This `port` property is actually the same as the standard Spring Boot `server.port` property.
Data Flow adds the ability to use the shorthand form `port` instead of `server.port`.
One may also specify the longhand version as well, as shown in the following example:

`dataflow:> stream create --definition "http --server.port=8000 | log" --name myhttpstream`

This shorthand behavior is discussed more in the section on <<spring-cloud-dataflow-stream-app-whitelisting>>.
If you have <<spring-cloud-dataflow-stream-app-metadata-artifact, registered application property metadata>> you can use tab completion in the shell after typing `--` to get a list of candidate property names.

The shell provides tab completion for application properties. The shell command `app info --name <appName> --type <appType>` provides additional documentation for all the supported properties.

NOTE: Supported Stream `<appType>` possibilities are: source, processor, and sink.




[[spring-cloud-dataflow-stream-lifecycle]]
== Stream Lifecycle

The lifecycle of a stream, goes through the following stages:

. <<spring-cloud-dataflow-register-stream-apps>>
. <<spring-cloud-dataflow-create-stream>>
. <<spring-cloud-dataflow-deploy-stream>>
. <<spring-cloud-dataflow-destroy-stream>> or <<spring-cloud-dataflow-undeploy-stream>>
. <<spring-cloud-dataflow-streams-upgrading,Upgrade>> or <<spring-cloud-dataflow-streams-rollback,Rollback>> applications in the Stream.

https://cloud.spring.io/spring-cloud-skipper/[Skipper] is a server that you discover Spring Boot applications and manage their lifecycle on multiple Cloud Platforms.

Applications in Skipper are bundled as packages that contain the application's resource location, application properties and deployment properties.
You can think Skipper packages as analogous to packages found in tools such as `apt-get` or `brew`.

When Data Flow deploys a Stream, it will generate and upload a package to Skipper that represents the applications in the Stream.
Subsequent commands to upgrade or rollback the applications within the Stream are passed through to Skipper.
In addition, the Stream definition is reverse engineered from the package and the status of the Stream is also delegated to Skipper.

[[spring-cloud-dataflow-register-stream-apps]]
=== Register a Stream App

Register a versioned stream application using the `app register` command. You must provide a unique name, application type, and a URI that can be resolved to the app artifact.
For the type, specify "source", "processor", or "sink". The version is resolved from the URI. Here are a few examples:
[source,bash]
----
dataflow:>app register --name mysource --type source --uri maven://com.example:mysource:0.0.1
dataflow:>app register --name mysource --type source --uri maven://com.example:mysource:0.0.2
dataflow:>app register --name mysource --type source --uri maven://com.example:mysource:0.0.3

dataflow:>app list --id source:mysource
╔═══╤══════════════════╤═════════╤════╤════╗
║app│      source      │processor│sink│task║
╠═══╪══════════════════╪═════════╪════╪════╣
║   │> mysource-0.0.1 <│         │    │    ║
║   │mysource-0.0.2    │         │    │    ║
║   │mysource-0.0.3    │         │    │    ║
╚═══╧══════════════════╧═════════╧════╧════╝

dataflow:>app register --name myprocessor --type processor --uri file:///Users/example/myprocessor-1.2.3.jar

dataflow:>app register --name mysink --type sink --uri https://example.com/mysink-2.0.1.jar

----

The application URI should conform to one the following schema formats:

* maven schema
[source,bash]
----
maven://<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>
----
* http schema
[source,bash]
----
http://<web-path>/<artifactName>-<version>.jar
----
* file schema
[source,bash]
----
file:///<local-path>/<artifactName>-<version>.jar
----
* docker schema
[source,bash]
----
docker:<docker-image-path>/<imageName>:<version>
----

[NOTE]
The URI `<version>` part is compulsory for the versioned stream applications.
Skipper leverages the multi-versioned stream applications to allow upgrade or rollback of those applications at runtime using the deployment properties.

If you would like to register the snapshot versions of the `http` and `log`
applications built with the RabbitMQ binder, you could do the following:

[source,bash]
----
dataflow:>app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.1.BUILD-SNAPSHOT
dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.2.1.BUILD-SNAPSHOT
----

If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as `<type>.<name>` and the values are the URIs.

For example, if you would like to register the snapshot versions of the `http` and `log`
applications built with the RabbitMQ binder, you could have the following in a properties file (for example, `stream-apps.properties`):

[source,bash]
----
source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.1.BUILD-SNAPSHOT
sink.log=maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.2.1.BUILD-SNAPSHOT
----

Then to import the apps in bulk, use the `app import` command and provide the location of the properties file with the `--uri` switch, as follows:

[source,bash]
----
dataflow:>app import --uri file:///<YOUR_FILE_LOCATION>/stream-apps.properties
----

Registering an application using `--type app` is the same as registering a `source`, `processor` or `sink`.
Applications of the type `app` are only allowed to be used in the Stream Application DSL, which uses a comma instead of the pipe symbol in the DSL, and instructs Data Flow not to configure the Spring Cloud Stream binding properties of the application.
The application that is registered using `--type app` does not have to be a Spring Cloud Stream app, it can be any Spring Boot application.
See the <<spring-cloud-dataflow-stream-app-dsl,Stream Application DSL introduction>> for more information on using this application type.

Multiple versions can be registered for the same applications (e.g. same name and type) but only one can be set as default.
The default version is used for deploying Streams.

The first time an application is registered it will be marked as default. The default application version can be altered with the `app default` command:
[source,bash]
----
dataflow:>app default --id source:mysource --version 0.0.2
dataflow:>app list --id source:mysource
╔═══╤══════════════════╤═════════╤════╤════╗
║app│      source      │processor│sink│task║
╠═══╪══════════════════╪═════════╪════╪════╣
║   │mysource-0.0.1    │         │    │    ║
║   │> mysource-0.0.2 <│         │    │    ║
║   │mysource-0.0.3    │         │    │    ║
╚═══╧══════════════════╧═════════╧════╧════╝
----

The `app list --id <type:name>` command lists all versions for a given stream application.

The `app unregister` command has an optional `--version` parameter to specify the app version to unregister.
[source,bash]
----
dataflow:>app unregister --name mysource --type source --version 0.0.1
dataflow:>app list --id source:mysource
╔═══╤══════════════════╤═════════╤════╤════╗
║app│      source      │processor│sink│task║
╠═══╪══════════════════╪═════════╪════╪════╣
║   │> mysource-0.0.2 <│         │    │    ║
║   │mysource-0.0.3    │         │    │    ║
╚═══╧══════════════════╧═════════╧════╧════╝
----
If a `--version` is not specified, the default version is unregistered.

[NOTE]
====
All applications in a stream should have a default version set for the stream to be deployed.
Otherwise they will be treated as unregistered application during the deployment.
Use the `app default` to set the defaults.
====

[source,bash]
----
app default --id source:mysource --version 0.0.3
dataflow:>app list --id source:mysource
╔═══╤══════════════════╤═════════╤════╤════╗
║app│      source      │processor│sink│task║
╠═══╪══════════════════╪═════════╪════╪════╣
║   │mysource-0.0.2    │         │    │    ║
║   │> mysource-0.0.3 <│         │    │    ║
╚═══╧══════════════════╧═════════╧════╧════╝
----

The `stream deploy` necessitates default app versions to be set.
The `stream update` and `stream rollback` commands though can use all (default and non-default) registered app versions.

[source,bash]
----
dataflow:>stream create foo --definition "mysource | log"
----
This will create stream using the default mysource version (0.0.3). Then we can update the version to 0.0.2 like this:
[source,bash]
----
dataflow:>stream update foo --properties version.mysource=0.0.2
----

[IMPORTANT]
====
Only pre-registered applications can be used to `deploy`, `update` or `rollback` a Stream.
====

An attempt to update the `mysource` to version `0.0.1` (not registered) will fail!

[[supported-apps-and-tasks]]
==== Register Supported Applications and Tasks
For convenience, we have the static files with application-URIs (for both maven and docker) available
for all the out-of-the-box stream and task/batch app-starters. You can point to this file and import
all the application-URIs in bulk. Otherwise, as explained previously, you can register them individually or have your own
custom property file with only the required application-URIs in it. It is recommended, however, to have a "`focused`"
list of desired application-URIs in a custom property file.

===== Spring Cloud Stream App Starters

The following table includes the bit.ly links to the available Stream Application Starters based on Spring Cloud Stream 2.0.x
and Spring Boot 2.0.x:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release

|RabbitMQ + Maven
|https://bit.ly/Darwin-SR3-stream-applications-rabbit-maven
|https://bit.ly/Darwin-BUILD-SNAPSHOT-stream-applications-rabbit-maven

|RabbitMQ + Docker
|https://bit.ly/Darwin-SR3-stream-applications-rabbit-docker
|https://bit.ly/Darwin-BUILD-SNAPSHOT-stream-applications-rabbit-docker

|Apache Kafka + Maven
|https://bit.ly/Darwin-SR3-stream-applications-kafka-maven
|https://bit.ly/Darwin-BUILD-SNAPSHOT-stream-applications-kafka-maven

|Apache Kafka + Docker
|https://bit.ly/Darwin-SR3-stream-applications-kafka-docker
|https://bit.ly/Darwin-BUILD-SNAPSHOT-stream-applications-kafka-docker
|======================

The following table includes the bit.ly links to the available Stream Application Starters based on Spring Cloud Stream 2.1.x
and Spring Boot 2.1.x:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release

|RabbitMQ + Maven
|https://bit.ly/Einstein-SR2-stream-applications-rabbit-maven
|https://bit.ly/Einstein-BUILD-SNAPSHOT-stream-applications-rabbit-maven

|RabbitMQ + Docker
|https://bit.ly/Einstein-SR2-stream-applications-rabbit-docker
|https://bit.ly/Einstein-BUILD-SNAPSHOT-stream-applications-rabbit-docker

|Apache Kafka + Maven
|https://bit.ly/Einstein-SR2-stream-applications-kafka-maven
|https://bit.ly/Einstein-BUILD-SNAPSHOT-stream-applications-kafka-maven

|Apache Kafka + Docker
|https://bit.ly/Einstein-SR2-stream-applications-kafka-docker
|https://bit.ly/Einstein-BUILD-SNAPSHOT-stream-applications-kafka-docker
|======================


NOTE: App Starter actuator endpoints are secured by default. You can disable security by deploying streams with the
property `app.*.spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.security.servlet.SecurityAutoConfiguration`.
On Kubernetes refer to the section <<getting-started-kubernetes-probes, Liveness and readiness probes>> to configure
security for actuator endpoints.

NOTE: Starting with Spring Cloud Stream 2.1 GA release, we now have robust interoperability with Spring Cloud Function
programming model. Building on that, with Einstein release-train, it is now possible to pick a few Stream App
Starters, and compose them into a single application using the functional-style programming model. Check out the
https://spring.io/blog/2019/01/09/composed-function-support-in-spring-cloud-data-flow["Composed Function Support in
Spring Cloud Data Flow"] blog to learn more about the developer and orchestration-experience with an example.

===== Spring Cloud Task App Starters

The following table includes the available Task Application Starters based on Spring Cloud Task 2.0.x and Spring Boot 2.0.x:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release

|Maven
|https://bit.ly/Dearborn-SR1-task-applications-maven
|https://bit.ly/Dearborn-BUILD-SNAPSHOT-task-applications-maven

|Docker
|https://bit.ly/Dearborn-SR1-task-applications-docker
|https://bit.ly/Dearborn-BUILD-SNAPSHOT-task-applications-docker
|======================

The following table includes the available Task Application Starters based on Spring Cloud Task 2.1.x and Spring Boot 2.1.x:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Milestone Release |SNAPSHOT Release

|Maven
|https://bit.ly/Elston-GA-task-applications-maven
|https://bit.ly/Elston-BUILD-SNAPSHOT-task-applications-maven

|Docker
|https://bit.ly/Elston-GA-task-applications-docker
|https://bit.ly/Elston-BUILD-SNAPSHOT-task-applications-docker
|======================

You can find more information about the available task starters in the https://cloud.spring.io/spring-cloud-task-app-starters/[Task App Starters Project Page] and
related reference documentation.  For more information about the available stream starters, look at the https://cloud.spring.io/spring-cloud-stream-app-starters/[Stream App Starters Project Page]
and related reference documentation.

As an example, if you would like to register all out-of-the-box stream applications built with the Kafka binder in bulk, you can use the following command:

[source,bash,subs=attributes]
----
$ dataflow:>app import --uri https://bit.ly/Einstein-SR2-stream-applications-kafka-maven
----

Alternatively you can register all the stream applications with the Rabbit binder, as follows:

[source,bash,subs=attributes]
----
$ dataflow:>app import --uri https://bit.ly/Einstein-SR2-stream-applications-rabbit-maven
----

You can also pass the `--local` option (which is `true` by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify `--local false`.

[WARNING]
====
When using either `app register` or `app import`, if an app is already registered with
the provided name and type and version, it is not overridden by default. If you would like to override the
pre-existing app uri or metadata-uri coordinates, then include the `--force` option.

Note, however, that, once downloaded, applications may be cached locally on the Data Flow server, based on the resource
location. If the resource location does not change (even though the actual resource _bytes_ may be different), then it
is not re-downloaded. When using `maven://` resources on the other hand, using a constant location may still circumvent
caching (if using `-SNAPSHOT` versions).

Moreover, if a stream is already deployed and using some version of a registered app, then (forcibly) re-registering a
different app has no effect until the stream is deployed again.
====

[NOTE]
In some cases, the Resource is resolved on the server side. In others, the
URI is passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.

[[spring-cloud-dataflow-stream-app-whitelisting]]
==== Whitelisting application properties

Stream and Task applications are Spring Boot applications that are aware of many <<spring-cloud-dataflow-global-properties>>, such as `server.port` but also families of properties such as those with the prefix `spring.jmx` and `logging`.  When creating your own application, you should whitelist properties so that the shell and the UI can display them first as primary properties when presenting options through TAB completion or in drop-down boxes.

To whitelist application properties, create a file named `spring-configuration-metadata-whitelist.properties` in the `META-INF` resource directory. There are two property keys that can be used inside this file. The first key is named `configuration-properties.classes`. The value is a comma separated list of fully qualified `@ConfigurationProperty` class names. The second key is `configuration-properties.names`, whose value is a comma-separated list of property names. This can contain the full name of the property, such as `server.port`, or a partial name to whitelist a category of property names, such as `spring.jmx`.

The link:https://github.com/spring-cloud-stream-app-starters[Spring Cloud Stream application starters] are a good place to look for examples of usage. The following example comes from the file sink's `spring-configuration-metadata-whitelist.properties` file:

```
configuration-properties.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties
```

If we also want to add `server.port` to be white listed, it would become the following line:

```
configuration-properties.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties
configuration-properties.names=server.port
```

[IMPORTANT]
====
Make sure to add 'spring-boot-configuration-processor' as an optional dependency to generate configuration metadata file for the properties.

[source,xml]
----
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-configuration-processor</artifactId>
    <optional>true</optional>
</dependency>
----
====


[[spring-cloud-dataflow-stream-app-metadata-artifact]]
==== Creating and Using a Dedicated Metadata Artifact
You can go a step further in the process of describing the main properties that your stream or task app supports by
creating a metadata companion artifact. This jar file contains only the Spring boot JSON file about
configuration properties metadata and the whitelisting file described in the previous section.

The following example shows the contents of such an artifact, for the canonical `log` sink:

[source, bash]
----
$ jar tvf log-sink-rabbit-1.2.1.BUILD-SNAPSHOT-metadata.jar
373848 META-INF/spring-configuration-metadata.json
   174 META-INF/spring-configuration-metadata-whitelist.properties
----

Note that the `spring-configuration-metadata.json` file is quite large. This is because it contains the concatenation of _all_ the properties that
are available at runtime to the `log` sink (some of them come from `spring-boot-actuator.jar`, some of them come from
`spring-boot-autoconfigure.jar`, some more from `spring-cloud-starter-stream-sink-log.jar`, and so on). Data Flow
always relies on all those properties, even when a companion artifact is not available, but here all have been merged
into a single file.

To help with that (you do not want to try to craft this giant JSON file by hand), you can use the
following plugin in your build:

[source, xml]
----
<plugin>
 	<groupId>org.springframework.cloud</groupId>
 	<artifactId>spring-cloud-app-starter-metadata-maven-plugin</artifactId>
 	<executions>
 		<execution>
 			<id>aggregate-metadata</id>
 			<phase>compile</phase>
 			<goals>
 				<goal>aggregate-metadata</goal>
 			</goals>
 		</execution>
 	</executions>
 </plugin>
----

NOTE: This plugin comes in addition to the `spring-boot-configuration-processor` that creates the individual JSON files.
Be sure to configure both.

The benefits of a companion artifact include:

* Being much lighter. (The companion artifact is usually a few kilobytes, as opposed to megabytes for the actual app.) Consequently, they are quicker to download,
allowing quicker feedback when using, for example, `app info` or the Dashboard UI.
* As a consequence of being lighter, they can be used in resource constrained environments (such as PaaS) when metadata is
the only piece of information needed.
* For environments that do not deal with Spring Boot uber jars directly (for example, Docker-based runtimes such as
Kubernetes or Cloud Foundry), this is the only way to provide metadata about the properties supported by the app.

Remember, though, that this is entirely optional when dealing with uber jars. The uber jar itself also includes the
metadata in it already.

==== Using the Companion Artifact
Once you have a companion artifact at hand, you need to make the system aware of it so that it can be used.

When registering a single app with `app register`, you can use the optional `--metadata-uri` option in the shell, as follows:

[source,bash,subs=attributes]
----
dataflow:>app register --name log --type sink
    --uri maven://org.springframework.cloud.stream.app:log-sink:2.1.0.RELEASE
    --metadata-uri maven://org.springframework.cloud.stream.app:log-sink:jar:metadata:2.1.0.RELEASE
----

When registering several files by using the `app import` command, the file should contain a `<type>.<name>.metadata` line
in addition to each `<type>.<name>` line. Strictly speaking, doing so is optional (if some apps have it but some others do not, it works), but it is best practice.

The following example shows a Dockerized app, where the metadata artifact is being hosted in a Maven repository (retrieving
it through `http://` or `file://` would be equally possible).

[source, properties]
----
...
source.http=docker:springcloudstream/http-source-rabbit:latest
source.http.metadata=maven://org.springframework.cloud.stream.app:http-source-rabbit:jar:metadata:2.1.0.RELEASE
...
----

[[custom-applications]]
==== Creating Custom Applications

While there are out-of-the-box source, processor, sink applications available, you can extend these applications or write a custom link:https://github.com/spring-cloud/spring-cloud-stream[Spring Cloud Stream] application.

The process of creating Spring Cloud Stream applications with https://start.spring.io/[Spring Initializr] is detailed in the Spring Cloud Stream {spring-cloud-stream-docs}#_getting_started[documentation].
It is possible to include multiple binders to an application.
If doing so, see the instructions in <<passing_producer_consumer_properties>> for how to configure them.

For supporting property whitelisting, Spring Cloud Stream applications running in Spring Cloud Data Flow may include the Spring Boot `configuration-processor` as an optional dependency, as shown in the following example:

[source,xml]
----
<dependencies>
  <!-- other dependencies -->
  <dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-configuration-processor</artifactId>
    <optional>true</optional>
  </dependency>
</dependencies>

----

[NOTE]
====
Make sure that the `spring-boot-maven-plugin` is included in the POM.
The plugin is necessary for creating the executable jar that is registered with Spring Cloud Data Flow.
Spring Initialzr includes the plugin in the generated POM.
====

Once a custom application has been created, it can be registered as described in <<spring-cloud-dataflow-register-stream-apps>>.


[[spring-cloud-dataflow-create-stream]]
=== Creating a Stream

The Spring Cloud Data Flow Server exposes a full RESTful API for managing the lifecycle of stream definitions, but the easiest way to use is it is through the Spring Cloud Data Flow shell. Start the shell as described in the xref:getting-started#getting-started[Getting Started] section.

New streams are created with the help of stream definitions. The definitions are built from a simple DSL. For example, consider what happens if we execute the following shell command:

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock
----

This defines a stream named `ticktock` that is based off the DSL expression `time | log`. The DSL uses the "pipe" symbol (`|`), to connect a source to a sink.

The `stream info` command shows useful information about the stream, as shown (with its output) in the following example:

[source,bash]
----
dataflow:>stream info ticktock
╔═══════════╤═════════════════╤══════════╗
║Stream Name│Stream Definition│  Status  ║
╠═══════════╪═════════════════╪══════════╣
║ticktock   │time | log       │undeployed║
╚═══════════╧═════════════════╧══════════╝
----

==== Application Properties

Application properties are the properties associated with each application in the stream. When the application is deployed, the application properties are applied to the application through
command line arguments or environment variables, depending on the underlying deployment implementation.

The following stream can have application properties defined at the time of stream creation:

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock
----

The shell command `app info --name <appName> --type <appType>` displays the white-listed application properties for the application.
For more info on the property white listing, refer to <<spring-cloud-dataflow-stream-app-whitelisting>>

The following listing shows the white_listed properties for the `time` app:

[source,bash,options="nowrap"]
----
dataflow:> app info --name time --type source
╔══════════════════════════════╤══════════════════════════════╤══════════════════════════════╤══════════════════════════════╗
║         Option Name          │         Description          │           Default            │             Type             ║
╠══════════════════════════════╪══════════════════════════════╪══════════════════════════════╪══════════════════════════════╣
║trigger.time-unit             │The TimeUnit to apply to delay│<none>                        │java.util.concurrent.TimeUnit ║
║                              │values.                       │                              │                              ║
║trigger.fixed-delay           │Fixed delay for periodic      │1                             │java.lang.Integer             ║
║                              │triggers.                     │                              │                              ║
║trigger.cron                  │Cron expression value for the │<none>                        │java.lang.String              ║
║                              │Cron Trigger.                 │                              │                              ║
║trigger.initial-delay         │Initial delay for periodic    │0                             │java.lang.Integer             ║
║                              │triggers.                     │                              │                              ║
║trigger.max-messages          │Maximum messages per poll, -1 │1                             │java.lang.Long                ║
║                              │means infinity.               │                              │                              ║
║trigger.date-format           │Format for the date value.    │<none>                        │java.lang.String              ║
╚══════════════════════════════╧══════════════════════════════╧══════════════════════════════╧══════════════════════════════╝
----

The following listing shows the white-listed properties for the `log` app:

[source,bash,options="nowrap"]
----
dataflow:> app info --name log --type sink
╔══════════════════════════════╤══════════════════════════════╤══════════════════════════════╤══════════════════════════════╗
║         Option Name          │         Description          │           Default            │             Type             ║
╠══════════════════════════════╪══════════════════════════════╪══════════════════════════════╪══════════════════════════════╣
║log.name                      │The name of the logger to use.│<none>                        │java.lang.String              ║
║log.level                     │The level at which to log     │<none>                        │org.springframework.integratio║
║                              │messages.                     │                              │n.handler.LoggingHandler$Level║
║log.expression                │A SpEL expression (against the│payload                       │java.lang.String              ║
║                              │incoming message) to evaluate │                              │                              ║
║                              │as the logged message.        │                              │                              ║
╚══════════════════════════════╧══════════════════════════════╧══════════════════════════════╧══════════════════════════════╝
----

The application properties for the `time` and `log` apps can be specified at the time of `stream` creation as follows:

[source,bash]
----
dataflow:> stream create --definition "time --fixed-delay=5 | log --level=WARN" --name ticktock
----

Note that, in the preceding example, the `fixed-delay` and `level` properties defined for the apps `time` and `log` are the "'short-form'" property names provided by the shell completion.
These "'short-form'" property names are applicable only for the white-listed properties. In all other cases, only fully qualified property names should be used.


[[spring-cloud-dataflow-global-properties]]
==== Common Application Properties

In addition to configuration through DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all
the streaming applications that are launched by it.
This can be done by adding properties prefixed with `spring.cloud.dataflow.applicationProperties.stream` when starting
the server.
When doing so, the server passes all the properties, without the prefix, to the instances it launches.

For example, all the launched applications can be configured to use a specific Kafka broker by launching the
Data Flow server with the following options:

```
--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.brokers=192.168.1.100:9092
--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.zkNodes=192.168.1.100:2181
```

Doing so causes the properties `spring.cloud.stream.kafka.binder.brokers` and `spring.cloud.stream.kafka.binder.zkNodes`
to be passed to all the launched applications.

[NOTE]
Properties configured with this mechanism have lower precedence than stream deployment properties.
They are overridden if a property with the same key is specified at stream deployment time (for example,
`app.http.spring.cloud.stream.kafka.binder.brokers` overrides the common property).


[[spring-cloud-dataflow-deploy-stream]]
=== Deploying a Stream

This section describes how to deploy a Stream when the Spring Cloud Data Flow server is responsible for deploying the stream. It covers the deployment and upgrade of Streams leveraging the Skipper service.  The description of how deployment properties applies to both approaches of Stream deployment.

Give the `ticktock` stream definition:

`dataflow:> stream create --definition "time | log" --name ticktock`

To deploy the stream, use the following shell command:


`dataflow:> stream deploy --name ticktock`

The Data Flow Server delegates to Skipper the resolution and deployment of the `time` and `log` applications.

The `stream info` command shows useful information about the stream, including the deployment properties:
[source,bash,options="nowrap"]
----
dataflow:>stream info --name ticktock
╔═══════════╤═════════════════╤═════════╗
║Stream Name│Stream Definition│  Status ║
╠═══════════╪═════════════════╪═════════╣
║ticktock   │time | log       │deploying║
╚═══════════╧═════════════════╧═════════╝

Stream Deployment properties: {
  "log" : {
    "resource" : "maven://org.springframework.cloud.stream.app:log-sink-rabbit",
    "spring.cloud.deployer.group" : "ticktock",
    "version" : "2.0.1.RELEASE"
  },
  "time" : {
    "resource" : "maven://org.springframework.cloud.stream.app:time-source-rabbit",
    "spring.cloud.deployer.group" : "ticktock",
    "version" : "2.0.1.RELEASE"
  }
}
----

There is an important optional command argument (called `--platformName`) to the `stream deploy` command.
Skipper can be configured to deploy to multiple platforms.
Skipper is pre-configured with a platform named `default`, which deploys applications to the local machine where Skipper is running.
The default value of the command line argument `--platformName` is `default`.
If you commonly deploy to one platform, when installing Skipper, you can override the configuration of the `default` platform.
Otherwise, specify the `platformName` to one of the values returned by the `stream platform-list` command.

In the preceding example, the time source sends the current time as a message each second, and the log sink outputs it by using the logging framework.
You can tail the `stdout` log (which has an `<instance>` suffix). The log files are located within the directory displayed in the Data Flow Server's log output, as shown in the following listing:

[source]
$ tail -f /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481708/ticktock.log/stdout_0.log
2016-06-01 09:45:11.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:11
2016-06-01 09:45:12.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:12
2016-06-01 09:45:13.251  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:13


You can also create and deploy the stream in one step by passing the `--deploy` flag when creating the stream, as follows:

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock --deploy
----

However, it is not very common in real-world use cases to create and deploy the stream in one step.
The reason is that when you use the `stream deploy` command, you can pass in properties that define how to map the applications onto the platform (for example, what is the memory size of the container to use, the number of each application to run, and whether to enable data partitioning features).
Properties can also override application properties that were set when creating the stream.
The next sections cover this feature in detail.

==== Deployment Properties

When deploying a stream, you can specify properties that fall into two groups:

* Properties that control how the apps are deployed to the target platform.
These properties use a `deployer` prefix and are referred to as `deployer` properties.
* Properties that set application properties or override application properties set during stream creation and are referred to as `application` properties.

The syntax for `deployer` properties is `deployer.<app-name>.<short-property-name>=<value>`, and the syntax for `application` properties `app.<app-name>.<property-name>=<value>`. This syntax is used when passing deployment properties through the shell. You may also specify them in a YAML file, which is discussed later in this chapter.

The following table shows the difference in behavior between setting `deployer` and `application` properties when deploying an application.

|===
| | Application Properties | Deployer Properties

| *Example Syntax*
| `app.filter.expression=something`
| `deployer.filter.count=3`

| *What the application "sees"*
| `expression=something` or, if `expression` is one of the whitelisted properties, `<some-prefix>.expression=something`
| Nothing

| *What the deployer "sees"*
| Nothing
| `spring.cloud.deployer.count=3`. The `spring.cloud.deployer` prefix is automatically and always prepended to the property name.

| *Typical usage*
| Passing/Overriding application properties, passing Spring Cloud Stream binder or partitioning properties
| Setting the number of instances, memory, disk, and others

|===


===== Passing Instance Count

If you would like to have multiple instances of an application in the stream, you
can include a deployer property called `count` with the `deploy` command:

[source,bash,subs=attributes]
----
dataflow:> stream deploy --name ticktock --properties "deployer.time.count=3"
----

Note that `count` is the reserved property name used by the underlying deployer. Consequently, if the application also has a custom property named `count`, it is not supported
when specified in 'short-form' form during stream deployment as it could conflict with the instance `count` deployer property. Instead, the `count` as a custom application property can be
specified in its fully qualified form (for example, `app.something.somethingelse.count`) during stream deployment or it can be specified by using the 'short-form' or the fully qualified form during the stream creation,
where it is processed as an app property.

IMPORTANT: See <<spring-cloud-dataflow-stream-dsl-labels>>.


===== Inline Versus File-based Properties

When using the Spring Cloud Data Flow Shell, there are two ways to provide deployment
properties: either *inline* or through a *file reference*. Those two ways are exclusive.

Inline properties use the `--properties` shell option and list properties as a comma separated
list of key=value pairs, as shown in the following example:

[source,bash]
----
stream deploy foo
    --properties "deployer.transform.count=2,app.transform.producer.partitionKeyExpression=payload"
----

File references use the `--propertiesFile` option and point it to a local `.properties`, `.yaml` or `.yml` file
(that is, a file that resides in the filesystem of the machine running the shell). Being read
as a `.properties` file, normal rules apply (ISO 8859-1 encoding, `=`, `<space>` or
`:` delimiter, and others), although we recommend using `=` as a key-value pair delimiter,
for consistency. The following example shows a `stream deploy` command that uses the `--propertiesFile` option:

[source,bash]
----
stream deploy something --propertiesFile myprops.properties
----

Assume that `myprops.properties` contains the following properties:

```
deployer.transform.count=2
app.transform.producer.partitionKeyExpression=payload
```

Both of the properties are passed as deployment properties for the `something` stream.

If you use YAML as the format for the deployment properties, use the `.yaml` or `.yml` file extention when deploying the stream, as shown in the following example:

[source,bash]
----
stream deploy foo --propertiesFile myprops.yaml
----

In that case, the `myprops.yaml` file might contain the following content:

[source]
deployer:
  transform:
    count: 2
app:
  transform:
    producer:
      partitionKeyExpression: payload



===== Passing application properties

The application properties can also be specified when deploying a stream. When specified during deployment, these application properties can either be specified as
 'short-form' property names (applicable for white-listed properties) or as fully qualified property names. The application properties should have the prefix `app.<appName/label>`.

For example, consider the following stream command:

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock
----

The stream in the precedig example can also be deployed with application properties by using the 'short-form' property names, as shown in the following example:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.fixed-delay=5,app.log.level=ERROR"
----

Consider the following example:

[source,bash]
----
stream create ticktock --definition "a: time | b: log"
----

When using the app label, the application properties can be defined as follows:

[source,bash]
----
stream deploy ticktock --properties "app.a.fixed-delay=4,app.b.level=ERROR"
----



[[passing_producer_consumer_properties]]
===== Passing Spring Cloud Stream properties
Spring Cloud Data Flow sets the `required` Spring Cloud Stream properties for the applications inside the stream. Most importantly, the `spring.cloud.stream.bindings.<input/output>.destination` is set internally for the apps to bind.

If you want to override any of the Spring Cloud Stream properties, they can be set with deployment properties.

For example, consider the following stream definition:

[source,bash]
----
dataflow:> stream create --definition "http | transform --expression=payload.getValue('hello').toUpperCase() | log" --name ticktock
----

If there are multiple binders available in the classpath for each of the applications and the binder is chosen for each deployment, then the stream can be deployed with the specific Spring Cloud Stream properties, as follows:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.spring.cloud.stream.bindings.output.binder=kafka,app.transform.spring.cloud.stream.bindings.input.binder=kafka,app.transform.spring.cloud.stream.bindings.output.binder=rabbit,app.log.spring.cloud.stream.bindings.input.binder=rabbit"
----

NOTE: Overriding the destination names is not recommended, because Spring Cloud Data Flow internally takes care of setting this property.

===== Passing Per-binding Producer and Consumer Properties
A Spring Cloud Stream application can have producer and consumer properties set on a `per-binding` basis.
While Spring Cloud Data Flow supports specifying short-hand notation for per-binding producer properties such as `partitionKeyExpression` and `partitionKeyExtractorClass` (as described in <<passing_stream_partition_properties>>), all the supported Spring Cloud Stream producer/consumer properties can be set as Spring Cloud Stream properties for the app directly as well.

The consumer properties can be set for the `inbound` channel name with the prefix `app.[app/label name].spring.cloud.stream.bindings.<channelName>.consumer.`. The producer properties can be set for the `outbound` channel name with the prefix `app.[app/label name].spring.cloud.stream.bindings.<channelName>.producer.`.
Consider the following example:

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock
----

The stream can be deployed with producer and consumer properties, as follows:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.spring.cloud.stream.bindings.output.producer.requiredGroups=myGroup,app.time.spring.cloud.stream.bindings.output.producer.headerMode=raw,app.log.spring.cloud.stream.bindings.input.consumer.concurrency=3,app.log.spring.cloud.stream.bindings.input.consumer.maxAttempts=5"
----

The `binder`-specific producer and consumer properties can also be specified in a similar way, as shown in the following example:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.spring.cloud.stream.rabbit.bindings.output.producer.autoBindDlq=true,app.log.spring.cloud.stream.rabbit.bindings.input.consumer.transacted=true"
----

[[passing_stream_partition_properties]]
===== Passing Stream Partition Properties
A common pattern in stream processing is to partition the data as it is streamed.
This entails deploying multiple instances of a message-consuming app and using
content-based routing so that messages with a given key (as determined at runtime)
are always routed to the same app instance. You can pass the partition properties during
stream deployment to declaratively configure a partitioning strategy to route each
message to a specific consumer instance.

The following list shows variations of deploying partitioned streams:

* *app.[app/label name].producer.partitionKeyExtractorClass*:
  The class name of a `PartitionKeyExtractorStrategy` (default: `null`)

* *app.[app/label name].producer.partitionKeyExpression*:
  A SpEL expression, evaluated against the message, to determine the partition key.
  Only applies if `partitionKeyExtractorClass` is null. If both are null, the app
  is not partitioned (default: `null`)

* *app.[app/label name].producer.partitionSelectorClass*:
  The class name of a `PartitionSelectorStrategy` (default: `null`)

* *app.[app/label name].producer.partitionSelectorExpression*:
  A SpEL expression, evaluated against the partition key, to determine the partition
  index to which the message is routed. The final partition index is the
  return value (an integer) modulo `[nextModule].count`. If both the class and
  expression are null, the underlying binder's default `PartitionSelectorStrategy`
  is applied to the key (default: `null`)

In summary, an app is partitioned if its count is > 1 and the previous app has a
`partitionKeyExtractorClass` or `partitionKeyExpression` (`partitionKeyExtractorClass` takes precedence).
When a partition key is extracted, the partitioned app instance is determined by
invoking the `partitionSelectorClass`, if present, or the `partitionSelectorExpression % partitionCount`.
`partitionCount` is application count, in the case of RabbitMQ, or the underlying
partition count of the topic, in the case of Kafka.

If neither a `partitionSelectorClass` nor a `partitionSelectorExpression` is
present, the result is `key.hashCode() % partitionCount`.

[[passing_content_type_properties]]
===== Passing application content type properties
In a stream definition, you can specify that the input or the output of an application must be converted to a different type.
You can use the `inputType` and `outputType` properties to specify the content type for the incoming data and outgoing data, respectively.

For example, consider the following stream:

[source]
dataflow:>stream create tuple --definition "http | filter --inputType=application/x-spring-tuple
 --expression=payload.hasFieldName('hello') | transform --expression=payload.getValue('hello').toUpperCase()
 | log" --deploy

The `http` app is expected to send the data in JSON and the `filter` app receives the JSON data
and processes it as a Spring Tuple.
In order to do so, we use the `inputType` property on the filter app to convert the data into the expected Spring Tuple format.
The `transform` application processes the Tuple data and sends the processed data to the downstream `log` application.

Consider the following example of sending some data to the `http` application:

[source,bash]
----
dataflow:>http post --data {"hello":"world","something":"somethingelse"} --contentType application/json --target http://localhost:<http-port>
----

At the log application, you see the content as follows:

`INFO 18745 --- [transform.tuple-1] log.sink                                 : WORLD`

Depending on how applications are chained, the content type conversion can be specified either as an `--outputType` in the upstream app or as an `--inputType` in the downstream app.
For instance, in the above stream, instead of specifying the `--inputType` on the 'transform' application to convert, the option `--outputType=application/x-spring-tuple` can also be specified on the 'http' application.

For the complete list of message conversion and message converters, please refer to Spring Cloud Stream {spring-cloud-stream-docs}#contenttypemanagement[documentation].

===== Overriding Application Properties During Stream Deployment

Application properties that are defined during deployment override the same properties defined during the stream creation.

For example, the following stream has application properties defined during stream creation:

[source,bash]
----
dataflow:> stream create --definition "time --fixed-delay=5 | log --level=WARN" --name ticktock
----

To override these application properties, you can specify the new property values during deployment, as follows:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.fixed-delay=4,app.log.level=ERROR"
----

[[spring-cloud-dataflow-destroy-stream]]
=== Destroying a Stream

You can delete a stream by issuing the `stream destroy` command from the shell, as follows:

`dataflow:> stream destroy --name ticktock`

If the stream was deployed, it is undeployed before the stream definition is deleted.

[[spring-cloud-dataflow-undeploy-stream]]
=== Undeploying a Stream

Often you want to stop a stream but retain the name and definition for future use. In that case, you can `undeploy` the stream by name.

[source]
dataflow:> stream undeploy --name ticktock
dataflow:> stream deploy --name ticktock

You can issue the `deploy` command at a later time to restart it.

`dataflow:> stream deploy --name ticktock`

[[spring-cloud-dataflow-validate-stream]]
=== Validating a Stream

Sometimes the one or more of the apps contained within a stream definition contain an invalid URI in its registration.
This can caused by an invalid URI entered at app registration time or the app was removed from the repository from which it was to be drawn.
To verify that all the apps contained in a stream are resolve-able, a user can use the `validate` command.
For example:
[source,bash]
----
dataflow:>stream validate ticktock
╔═══════════╤═════════════════╗
║Stream Name│Stream Definition║
╠═══════════╪═════════════════╣
║ticktock   │time | log       ║
╚═══════════╧═════════════════╝


ticktock is a valid stream.
╔═══════════╤═════════════════╗
║ App Name  │Validation Status║
╠═══════════╪═════════════════╣
║source:time│valid            ║
║sink:log   │valid            ║
╚═══════════╧═════════════════╝
----
In the example above the user validated their ticktock stream.   As we see that both the `source:time` and `sink:log` are valid.
Now let's see what happens if we have a stream definition with a registered app with an invalid URI.
[source,bash]
----
dataflow:>stream validate bad-ticktock
╔════════════╤═════════════════╗
║Stream Name │Stream Definition║
╠════════════╪═════════════════╣
║bad-ticktock│bad-time | log   ║
╚════════════╧═════════════════╝


bad-ticktock is an invalid stream.
╔═══════════════╤═════════════════╗
║   App Name    │Validation Status║
╠═══════════════╪═════════════════╣
║source:bad-time│invalid          ║
║sink:log       │valid            ║
╚═══════════════╧═════════════════╝
----
In this case Spring Cloud Data Flow states that the stream is invalid because source:bad-time has an invalid URI.

[[spring-cloud-dataflow-stream-lifecycle-update]]
=== Updating a Stream
To update the stream, use the command `stream update` which takes as a command argument either `--properties` or `--propertiesFile`.
There is an important new top level prefix available when using Skipper, which is `version`.
If the Stream `http | log` was deployed, and the version of `log` which registered at the time of deployment was `1.1.0.RELEASE`:

[source,bash]
----
dataflow:> stream create --name httptest --definition "http --server.port=9000 | log"
dataflow:> stream deploy --name httptest
dataflow:>stream info httptest
╔══════════════════════════════╤══════════════════════════════╤════════════════════════════╗
║             Name             │             DSL              │          Status            ║
╠══════════════════════════════╪══════════════════════════════╪════════════════════════════╣
║httptest                      │http --server.port=9000 | log │deploying                   ║
╚══════════════════════════════╧══════════════════════════════╧════════════════════════════╝

Stream Deployment properties: {
  "log" : {
    "spring.cloud.deployer.indexed" : "true",
    "spring.cloud.deployer.group" : "httptest",
    "maven://org.springframework.cloud.stream.app:log-sink-rabbit" : "1.1.0.RELEASE"
  },
  "http" : {
    "spring.cloud.deployer.group" : "httptest",
    "maven://org.springframework.cloud.stream.app:http-source-rabbit" : "1.1.0.RELEASE"
  }
}
----

Then the following command will update the Stream to use the `1.2.0.RELEASE` of the log application.
Before updating the stream with the specific version of the app, we need to make sure that the app is registered with that version.
[source,bash]
----
dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.2.0.RELEASE
Successfully registered application 'sink:log'
----

[source,bash]
----
dataflow:>stream update --name httptest --properties version.log=1.2.0.RELEASE
----
[IMPORTANT]
====
Only pre-registered application versions can be used to `deploy`, `update`, or `rollback` a stream.
====

To verify the deployment properties and the updated version, we can use `stream info`, as shown (with its output) in the following example:

[source,bash]
----
dataflow:>stream info httptest
╔══════════════════════════════╤══════════════════════════════╤════════════════════════════╗
║             Name             │             DSL              │          Status            ║
╠══════════════════════════════╪══════════════════════════════╪════════════════════════════╣
║httptest                      │http --server.port=9000 | log │deploying                   ║
╚══════════════════════════════╧══════════════════════════════╧════════════════════════════╝

Stream Deployment properties: {
  "log" : {
    "spring.cloud.deployer.indexed" : "true",
    "spring.cloud.deployer.count" : "1",
    "spring.cloud.deployer.group" : "httptest",
    "maven://org.springframework.cloud.stream.app:log-sink-rabbit" : "1.2.0.RELEASE"
  },
  "http" : {
    "spring.cloud.deployer.group" : "httptest",
    "maven://org.springframework.cloud.stream.app:http-source-rabbit" : "1.1.0.RELEASE"
  }
}

----
[[spring-cloud-dataflow-stream-lifecycle-force-update]]
=== Force update of a Stream
When upgrading a stream, the `--force` option can be used to deploy new instances of currently deployed applications even if no application or deployment properties have changed.
This behavior is needed in the case when configuration information is obtained by the application itself at startup time, for example from Spring Cloud Config Server.
You can specify which applications to force upgrade by using the option `--app-names`.
If you do not specify any application names, all the applications will be force upgraded.
You can specify `--force` and `--app-names` options together with `--properties` or `--propertiesFile` options.

=== Stream versions
Skipper keeps a history of the streams that were deployed.
After updating a Stream, there will be a second version of the stream.
You can query for the history of the versions using the command `stream history --name <name-of-stream>`.

[source,bash]
----
dataflow:>stream history --name httptest
╔═══════╤════════════════════════════╤════════╤════════════╤═══════════════╤════════════════╗
║Version│        Last updated        │ Status │Package Name│Package Version│  Description   ║
╠═══════╪════════════════════════════╪════════╪════════════╪═══════════════╪════════════════╣
║2      │Mon Nov 27 22:41:16 EST 2017│DEPLOYED│httptest    │1.0.0          │Upgrade complete║
║1      │Mon Nov 27 22:40:41 EST 2017│DELETED │httptest    │1.0.0          │Delete complete ║
╚═══════╧════════════════════════════╧════════╧════════════╧═══════════════╧════════════════╝
----

=== Stream Manifests
Skipper keeps a "`manifest`" of the all the applications, their application properties, and their deployment properties after all values have been substituted.
This represents the final state of what was deployed to the platform.
You can view the manifest for any of the versions of a Stream by using the following command:

`stream manifest --name <name-of-stream> --releaseVersion <optional-version>`

If the `--releaseVersion` is not specified, the manifest for the last version is returned.

The following example shows the use of the manifest:

[source,bash]
----
dataflow:>stream manifest --name httptest
----

Using the command results in the following output:

[source,yaml]
----
# Source: log.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: log
spec:
  resource: maven://org.springframework.cloud.stream.app:log-sink-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.dataflow.stream.app.label: log
    spring.cloud.stream.metrics.key: httptest.log.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.input.group: httptest
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.dataflow.stream.name: httptest
    spring.cloud.dataflow.stream.app.type: sink
    spring.cloud.stream.bindings.input.destination: httptest.http
  deploymentProperties:
    spring.cloud.deployer.indexed: true
    spring.cloud.deployer.group: httptest
    spring.cloud.deployer.count: 1

---
# Source: http.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: http
spec:
  resource: maven://org.springframework.cloud.stream.app:http-source-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.dataflow.stream.app.label: http
    spring.cloud.stream.metrics.key: httptest.http.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.output.producer.requiredGroups: httptest
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    server.port: 9000
    spring.cloud.stream.bindings.output.destination: httptest.http
    spring.cloud.dataflow.stream.name: httptest
    spring.cloud.dataflow.stream.app.type: source
  deploymentProperties:
    spring.cloud.deployer.group: httptest
----

The majority of the deployment and application properties were set by Data Flow to enable the applications to talk to each other and to send application metrics with identifying labels.



[[spring-cloud-dataflow-stream-lifecycle-rollback]]
=== Rollback a Stream

You can rollback to a previous version of the stream using the command `stream rollback`.
[source,bash]
----
dataflow:>stream rollback --name httptest
----

The optional `--releaseVersion` command argument adds the version of the stream.
If not specified, the rollback goes to the previous stream version.

=== Application Count

The application count is a dynamic property of the system.
If, due to scaling at runtime, the application to be upgraded has 5 instances running, then 5 instances of the upgraded application are deployed.

=== Skipper's Upgrade Strategy

Skipper has a simple 'red/black' upgrade strategy.  It deploys the new version of the applications, using as many instances as the currently running version, and checks the `/health` endpoint of the application.
If the health of the new application is good, then the previous application is undeployed.
If the health of the new application is bad, then all new applications are undeployed and the upgrade is considered to be not successful.

The upgrade strategy is not a rolling upgrade, so if five applications of the application are running, then in a sunny-day scenario, five of the new applications are also running before the older version is undeployed.

== Stream DSL

This section covers additional features of the Stream DSL not covered in the  <<spring-cloud-dataflow-stream-intro-dsl,Stream DSL introduction>>.

[[spring-cloud-dataflow-stream-dsl-tap]]
=== Tap a Stream

Taps can be created at various producer endpoints in a stream. For a stream such as that defined in the following example, taps can be created at the output of `http`, `step1` and `step2`:

`stream create --definition "http | step1: transform --expression=payload.toUpperCase() | step2: transform --expression=payload+'!' | log" --name mainstream --deploy`

To create a stream that acts as a 'tap' on another stream requires specifying the `source destination name` for the tap stream. The syntax for the source destination name is as follows:

`:<streamName>.<label/appName>`

To create a tap at the output of `http` in the preceding stream, the source destination name is `mainstream.http`
To create a tap at the output of the first transform app in the stream above, the source destination name is `mainstream.step1`

The tap stream DSL resembles the following:

[source]
----
stream create --definition ":mainstream.http > counter" --name tap_at_http --deploy

stream create --definition ":mainstream.step1 > jdbc" --name tap_at_step1_transformer --deploy
----

Note the colon (`:`) prefix before the destination names. The colon lets the parser recognize this as a destination name instead of an app name.

[[spring-cloud-dataflow-stream-dsl-labels]]
=== Using Labels in a Stream
When a stream is made up of multiple apps with the same name, they must be qualified with labels:
`stream create --definition "http | firstLabel: transform --expression=payload.toUpperCase() | secondLabel: transform --expression=payload+'!' | log" --name myStreamWithLabels --deploy`



[[spring-cloud-dataflow-stream-dsl-named-destinations]]
=== Named Destinations

Instead of referencing a source or sink application, you can use a named destination.
A named destination corresponds to a specific destination name in the middleware broker (Rabbit, Kafka, and others).
When using the `|` symbol, applications are connected to each other with messaging middleware destination names created by the Data Flow server.
In keeping with the Unix analogy, one can redirect standard input and output using the less-than (`<`) and greater-than (`>`) characters.
To specify the name of the destination, prefix it with a colon (`:`).
For example, the following stream has the destination name in the `source` position:

`dataflow:>stream create --definition ":myDestination > log" --name ingest_from_broker --deploy`


This stream receives messages from the destination called `myDestination`, located at the broker, and connects it to the `log` app. You can also create additional streams that consume data from the same named destination.

The following stream has the destination name in the `sink` position:

`dataflow:>stream create --definition "http > :myDestination" --name ingest_to_broker --deploy`


It is also possible to connect two different destinations (`source` and `sink` positions) at the broker in a stream, as shown in the following example:

`dataflow:>stream create --definition ":destination1 > :destination2" --name bridge_destinations --deploy`

In the preceding stream, both the destinations (`destination1` and `destination2`) are located in the broker. The messages flow from the source destination to the sink destination over a `bridge` app that connects them.


[[spring-cloud-dataflow-stream-dsl-fanin-fanout]]
=== Fan-in and Fan-out

By using named destinations, you can support fan-in and fan-out use cases.  Fan-in use cases are when multiple sources all send data to the same named destination, as shown in the following example:

[source,bash,subs=attributes]
----
s3 > :data
ftp > :data
http > :data
----

The preceding example directs the data payloads from the Amazon S3, FTP, and HTTP sources to the same named destination called `data`. Then an additional stream created with the following DSL expression would have all the data from those three sources sent to the file sink:

`:data > file`

The fan-out use case is when you determine the destination of a stream based on some information that is only known at runtime.
In this case, the link:{scs-app-starters-docs}/spring-cloud-stream-modules-sinks.html#spring-cloud-stream-modules-router-sink[Router Application] can be used to specify how to direct the incoming message to one of N named destinations.

A nice link:https://youtu.be/l8SgHtP5QCI[Video] showing Fan-in and Fan-out behavior is also available.

[[spring-cloud-dataflow-stream-java-dsl]]
== Stream Java DSL

Instead of using the shell to create and deploy streams, you can use the Java-based DSL provided by the `spring-cloud-dataflow-rest-client` module.
The Java DSL is a convenient wrapper around the `DataFlowTemplate` class that enables creating and deploying streams programmatically.

To get started, you need to add the following dependency to your project, as follows:

[source,xml,subs="attributes+"]
----
<dependency>
	<groupId>org.springframework.cloud</groupId>
	<artifactId>spring-cloud-dataflow-rest-client</artifactId>
	<version>{project-version}</version>
</dependency>
----

NOTE: A complete sample can be found in the https://github.com/spring-cloud/spring-cloud-dataflow-samples[Spring Cloud Data Flow Samples Repository].

=== Overview
The classes at the heart of the Java DSL are `StreamBuilder`, `StreamDefinition`, `Stream`,  `StreamApplication`, and `DataFlowTemplate`.
The entry point is a `builder` method on `Stream` that takes an instance of a `DataFlowTemplate`.
To create an instance of a `DataFlowTemplate`, you need to provide a `URI` location of the Data Flow Server.

Spring Boot auto-configuration for `StreamBuilder` and `DataFlowTemplate` is also available.  The properties in https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-rest-client/src/main/java/org/springframework/cloud/dataflow/rest/client/config/DataFlowClientProperties.java[DataFlowClientProperties] can be used to configure the connection to the Data Flow server.  The common property to start using is `spring.cloud.dataflow.client.uri`

Consider the following example, using the `definition` style.

[source,java,options="nowrap"]
----
URI dataFlowUri = URI.create("http://localhost:9393");
DataFlowOperations dataFlowOperations = new DataFlowTemplate(dataFlowUri);
dataFlowOperations.appRegistryOperations().importFromResource(
                     "https://bit.ly/Darwin-SR3-stream-applications-rabbit-maven", true);
StreamDefinition streamDefinition = Stream.builder(dataFlowOperations)
                                      .name("ticktock")
                                      .definition("time | log")
                                      .create();
----

The `create` method returns an instance of a `StreamDefinition` representing a Stream that has been created but not deployed.
This is called the `definition` style since it takes a single string for the stream definition, same as in the shell.
If applications have not yet been registered in the Data Flow server, you can use the `DataFlowOperations` class to register them.
With the `StreamDefinition` instance, you have methods available to `deploy` or `destory` the stream.
[source,java]
----
Stream stream = streamDefinition.deploy();
----
The `Stream` instance provides `getStatus`, `destroy` and `undeploy` methods to control and query the stream.
If you are going to immediately deploy the stream, there is no need to create a separate local variable of the type `StreamDefinition`.  You can just chain the calls together, as follows:
[source,java,options="nowrap"]
----
Stream stream = Stream.builder(dataFlowOperations)
                  .name("ticktock")
                  .definition("time | log")
                  .create()
                  .deploy();
----

The `deploy` method is overloaded to take a `java.util.Map` of deployment properties.

The `StreamApplication` class is used in the 'fluent' Java DSL style and is discussed in the next section. The `StreamBuilder` class is returned from the method `Stream.builder(dataFlowOperations)`. In larger applications, it is common to create a single instance of the `StreamBuilder` as a Spring `@Bean` and share it across the application.

=== Java DSL styles

The Java DSL offers two styles to create Streams.

* The `definition` style keeps the feel of using the pipes and filters textual DSL in the shell. This style is selected by using the `definition` method after setting the stream name - for example, `Stream.builder(dataFlowOperations).name("ticktock").definition(<definition goes here>)`.
* The `fluent` style lets you chain together sources, processors, and sinks by passing in an instance of a `StreamApplication`. This style is selected by using the `source` method after setting the stream name - for example, `Stream.builder(dataFlowOperations).name("ticktock").source(<stream application instance goes here>)`. You then chain together `processor()` and `sink()` methods to create a stream definition.

To demonstrate both styles, we include a simple stream that uses both approaches.
A complete sample for you to get started can be found in the https://github.com/spring-cloud/spring-cloud-dataflow-samples[Spring Cloud Data Flow Samples Repository].

The following example demonstrates the definition approach:

[source,java,options="nowrap"]
----
public void definitionStyle() throws Exception{

  Map<String, String> deploymentProperties = createDeploymentProperties();

  Stream woodchuck = Stream.builder(dataFlowOperations)
          .name("woodchuck")
          .definition("http --server.port=9900 | splitter --expression=payload.split(' ') | log")
          .create()
          .deploy(deploymentProperties);

  waitAndDestroy(woodchuck)
}
----

The following example demonstrates the fluent approach:

[source,java,options="nowrap"]
----
private void fluentStyle(DataFlowOperations dataFlowOperations) throws InterruptedException {

  logger.info("Deploying stream.");

  Stream woodchuck = builder
    .name("woodchuck")
    .source(source)
    .processor(processor)
    .sink(sink)
    .create()
    .deploy();

  waitAndDestroy(woodchuck);
}
----
The `waitAndDestroy` method uses the `getStatus` method to poll for the stream's status, as shown in the following example:
[source,java,options="nowrap"]
----
private void waitAndDestroy(Stream stream) throws InterruptedException {

  while(!stream.getStatus().equals("deployed")){
    System.out.println("Wating for deployment of stream.");
    Thread.sleep(5000);
  }

  System.out.println("Letting the stream run for 2 minutes.");
  // Let the stream run for 2 minutes
  Thread.sleep(120000);

  System.out.println("Destroying stream");
  stream.destroy();
}
----

When using the definition style, the deployment properties are specified as a `java.util.Map` in the same manner as using the shell. The `createDeploymentProperties` method is defined as follows:

[source,java,options="nowrap"]
----
private Map<String, String> createDeploymentProperties() {
  DeploymentPropertiesBuilder propertiesBuilder = new DeploymentPropertiesBuilder();
  propertiesBuilder.memory("log", 512);
  propertiesBuilder.count("log",2);
  propertiesBuilder.put("app.splitter.producer.partitionKeyExpression", "payload");
  return propertiesBuilder.build();
}
----

Is this case, application properties are also overridden at deployment time in addition to setting the deployer property `count` for the log application.
When using the fluent style, the deployment properties are added by using the method `addDeploymentProperty` (for example, `new StreamApplication("log").addDeploymentProperty("count", 2)`), and you do not need to prefix the property with `deployer.<app_name>`.

[NOTE]
In order to create and deploy your streams, you need to make sure that the corresponding apps have been registered in the DataFlow server first.
Attempting to create or deploy a stream that contains an unknown app throws an exception. You can register your application by using the `DataFlowTemplate`, as follows:
[source,java,options="nowrap"]
----
dataFlowOperations.appRegistryOperations().importFromResource(
            "https://bit.ly/Darwin-SR3-stream-applications-rabbit-maven", true);
----

The Stream applications can also be beans within your application that are injected in other classes to create Streams.
There are many ways to structure Spring applications, but one way is to have an `@Configuration` class define the `StreamBuilder` and `StreamApplications`, as shown in the following example:

[source,java,options="nowrap"]
----
@Configuration
public class StreamConfiguration {

  @Bean
  public StreamBuilder builder() {
    return Stream.builder(new DataFlowTemplate(URI.create("http://localhost:9393")));
  }

  @Bean
  public StreamApplication httpSource(){
    return new StreamApplication("http");
  }

  @Bean
  public StreamApplication logSink(){
    return new StreamApplication("log");
  }
}
----

Then in another class you can `@Autowire` these classes and deploy a stream.

[source,java,options="nowrap"]
----
@Component
public class MyStreamApps {

  @Autowired
  private StreamBuilder streamBuilder;

  @Autowired
  private StreamApplication httpSource;

  @Autowired
  private StreamApplication logSink;


  public void deploySimpleStream() {
    Stream simpleStream = streamBuilder.name("simpleStream")
                            .source(httpSource)
                            .sink(logSink)
                            .create()
                            .deploy();
  }
}
----

This style lets you share `StreamApplications` across multiple Streams.

=== Using the DeploymentPropertiesBuilder

Regardless of style you choose, the `deploy(Map<String, String> deploymentProperties)` method allows customization of how your streams will be deployed. We made it a easier to create a map with properties by using a builder style, as well as creating static methods for some properties so you don't need to remember the name of such properties. If you take the previous example of `createDeploymentProperties` it could be rewritten as:

[source,java,options="nowrap"]
----
private Map<String, String> createDeploymentProperties() {
	return new DeploymentPropertiesBuilder()
		.count("log", 2)
		.memory("log", 512)
		.put("app.splitter.producer.partitionKeyExpression", "payload")
		.build();
}
----

This utility class is meant to help with the creation of a Map and adds a few methods to assist with defining pre-defined properties.

=== Skipper Deployment Properties

In addition to Spring Cloud Data Flow, you need to pass certain Skipper specific deployment properties, for example selecting the target platform.
The `SkipperDeploymentPropertiesBuilder` provides you all the properties in `DeploymentPropertiesBuilder` and adds those needed for Skipper.

[source,java,options="nowrap"]
----
private Map<String, String> createDeploymentProperties() {
	return new SkipperDeploymentPropertiesBuilder()
		.count("log", 2)
		.memory("log", 512)
		.put("app.splitter.producer.partitionKeyExpression", "payload")
		.platformName("pcf")
		.build();
}
----


[[spring-cloud-dataflow-stream-multi-binder]]
== Stream Applications with Multiple Binder Configurations

In some cases, a stream can have its applications bound to multiple spring cloud stream binders when they are required to connect to different messaging
middleware configurations. In those cases, it is important to make sure the applications are configured appropriately with their binder
configurations. For example, a multi-binder transformer that supports both Kafka and Rabbit binders is the processor in the following stream:

[source,bash,subs=attributes]
----
http | multibindertransform --expression=payload.toUpperCase() | log
----

NOTE: In the example above you would write your own multibindertransform application.

In this stream, each application connects to messaging middleware in the following way:

. The HTTP source sends events to RabbitMQ (`rabbit1`).
. The Multi-Binder Transform processor receives events from RabbitMQ (`rabbit1`) and sends the processed events into Kafka (`kafka1`).
. The log sink receives events from Kafka (`kafka1`).

Here, `rabbit1` and `kafka1` are the binder names given in the spring cloud stream application properties.
Based on this setup, the applications have the following binder(s) in their classpath with the appropriate configuration:

* HTTP: Rabbit binder
* Transform: Both Kafka and Rabbit binders
* Log: Kafka binder

The spring-cloud-stream `binder` configuration properties can be set within the applications themselves.
If not, they can be passed through `deployment` properties when the stream is deployed as shown in the following example:

[source,bash]
----
dataflow:>stream create --definition "http | multibindertransform --expression=payload.toUpperCase() | log" --name mystream

dataflow:>stream deploy mystream --properties "app.http.spring.cloud.stream.bindings.output.binder=rabbit1,app.multibindertransform.spring.cloud.stream.bindings.input.binder=rabbit1,
app.multibindertransform.spring.cloud.stream.bindings.output.binder=kafka1,app.log.spring.cloud.stream.bindings.input.binder=kafka1"
----

One can override any of the binder configuration properties by specifying them through deployment properties.

[[spring-cloud-dataflow-stream-examples]]
== Examples

This chapter includes the following examples:

* <<spring-cloud-dataflow-simple-stream>>
* <<spring-cloud-dataflow-stream-partitions>>
* <<spring-cloud-dataflow-stream-app-types>>

You can find links to more samples in the "`<<dataflow-samples>>`" chapter.

[[spring-cloud-dataflow-simple-stream]]
=== Simple Stream Processing

As an example of a simple processing step, we can transform the payload of the HTTP posted data to upper case by using the following stream definition:
`http | transform --expression=payload.toUpperCase() | log`

To create this stream enter the following command in the shell
```
dataflow:> stream create --definition "http --server.port=9000 | transform --expression=payload.toUpperCase() | log" --name mystream --deploy
```

The following example uses a shell command to post some data:

```
dataflow:> http post --target http://localhost:9000 --data "hello"
```

The preceding example results in an upper-case 'HELLO' in the log, as follows:

`2016-06-01 09:54:37.749  INFO 80083 --- [  kafka-binder-] log.sink    : HELLO`

[[spring-cloud-dataflow-stream-partitions]]
=== Stateful Stream Processing

To demonstrate the data partitioning functionality, the following listing deploys a stream with Kafka as the binder:

```
dataflow:>stream create --name words --definition "http --server.port=9900 | splitter --expression=payload.split(' ') | log"
Created new stream 'words'

dataflow:>stream deploy words --properties "app.splitter.producer.partitionKeyExpression=payload,deployer.log.count=2"
Deployed stream 'words'

dataflow:>http post --target http://localhost:9900 --data "How much wood would a woodchuck chuck if a woodchuck could chuck wood"
> POST (text/plain;Charset=UTF-8) http://localhost:9900 How much wood would a woodchuck chuck if a woodchuck could chuck wood
> 202 ACCEPTED


dataflow:>runtime apps
╔════════════════════╤═══════════╤═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
║App Id / Instance Id│Unit Status│                                                               No. of Instances / Attributes                                                               ║
╠════════════════════╪═══════════╪═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╣
║words.log-v1        │ deployed  │                                                                             2                                                                             ║
╟┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╢
║                    │           │       guid = 24166                                                                                                                                        ║
║                    │           │        pid = 33097                                                                                                                                        ║
║                    │           │       port = 24166                                                                                                                                        ║
║words.log-v1-0      │ deployed  │     stderr = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461063/words.log-v1/stderr_0.log     ║
║                    │           │     stdout = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461063/words.log-v1/stdout_0.log     ║
║                    │           │        url = https://192.168.0.102:24166                                                                                                                   ║
║                    │           │working.dir = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461063/words.log-v1                  ║
╟┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╢
║                    │           │       guid = 41269                                                                                                                                        ║
║                    │           │        pid = 33098                                                                                                                                        ║
║                    │           │       port = 41269                                                                                                                                        ║
║words.log-v1-1      │ deployed  │     stderr = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461063/words.log-v1/stderr_1.log     ║
║                    │           │     stdout = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461063/words.log-v1/stdout_1.log     ║
║                    │           │        url = https://192.168.0.102:41269                                                                                                                   ║
║                    │           │working.dir = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461063/words.log-v1                  ║
╟────────────────────┼───────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╢
║words.http-v1       │ deployed  │                                                                             1                                                                             ║
╟┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╢
║                    │           │       guid = 9900                                                                                                                                         ║
║                    │           │        pid = 33094                                                                                                                                        ║
║                    │           │       port = 9900                                                                                                                                         ║
║words.http-v1-0     │ deployed  │     stderr = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461054/words.http-v1/stderr_0.log    ║
║                    │           │     stdout = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461054/words.http-v1/stdout_0.log    ║
║                    │           │        url = https://192.168.0.102:9900                                                                                                                    ║
║                    │           │working.dir = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803461054/words.http-v1                 ║
╟────────────────────┼───────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╢
║words.splitter-v1   │ deployed  │                                                                             1                                                                             ║
╟┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╢
║                    │           │       guid = 33963                                                                                                                                        ║
║                    │           │        pid = 33093                                                                                                                                        ║
║                    │           │       port = 33963                                                                                                                                        ║
║words.splitter-v1-0 │ deployed  │     stderr = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803437542/words.splitter-v1/stderr_0.log║
║                    │           │     stdout = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803437542/words.splitter-v1/stdout_0.log║
║                    │           │        url = https://192.168.0.102:33963                                                                                                                   ║
║                    │           │working.dir = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/words-1542803437542/words.splitter-v1             ║
╚════════════════════╧═══════════╧═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝

```

When you review the `words.log-v1-0` logs, you should see the following:

```
2016-06-05 18:35:47.047  INFO 58638 --- [  kafka-binder-] log.sink                                 : How
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck
```

When you review the `words.log-v1-1` logs, you should see the following:

```
2016-06-05 18:35:47.047  INFO 58639 --- [  kafka-binder-] log.sink                                 : much
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : would
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : if
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : could
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood
```

This example has shown that payload splits that contain the same word are routed to the same application instance.

[[spring-cloud-dataflow-stream-app-types]]
=== Other Source and Sink Application Types

This example shows something a bit more complicated: swapping out the `time` source for something else. Another supported source type is `http`, which accepts data for ingestion over HTTP POSTs. Note that the `http` source accepts data on a different port from the Data Flow Server (default 8080). By default, the port is randomly assigned.

To create a stream using an `http` source but still using the same `log` sink, we would change the original command in the <<spring-cloud-dataflow-simple-stream>> example to the following:

[source,bash,options="nowrap"]
----
dataflow:> stream create --definition "http | log" --name myhttpstream --deploy
----

Note that we do not see any other output this time until we actually post some data (by using a shell command). In order to see the randomly assigned port on which the http source is listening, run the following command:

[source,bash,options="nowrap"]
----
dataflow:>runtime apps

╔══════════════════════╤═══════════╤═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
║ App Id / Instance Id │Unit Status│                                                                    No. of Instances / Attributes                                                                    ║
╠══════════════════════╪═══════════╪═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╣
║myhttpstream.log-v1   │ deploying │                                                                                  1                                                                                  ║
╟┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╢
║                      │           │       guid = 39628                                                                                                                                                  ║
║                      │           │        pid = 34403                                                                                                                                                  ║
║                      │           │       port = 39628                                                                                                                                                  ║
║myhttpstream.log-v1-0 │ deploying │     stderr = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/myhttpstream-1542803867070/myhttpstream.log-v1/stderr_0.log ║
║                      │           │     stdout = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/myhttpstream-1542803867070/myhttpstream.log-v1/stdout_0.log ║
║                      │           │        url = https://192.168.0.102:39628                                                                                                                             ║
║                      │           │working.dir = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/myhttpstream-1542803867070/myhttpstream.log-v1              ║
╟──────────────────────┼───────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╢
║myhttpstream.http-v1  │ deploying │                                                                                  1                                                                                  ║
╟┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┼┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╢
║                      │           │       guid = 52143                                                                                                                                                  ║
║                      │           │        pid = 34401                                                                                                                                                  ║
║                      │           │       port = 52143                                                                                                                                                  ║
║myhttpstream.http-v1-0│ deploying │     stderr = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/myhttpstream-1542803866800/myhttpstream.http-v1/stderr_0.log║
║                      │           │     stdout = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/myhttpstream-1542803866800/myhttpstream.http-v1/stdout_0.log║
║                      │           │        url = https://192.168.0.102:52143                                                                                                                             ║
║                      │           │working.dir = /var/folders/js/7b_pn0t575l790x7j61slyxc0000gn/T/spring-cloud-deployer-6467595568759190742/myhttpstream-1542803866800/myhttpstream.http-v1             ║
╚══════════════════════╧═══════════╧═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
----

You should see that the corresponding `http` source has a `url` property containing the host and port information on which it is listening. You are now ready to post to that url, as shown in the following example:

[source,bash,subs=attributes]
----
dataflow:> http post --target http://localhost:1234 --data "hello"
dataflow:> http post --target http://localhost:1234 --data "goodbye"
----

The stream then funnels the data from the http source to the output log implemented by the log sink, yielding output similar to the following:

```
2016-06-01 09:50:22.121  INFO 79654 --- [  kafka-binder-] log.sink    : hello
2016-06-01 09:50:26.810  INFO 79654 --- [  kafka-binder-] log.sink    : goodbye
```

We could also change the sink implementation. You could pipe the output to a file (`file`), to hadoop (`hdfs`), or to any of the other sink applications that are available. You can also define your own applications.
