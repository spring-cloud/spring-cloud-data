[[getting-started]]
= Getting started

[partintro]
--
If you're just getting started with Spring Cloud Data Flow, this is the section
for you! Here we answer the basic "`what?`", "`how?`" and "`why?`" questions. You'll
find a gentle introduction to Spring Cloud Data Flow along with installation instructions.
We'll then build our first Spring Cloud Data Flow application, discussing some core principles as
we go.
--

[[getting-started-system-requirements]]
== System Requirements

You need Java installed (Java 7 or better, we recommend Java 8), and to build, you need to have Maven installed as well.

You need to have an RDBMS for storing stream, task and app states in the database. The `local` Data Flow server by default uses embedded H2 database for this.

You also need to have link:http://redis.io[Redis] running if you are running any streams that involve analytics applications. Redis may also be required run the unit/integration tests.

For the deployed streams and tasks to communicate, either link:http://rabbitmq.com[RabbitMQ] or link:http://kafka.apache.org[Kafka] needs to be installed.  The local server registers sources, sink, processors and tasks the are published from the link:https://github.com/spring-cloud/spring-cloud-stream-app-starters[Spring Cloud Stream App Starters] and link:https://github.com/spring-cloud/spring-cloud-task-app-starters[Spring Cloud Task App Starters] repository.  By default the server registers these applications that use Kafka, but setting the property `binding` to `rabbit` will register a list of applications that use RabbitMQ as the message broker.



[[getting-started-deploying-spring-cloud-dataflow]]
== Deploying Spring Cloud Data Flow Local Server

. Download the Spring Cloud Data Flow Server and Shell apps:
+
[source,bash,subs=attributes]
----
wget http://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-server-local/{project-version}/spring-cloud-dataflow-server-local-{project-version}.jar

wget http://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{project-version}/spring-cloud-dataflow-shell-{project-version}.jar
----
+
. Launch the Data Flow Server
+
.. Since the Data Flow Server is a Spring Boot application, you can run it just by using `java -jar`.
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar
----
+
.. Running with Custom Maven Settings and/or Behind a Proxy
If you want to override specific maven configuration properties (remote repositories, etc.) and/or run the Data Flow Server behind a proxy,
you need to specify those properties as command line arguments when starting the Data Flow Server. For example:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar --maven.localRepository=mylocal
--maven.remote-repositories.repo1.url=https://repo1
--maven.remote-repositories.repo1.auth.username=user1
--maven.remote-repositories.repo1.auth.password=pass1
--maven.remote-repositories.repo2.url=https://repo2 --maven.proxy.host=proxy1
--maven.proxy.port=9010 --maven.proxy.auth.username=proxyuser1
--maven.proxy.auth.password=proxypass1
----
+
By default, the protocol is set to `http`. You can omit the auth properties if the proxy doesn't need a username and password.
+
By default, the maven `localRepository` is set to `${user.home}/.m2/repository/`,
and `https://repo.spring.io/libs-snapshot` will be the only remote repository. Like in the above example, the remote
repositories can be specified along with their authentication (if needed). If the remote repositories are behind a proxy,
then the proxy properties can be specified as above.
+
If you want to pass these properties as environment properties, then you need to use `SPRING_APPLICATION_JSON` to set
these properties and pass `SPRING_APPLICATION_JSON` as environment variable as below:
+
[source,bash]
----
$ SPRING_APPLICATION_JSON='{ "maven": { "local-repository": null,
"remote-repositories": { "repo1": { "url": "https://repo1", "auth": { "username": "repo1user", "password": "repo1pass" } }, "repo2": { "url": "https://repo2" } },
"proxy": { "host": "proxyhost", "port": 9018, "auth": { "username": "proxyuser", "password": "proxypass" } } } }' java -jar spring-cloud-dataflow-server-local-{project-version}.jar
----
+
. Launch the shell:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar
----
+
If the Data Flow Server and shell are not running on the same host, point the shell to the Data Flow server:
+
[source,bash]
----
server-unknown:>dataflow config server http://dataflow-server.cfapps.io
Successfully targeted http://dataflow-server.cfapps.io
dataflow:>
----
+
By default, the application registry will be empty. If you would like to register all out-of-the-box stream applications built with the Kafka binder in bulk, you can with the following command. For more details, review how to <<streams.adoc#spring-cloud-dataflow-register-apps, register applications>>.
+
[source,bash,subs=attributes]
----
$ dataflow:>app import --uri http://bit.ly/1-0-4-GA-stream-applications-kafka-maven
----
+
. You can now use the shell commands to list available applications (source/processors/sink) and create streams. For example:
+
[source,bash]
----
dataflow:> stream create --name httptest --definition "http --server.port=9000 | log" --deploy
----
+
NOTE: You will need to wait a little while until the apps are actually deployed successfully
before posting data.  Look in the log file of the Data Flow server for the location of the log
files for the `http` and `log` applications.  Tail the log file for each application to verify
the application has started.
+
Now post some data
[source,bash]
----
dataflow:> http post --target http://localhost:9000 --data "hello world"
----
Look to see if `hello world` ended up in log files for the `log` application.

[NOTE]
====
When deploying locally, each app (and each app instance, in case of `count>1`) gets a dynamically assigned `server.port`
unless you explicitly assign one with `--server.port=x`. In both cases, this setting is propagated as a configuration
property that will override any lower-level setting that you may have used (_e.g._ in `application.yml` files).
====


[TIP]
====
In case you encounter unexpected errors when executing shell commands, you can
retrieve more detailed error information by setting the exception logging level
to `WARNING` in `logback.xml`:

[source,xml]
----
<logger name="org.springframework.shell.core.JLineShellComponent.exceptions" level="WARNING"/>
----

====

[[getting-started-application-configuration]]
== Application Configuration
You can use the following configuration properties of the Data Flow server to customize how appliations are deployed.

[source,properties,indent=0,subs="verbatim,attributes,macros"]
----
spring.cloud.deployer.local.workingDirectoriesRoot=java.io.tmpdir # Directory in which all created processes will run and create log files.

spring.cloud.deployer.local.deleteFilesOnExit=true # Whether to delete created files and directories on JVM exit.

spring.cloud.deployer.local.envVarsToInherit=TMP,LANG,LANGUAGE,"LC_.*. # Array of regular expression patterns for environment variables that will be passed to launched applications.

spring.cloud.deployer.local.javaCmd=java # Command to run java.

spring.cloud.deployer.local.shutdownTimeout=30 # Max number of seconds to wait for app shutdown.

spring.cloud.deployer.local.javaOpts= # The Java options to pass to the JVN
----

When deploying the application you can also set deployer properties prefixed with `app.<name of application>`, So for example to set Java options for the time application in the `ticktock` stream, use the following stream deployment propertites.
[source,bash]
----
dataflow:> stream create --name ticktock --definition "time --server.port=9000 | log"
dataflow:> stream deploy --name ticktock --properties "app.time.spring.cloud.deployer.local.javaopts=-Xmx2048m -Dtest=foo"
----

As a convenience you can set the property `spring.cloud.deployer.local.memory` to set the Java option `-Xmx`.  So for example, 

[source,bash]
----
dataflow:> stream deploy --name ticktock --properties "app.time.spring.cloud.deployer.local.memory=2048m"
----

At deployment time, if you specify a `-Xmx` options in the `javaOpts` property in addition to a value of the `memory` option, the value in the `javaOpts` property has precidence.  Also, the `javaOpts` property set when deploying the application has precidence over the Data Flow server's `javaOpts` property.
