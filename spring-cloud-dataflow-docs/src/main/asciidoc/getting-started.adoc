[[getting-started]]
= Getting started

[partintro]
--
If you are getting started with Spring Cloud Data Flow, this section is for you.
In this section, we answer the basic "`what?`", "`how?`" and "`why?`" questions.
You can find a gentle introduction to Spring Cloud Data Flow along with installation instructions.
We then build an introductory Spring Cloud Data Flow application, discussing some core principles as we go.
--



[[getting-started-system-requirements]]
== System Requirements

You need Java 8 to run and to build you need to have Maven.

You need to have an RDBMS for storing stream definition and deployment properites and task/batch job states.
By default, the Data Flow server uses embedded H2 database for this purpose but you can easily configure the server to use another external database.

You also need to have link:https://redis.io[Redis] running if you are running any streams that involve analytics applications. Redis may also be required to run the unit/integration tests.

For the deployed streams applications communicate, either link:https://www.rabbitmq.com[RabbitMQ] or link:https://kafka.apache.org[Kafka] needs to be installed.

If you would like to have the feature of upgrading and rolling back applications in Streams at runtime, you should install the Spring Cloud Skipper server.


[[getting-started-deploying-spring-cloud-dataflow]]
== Installation

. Download the Spring Cloud Data Flow Server and Shell apps:
+
[source,bash,subs=attributes]
----
wget https://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-server-local/{project-version}/spring-cloud-dataflow-server-local-{project-version}.jar

wget https://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{project-version}/spring-cloud-dataflow-shell-{project-version}.jar
----
+
Starting 1.3.x, the Data Flow Server can run in either the  `skipper` or `classic` mode.
The `classic` mode is how the Data Flow Server worked in the 1.2.x releases.
The mode is specified when starting the Data Flow server using the property `spring.cloud.dataflow.features.skipper-enabled`.
By default, the `classic` mode is enabled.
+
. Download https://cloud.spring.io/spring-cloud-skipper/[Skipper] if you would like the added features of upgrading and rolling back applications inside Streams, since Data Flow delegates to Skipper for those features.
+
[source,yaml,options=nowrap,subs=attributes]
----
wget https://repo.spring.io/{skipper-version-type-lowercase}/org/springframework/cloud/spring-cloud-skipper-server/{skipper-version}/spring-cloud-skipper-server-{skipper-version}.jar

wget https://repo.spring.io/{skipper-version-type-lowercase}/org/springframework/cloud/spring-cloud-skipper-shell/{skipper-version}/spring-cloud-skipper-shell-{skipper-version}.jar
----
+
. Launch Skipper (Required only if you want to run Spring Cloud Data Flow server in `skipper` mode)
+
In the directory where you downloaded Skipper, run the server using `java -jar`, as follows:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-skipper-server-{skipper-version}.jar
----
+
. Launch the Data Flow Server
+
In the directory where you downloaded Data Flow, run the server using `java -jar`, as follows:
+
To run the Data Flow server in `classic` mode:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar
----
+
To run the Data Flow server in `skipper` mode:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar --spring.cloud.dataflow.features.skipper-enabled=true
----
+
If Skipper and the Data Flow server are not running on the same host, set the configuration property `spring.cloud.skipper.client.serverUri` to the location of Skipper, e.g.
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar --spring.cloud.skipper.client.serverUri=https://192.51.100.1:7577/api
----
+
. Launch the Data Flow Shell, as follows:
+
Launching the Data Flow shell requires the appropriate data flow server mode to be specified.
To start the Data Flow Shell for the Data Flow server running in `classic` mode:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar
----
+
To start the Data Flow Shell for the Data Flow server running in `skipper` mode:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar --dataflow.mode=skipper
----
+
NOTE: Both the Data Flow Server and the Shell must be on the same mode.
+
If the Data Flow Server and shell are not running on the same host, you can also point the shell to the Data Flow server URL using the `dataflow config server` command when in the shell's interactive mode.
+
If the Data Flow Server and shell are not running on the same host, point the shell to the Data Flow server URL, as follows:
+
[source,bash]
----
server-unknown:>dataflow config server https://198.51.100.0
Successfully targeted https://198.51.100.0
dataflow:>
----
+
Alternatively, pass in the command line option `--dataflow.uri`.  The shell's command line option `--help` shows what is available.

[[getting-started-deploying-streams-spring-cloud-dataflow]]
== Deploying Streams
. Register Stream Apps
+
By default, the application registry is empty.
As an example, register two applications, `http` and `log`, that communicate by using RabbitMQ.
+
```
dataflow:>app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE
Successfully registered application 'source:http'

dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.0.RELEASE
Successfully registered application 'sink:log'
```
+
For more details, such as how to register applications that are based on docker containers or use Kafka as the messaging middleware, review the section on how to <<streams.adoc#spring-cloud-dataflow-register-stream-apps, register applications>>.
+
NOTE: Depending on your environment, you may need to configure the Data Flow Server to point to a custom
Maven repository location or configure proxy settings.  See <<configuration-maven>> for more information.
+
In this getting started section, we only show deploying a stream, so the commands are the same in `skipper` as well as `classic` mode of the server.
+
. Create a stream
+
Use the `stream create` command to create a stream with a `http` source and a `log` sink and deploy it:
+
[source,bash]
----
dataflow:> stream create --name httptest --definition "http --server.port=9000 | log" --deploy
----
+
NOTE: You need to wait a little while, until the apps are actually deployed successfully, before posting data.
Look in the log file of the Data Flow server for the location of the log files for the `http` and `log` applications.
Use the `tail` command on the log file for each application to verify that the application has started.
+
Now post some data, as shown in the following example:
+
[source,bash]
----
dataflow:> http post --target http://localhost:9000 --data "hello world"
----
Check to see if `hello world` ended up in log files for the `log` application.
The location of the log file for the `log` application will be shown in the Data Flow server's log.

You can read more about the general features of using Skipper to deploy streams in the section <<spring-cloud-dataflow-stream-lifecycle-skipper>> and how to upgrade and rollback streams in <<spring-cloud-dataflow-streams-skipper>>.

[NOTE]
====
When deploying locally, each app (and each app instance, in case of `count > 1`) gets a dynamically assigned `server.port`, unless you explicitly assign one with `--server.port=x`.
In both cases, this setting is propagated as a configuration property that overrides any lower-level setting that you may have used (for example, in `application.yml` files).
====

== Deploying Tasks
In this getting started section, we show how to register a task, create a task definition and then launch it.
We will then also review information about the task executions.
+
NOTE: Launching Spring Cloud Task applications are not delegated to Skipper since they are short lived applications.  Tasks are alwasy deployed directly via the Data Flow Server.
+
. Register a Task App
+
By default, the application registry is empty.
As an example, we will register one task application, `timestamp` which simply prints the current time to the log.
+
[source,bash]
----
dataflow:>app register --name timestamp --type task --uri maven://org.springframework.cloud.task.app:timestamp-task:1.3.0.RELEASE
Successfully registered application 'task:timestamp'
----
+
NOTE: Depending on your environment, you may need to configure the Data Flow Server to point to a custom
Maven repository location or configure proxy settings.  See <<configuration-maven>> for more information.
+
. Create a Task Definition
+
Use the `task create` command to create a task definition using the previously registered `timestamp` application.
In this example, no additional properties are used to configure the `timestamp` application.
+
[source,bash]
----
dataflow:> task create --name printTimeStamp --definition "timestamp"
----
+
. Launch a Task
+
The launching of task definitions is done through the shell's `task launch` command.
+
[source,bash]
----
dataflow:> task launch printTimeStamp
----
+
Check to see if the a timestamp ended up in log file for the timestamp task.
The location of the log file for the task application will be shown in the Data Flow server’s log.
You should see a log entry similar to
+
[source,bash]
----
TimestampTaskConfiguration$TimestampTask : 2018-02-28 16:42:21.051
----
+
. Review task execution
+
Information about the task execution can be obtained using the command `task execution list`.
+
[source,bash]
----
dataflow:>task execution list
╔══════════════╤══╤════════════════════════════╤════════════════════════════╤═════════╗
║  Task Name   │ID│         Start Time         │          End Time          │Exit Code║
╠══════════════╪══╪════════════════════════════╪════════════════════════════╪═════════╣
║printTimeStamp│1 │Wed Feb 28 16:42:21 EST 2018│Wed Feb 28 16:42:21 EST 2018│0        ║
╚══════════════╧══╧════════════════════════════╧════════════════════════════╧═════════╝
----
+
Additional information can be obtained using the command `task execution status`.
+
[source,bash]
----
dataflow:>task execution status --id 1
╔══════════════════════╤═══════════════════════════════════════════════════╗
║         Key          │                       Value                       ║
╠══════════════════════╪═══════════════════════════════════════════════════╣
║Id                    │1                                                  ║
║Name                  │printTimeStamp                                     ║
║Arguments             │[--spring.cloud.task.executionid=1]                ║
║Job Execution Ids     │[]                                                 ║
║Start Time            │Wed Feb 28 16:42:21 EST 2018                       ║
║End Time              │Wed Feb 28 16:42:21 EST 2018                       ║
║Exit Code             │0                                                  ║
║Exit Message          │                                                   ║
║Error Message         │                                                   ║
║External Execution Id │printTimeStamp-ab86b2cc-0508-4c1e-b33d-b3896d17fed7║
╚══════════════════════╧═══════════════════════════════════════════════════╝
----
+
The <<spring-cloud-dataflow-task>> section has more information on the lifecycle of Tasks and also how to use
<<spring-cloud-dataflow-composed-tasks>> which let you create a directed graph where each node of the graph is a task application.

