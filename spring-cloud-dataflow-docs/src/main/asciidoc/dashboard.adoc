[[dashboard]]
= Dashboard

[partintro]
--
This section describes how to use the dashboard of Spring Cloud Data Flow.
--



[[dashboard-introduction]]
== Introduction

Spring Cloud Data Flow provides a browser-based GUI called the dashboard to manage the following information:

* *Apps*: The Apps tab lists all available applications and provides the controls to register/unregister them.
* *Runtime*: The Runtime tab provides the list of all running applications.
* *Streams*: The Streams tab lets you list, design, create, deploy, and destroy Stream Definitions.
ifndef::omit-tasks-docs[]
* *Tasks*: The Tasks tab lets you list, create, launch and, destroy Task Definitions.
endif::omit-tasks-docs[]
* *Jobs*: The Jobs tab lets you perform batch job related functions.
* *Analytics*: The Analytics tab lets you create data visualizations for the various analytics applications.

Upon starting Spring Cloud Data Flow, the dashboard is available at:

`http://<host>:<port>/dashboard`

For example, if Spring Cloud Data Flow is running locally, the dashboard is available at `\http://localhost:9393/dashboard`.

If you have enabled https, then the dashboard will be located at `\https://localhost:9393/dashboard`.
If you have enabled security, a login form is available at `\http://localhost:9393/dashboard/#/login`.

NOTE: The default Dashboard server port is `9393`.

The following image shows the opening page of the Spring Cloud Data Flow dashboard:

.The Spring Cloud Data Flow Dashboard
image::{dataflow-asciidoc}/images/dataflow-dashboard-about.png[The Spring Cloud Data Flow Dashboard, scaledwidth="100%"]



[[dashboard-apps]]
== Apps

The Apps section of the dashboard lists all the available applications and provides the controls to register and unregister them (if applicable).
It is possible to import a number of applications at once by using the Bulk Import Applications action.

The following image shows a typical list of available apps within the dashboard:

.List of Available Applications
image::{dataflow-asciidoc}/images/dataflow-available-apps-list.png[List of available applications, scaledwidth="100%"]



=== Bulk Import of Applications

The Bulk Import Applications page provides numerous options for defining and importing a set of applications all at once.
For bulk import, the application definitions are expected to be expressed in a properties style, as follows:

`<type>.<name> = <coordinates>`

The following examples show a typical application definitions:

[source]
task.timestamp=maven://org.springframework.cloud.task.app:timestamp-task:1.2.0.RELEASE
processor.transform=maven://org.springframework.cloud.stream.app:transform-processor-rabbit:1.2.0.RELEASE

At the top of the bulk import page, a URI can be specified that points to a properties file stored elsewhere, it should contain properties formatted as shown in the previous example.
Alternatively, by using the textbox labeled "`Apps as Properties`", you can directly list each property string. Finally, if the properties are stored in a local file, the "`Select Properties File`" option opens a local file browser to select the file.
After setting your definitions through one of these routes, click Import.

The following image shows the Bulk Import Applications page:

.Bulk Import Applications
image::{dataflow-asciidoc}/images/dataflow-bulk-import-applications.png[Bulk Import Applications, scaledwidth="100%"]



[[dashboard-runtime]]
== Runtime

The Runtime section of the Dashboard application shows the list of all running applications.
For each runtime app, the state of the deployment and the number of deployed instances is shown.
A list of the used deployment properties is available by clicking on the App Id.

The following image shows an example of the Runtime tab in use:

.List of Running Applications
image::{dataflow-asciidoc}/images/dataflow-runtime.png[List of running applications, scaledwidth="100%"]



[[dashboard-streams]]
== Streams

The Streams tab has two child tabs: Definitions and Create Stream. The following topics describe how to work with each one:

* <<dashboard-stream-definitions>>
* <<dashboard-flo-streams-designer>>
* <<dashboard-stream-deploy>>



[[dashboard-stream-definitions]]
=== Working with Stream Definitions

The Streams section of the Dashboard includes the Definitions tab that provides a listing of Stream definitions.
There you have the option to deploy or undeploy those stream definitions.
Additionally, you can remove the definition by clicking on Destroy.
Each row includes an arrow on the left, which you can click to see a visual representation of the definition.
Hovering over the boxes in the visual representation shows more details about the apps, including any options passed to them.

In the following screenshot, the `timer` stream has been expanded to show the visual representation:

.List of Stream Definitions
image::{dataflow-asciidoc}/images/dataflow-streams-list-definitions.png[List of Stream Definitions, scaledwidth="100%"]

If you click the details button, the view changes to show a visual representation of that stream and any related streams.
In the preceding example, if you click details for the `timer` stream, the view changes to the following view, which clearly shows the relationship between the three streams (two of them are tapping into the `timer` stream):

.Stream Details Page
image::{dataflow-asciidoc}/images/dataflow-stream-details.png[Stream Details Page, scaledwidth="100%"]



[[dashboard-flo-streams-designer]]
=== Creating a Stream

The Streams section of the Dashboard includes the Create Stream tab, which makes available the https://github.com/spring-projects/spring-flo[Spring Flo] designer: a canvas application that offers an interactive graphical interface for creating data pipelines.

In this tab, you can:

* Create, manage, and visualize stream pipelines using DSL, a graphical canvas, or both
* Write pipelines via DSL with content-assist and auto-complete
* Use auto-adjustment and grid-layout capabilities in the GUI for simpler and interactive organization of pipelines

You should watch this https://www.youtube.com/watch?v=78CgV46OstI[screencast] that highlights some of the "Flo for Spring Cloud Data Flow" capabilities.
The Spring Flo https://github.com/spring-projects/spring-flo/wiki[wiki] includes more detailed content on core Flo capabilities.

The following image shows the Flo designer in use:

.Flo for Spring Cloud Data Flow
image::{dataflow-asciidoc}/images/dataflow-flo-create-stream.png[Flo for Spring Cloud Data Flo, scaledwidth="100%"]



[[dashboard-stream-deploy]]
=== Deploying a Stream

The stream deploy page include tabs that provide different ways to setup the deployment properties and deploy the stream.
The following screenshots shows the stream deploy page for `foobar` (`time | log`).

You can define deployments properties using:

* Form builder tab: a builder which help you to define deployment properties (deployer, application properties...)
* Free text tab: a free textarea (key/value pairs)

You can switch between the both views, the form builder provides a more stronger validation of the inputs.

.The following image shows the form builder
image::{dataflow-asciidoc}/images/dataflow-stream-deploy-builder.png[Form builder, scaledwidth="100%"]

.The following image shows the same properties in the free text
image::{dataflow-asciidoc}/images/dataflow-stream-deploy-freetext.png[Free text, scaledwidth="100%"]



[dashboard-flo-streams-designer-fanin-fanout]]
=== Creating Fan-In/Fan-Out Streams

In chapter <<spring-cloud-dataflow-stream-dsl-fanin-fanout>> you learned how we can support fan-in and fan-out use cases using <<spring-cloud-dataflow-stream-dsl-named-destinations,named destinations>>.
The UI provides dedicated support for named destinations as well:

.Flo for Spring Cloud Data Flow
image::{dataflow-asciidoc}/images/dataflow-flo-create-stream-fanin-fanout.png[Fan-in and Fan-out example, scaledwidth="100%"]

In this example we have data from an _HTTP Source_ and a _JDBC Source_ that is being sent to the
_sharedData_ channel which represents a *Fan-in* use case.
On the other end we have a _Cassandra Sink_ and a _File Sink_ subscribed to the _sharedData_ channel which represents a *Fan-out* use case.

=== Creating a Tap Stream

Creating Taps using the Dashboard is straightforward.
Let's say you have stream consisting of an _HTTP Source_ and a _File Sink_ and you would like to tap into the stream
to also send data to a _JDBC Sink_.
In order to create the tap stream simply connect the output connector of the _HTTP Source_ to the _JDBC Sink_.
The connection will be displayed as a dotted line, indicating that you created a tap stream.

.Creating a Tap Stream
image::{dataflow-asciidoc}/images/dataflow-flo-create-tap-stream.png[Tap stream example, scaledwidth="100%"]

The primary stream (_HTTP Source_ to _File Sink_) will be automatically named, in case you did not provide a name for the stream, yet.
When creating tap streams, the primary stream must always be explicitly named.
In the picture above, the primary stream was named _HTTP_INGEST_.

Using the Dashboard, you can also switch the primary stream to become the secondary tap stream.

.Change Primary Stream to Secondary Tap Stream
image::{dataflow-asciidoc}/images/dataflow-flo-tap-stream-switch-to-primary-stream.png[Switch tap stream to primary stream, scaledwidth="100%"]

Simply hover over the existing primary stream, the line between _HTTP Source_ and _File Sink_.
Several control icons will appear, and by clicking on the icon labeled _Switch to/from tap_,
you change the primary stream into a tap stream.
Do the same for the tap stream and switch it to a primary stream.

.End Result of Switching the Primary Stream
image::{dataflow-asciidoc}/images/dataflow-flo-tap-stream-switch-to-primary-stream-result.png[End result of switching the tap stream to a primary stream, scaledwidth="100%"]
=======
TIP: When interacting directly with <<spring-cloud-dataflow-stream-dsl-named-destinations,named destinations>>,
there can be "n" combinations (Inputs/Outputs). This allows you to create complex topologies involving a
wide variety of data sources and destinations.
=======

ifndef::omit-tasks-docs[]
[[dashboard-tasks]]
== Tasks

The Tasks section of the Dashboard currently has three tabs:

* <<dashboard-tasks-apps>>
* <<dashboard-task-definition>>
* <<dashboard-tasks-executions>>

[[dashboard-tasks-apps]]
=== Apps

Each app encapsulates a unit of work into a reusable component.
Within the Data Flow runtime environment, apps let users create definitions for streams as well as tasks.
Consequently, the Apps tab within the Tasks section lets users create task definitions.

TIP: You can also use this tab to create Batch Jobs.

The following image shows a typical list of task apps:

.List of Task Apps
image::{dataflow-asciidoc}/images/dataflow-task-apps-list.png[List of Task Apps, scaledwidth="100%"]

On this screen, you can perform the following actions:

* View details, such as the task app options.
* Create a task definition from the respective app.



==== View Task App Details

On this page you can view the details of a selected task app, including the list of available options (properties) for that app.



==== Create a Task Definition from a Selected Task App

On this screen you can create a new Task Definition.
At a minimum, you must provide a name for the new definition.
You also have the option to specify various properties that are used during the deployment of the app.

NOTE: Each parameter is included only if the Include checkbox is selected.



[[dashboard-task-definition]]
=== Definitions

This page lists the Data Flow task definitions and provides actions to launch or destroy those tasks.
It also provides a shortcut operation to define one or more tasks with simple textual input, indicated by
the Bulk Define Tasks button.

The following image shows the Definitions page:

.List of Task Definitions
image::{dataflow-asciidoc}/images/dataflow-task-definitions-list.png[List of Task Definitions, scaledwidth="100%"]



==== Creating Task Definitions with the Bulk Define Interface

After pressing Bulk Define Tasks, the following screen appears:

.Bulk Define Tasks
image::{dataflow-asciidoc}/images/dataflow-bulk-define-tasks.png[Bulk Define Tasks, scaledwidth="100%"]

It includes a textbox where one or more definitions can be entered
and, for each definition, various actions that can be performed.
Each line of the text input should be formatted as follows:

`<task-definition-name> = <task-application> <options>`

For example, the following line defines an a task called `demo-timestamp`:

`demo-timestamp = timestamp --format=hhmmss`

After entering any data, a validator runs asynchronously to verify the syntax, that the application name entered is a valid application, and that it supports the options specified.
If validation fails, the editor shows the errors with more information available in tooltips.

To make it easier to enter definitions into the text area, content assist is supported.
Pressing Ctrl+Space invokes content assist to suggest simple task names (based on the line on which it is invoked), task applications, and task application options.
Press Escape to close the content assist window without choosing a selection.

If the validator should not verify the applications or the options (for example, when specifying non-whitelisted options to the applications), you can turn off that part of validation by toggling the checkbox off on the Verify Apps button.
The validator then performs only syntax checking.
When an app is correctly validated, the Create button becomes clickable.
Clicking it causes the creation of each task definition.
If there are any errors during creation, then, after creation finishes, the editor shows any lines of input that cannot be used in task definitions.
These can then be fixed, and the creation process can be repeated.
The Import File button opens a file browser on the local file system, for those cases where the definitions are in a file and it is easier to import than copy and paste.

NOTE: Bulk loading of composed task definitions is not currently supported.



==== Creating Composed Task Definitions

The dashboard includes the Create Composed Task tab, which provides an interactive graphical interface for creating composed tasks.

In this tab, you can:

* Create and visualize composed tasks using DSL, a graphical canvas, or both.
* Use auto-adjustment and grid-layout capabilities in the GUI for simpler and interactive organization of the composed task.

On the Create Composed Task screen, you can define one or more task parameters by entering both the parameter key and the parameter value.

NOTE: Task parameters are not typed.

The following image shows the composed task designer:

.Composed Task Designer
image::{dataflow-asciidoc}/images/dataflow-ctr-flo-tab.png[Composed Task Designer, scaledwidth="100%"]



==== Launching Tasks

Once the task definition has been created, the tasks can be launched through the dashboard.
To do so, click the Definitions tab and select the task you want to launch by pressing `Launch`.



[[dashboard-tasks-executions]]
=== Executions

The Executions tab shows the current running and completed tasks.

The following image shows the Executions tab:

.List of Task Executions
image::{dataflow-asciidoc}/images/dataflow-task-executions-list.png[List of Task Executions, scaledwidth="100%"]



[[dashboard-jobs]]
== Jobs

The Jobs section of the Dashboard lets you inspect batch jobs.
The main section of the screen provides a list of job executions.
Batch jobs are tasks that each execute one or more batch jobs.
Each job execution has a reference to the task execution ID (in the Task Id column).

The list of Job Executions also shows the state of the underlying Job Definition.
Thus, if the underlying definition has been deleted, "`No definition found`" appears in the Status column.

You can take the following actions for each job:

* Restart (for failed jobs).
* Stop (for running jobs).
* View execution details.

Note: Clicking the stop button actually sends a stop request to the running job, which may not immediately stop.

The following image shows the Jobs page:

.List of Job Executions
image::{dataflow-asciidoc}/images/dataflow-job-executions-list.png[List of Job Executions, scaledwidth="100%"]



[[dashboard-job-executions-details]]
==== Job Execution Details

After having launched a batch job, the Job Execution Details page will show information about the job.

The following image shows the Job Execution Details page:

.Job Execution Details
image::{dataflow-asciidoc}/images/dataflow-jobs-job-execution-details.png[Job Execution Details, scaledwidth="100%"]

The Job Execution Details page contains a list of the executed steps.
You can further drill into the details of each step's execution by clicking the magnifying glass icon.



[[dashboard-job-executions-steps]]
==== Step Execution Details

The Step Execution Details page provides information about an individual step within a job.

The following image shows the Step Execution Details page:

.Step Execution Details
image::{dataflow-asciidoc}/images/dataflow-step-execution-history.png[Step Execution History, scaledwidth="100%"]

On the top of the page, you can see a progress indicator the respective step, with the option to refresh the indicator.
A link is provided to view the step execution history.

The Step Execution Details screen provides a complete list of all Step Execution Context key/value pairs.

IMPORTANT: For exceptions, the Exit Description field contains additional error information.
However, this field can have a maximum of 2500 characters.
Therefore, in the case of long exception stack traces, trimming of error messages may occur.
When that happens, refer to the server log files for further details.



[[dashboard-job-executions-steps-progress]]
==== Step Execution Progress

On this screen, you can see a progress bar indicator in regards to the execution
of the current step. Under the Step Execution History, you can also view various
metrics associated with the selected step, such as duration, read counts, write
counts, and others.

endif::omit-tasks-docs[]



[[dashboard-analytics]]
== Analytics

The Analytics page of the Dashboard provides the following data visualization capabilities for the various analytics applications available in Spring Cloud Data Flow:

* Counters
* Field-Value Counters
* Aggregate Counters

For example, if you create a stream with a link:https://github.com/spring-cloud-stream-app-starters/counter/tree/master/spring-cloud-starter-stream-sink-counter[Counter] application, you can create the corresponding graph from within the Dashboard tab.
To do so:

. Under `Metric Type`, select `Counters` from the select box.
. Under `Stream`, select `tweetcount`.
. Under `Visualization`, select the desired chart option, `Bar Chart`.

Using the icons to the right, you can add additional charts to the Dashboard, re-arange the order of created dashboards, or remove data visualizations.
