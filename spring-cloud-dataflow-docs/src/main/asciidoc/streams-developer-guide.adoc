[[streams-dev-guide]]
= Stream Developer Guide

This section covers how to create, test, and run Spring Cloud Stream applications on your local machine.
It also shows how to map these applications into Spring Cloud Data Flow and deploy them.



[[streams-dev-guide-prebuilt-apps]]
== Prebuilt Applications

The link:https://cloud.spring.io/spring-cloud-stream-app-starters/[Spring Cloud Stream App Starters] project provides many applications that you can start using right away.
For example, there is an HTTP source application that receives messages posted to an HTTP endpoint and publishes the data to the messaging middleware.
Each existing application comes in three variations, one for each type of messaging middleware that is supported.
The current supported messaging middleware systems are RabbitMQ, Apache Kafka 0.9, and Apache Kafka 0.10.
All the applications are based on link:https://projects.spring.io/spring-boot/[Spring Boot] and link:https://cloud.spring.io/spring-cloud-stream/[Spring Cloud Stream].

Applications are published as a Maven artifact as well as a Docker image.
For GA releases, the Maven artifacts are published to Maven central and the link:https://repo.spring.io/release[Spring Release Repository].
Milestone and snapshot releases are published to the link:https://repo.spring.io/milestone[Spring Milestone] and link:https://repo.spring.io/snapshot[Spring Snapshot] repositories, respectively.
Docker images are pushed to link:https://hub.docker.com/u/springcloudstream/[Docker Hub].

We use the Maven artifacts for our examples.
The root location of the Spring Repository that hosts the GA artifacts of prebuilt applications is https://repo.spring.io/release/org/springframework/cloud/stream/app/



[[streams-dev-guiderunning-prebuilt-apps]]
== Running prebuilt applications

In this example, we use RabbitMQ as the messaging middleware.
Follow the directions on link:https://www.rabbitmq.com/download.html[rabbitmq.com] for your platform.
Then install the link:https://www.rabbitmq.com/management.html[management plugin].

We will first run the HTTP source application and the log sink as stand alone applications using `java -jar`.
The two applications use RabbitMQ to communicate.

Download each sample application, as follows:

[source,bash]
----
wget https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/http-source-rabbit/1.3.1.RELEASE//http-source-rabbit-1.3.1.RELEASE.jar

wget https://repo.spring.io/release/org/springframework/cloud/stream/app/log-sink-rabbit/1.3.1.RELEASE/log-sink-rabbit-1.3.1.RELEASE.jar
----

These are Spring Boot applications that include the link:{spring-boot-docs-reference}/html/production-ready.html[Spring Boot Actuator] and the link:{spring-boot-docs-reference}/html/boot-features-security.html[Spring Security Starter].
You can specify link:{spring-boot-docs-reference}/html/common-application-properties.html[common Spring Boot properties] to configure each application.
The properties that are specific to each application are listed in the  link:{scs-app-starters-docs}/[documentation for the Spring App Starters] - for example, the link:{scs-app-starters-docs}/sources.html#spring-cloud-stream-modules-http-source[HTTP source] and the link:{scs-app-starters-docs}/spring-cloud-stream-modules-sinks.html#spring-cloud-stream-modules-log-sink[log sink]

Now you can run the http source application.
Just for fun, pass in a few Boot applications options using system properties, as shown in the following example:

[source,bash]
----
java -Dserver.port=8123 -Dhttp.path-pattern=/data -Dspring.cloud.stream.bindings.output.destination=sensorData -jar http-source-rabbit-1.3.1.RELEASE.jar
----

The `server.port` property comes from Spring Boot's Web support, and the `http.path-pattern` property comes from the HTTP source application, link:https://github.com/spring-cloud-stream-app-starters/http/blob/master/spring-cloud-starter-stream-source-http/src/main/java/org/springframework/cloud/stream/app/http/source/HttpSourceProperties.java[HttpSourceProperties].
The HTTP source app is now listening on port 8123 under the the path `/data`.

The `spring.cloud.stream.bindings.output.destination` property comes from the Spring Cloud Stream library and is the name of the messaging destination that is shared between the source and the sink.
The string `sensorData` in this property is the name of the Spring Integration channel whose contents will be published to the messaging middleware.

Now you can run the log sink application and change the logging level to WARN, as follows:

[source,bash]
----
java -Dlog.level=WARN -Dspring.cloud.stream.bindings.input.destination=sensorData -jar log-sink-rabbit-1.3.1.RELEASE.jar
----

The `log.level` property comes from the log sink application,  link:https://github.com/spring-cloud-stream-app-starters/log/blob/master/spring-cloud-starter-stream-sink-log/src/main/java/org/springframework/cloud/stream/app/log/sink/LogSinkProperties.java[LogSinkProperties].

The value of the property `spring.cloud.stream.bindings.input.destination` is set to `sensorData` so that the source and sink applications can communicate with each other.

You can use the following `curl` command to send data to the `http` application:

[source,bash]
----
curl -H "Content-Type: application/json" -X POST -d '{"id":"1","temperature":"100"}' http://localhost:8123/data
----

The log sink application then shows the following output:

[source,bash]
----
2017-03-17 15:30:17.825  WARN 22710 --- [_qquaYekbQ0nA-1] log-sink                                 : {"id":"1","temperature":"100"}
----



== Custom Processor Application

Now you can create and test an application that does some processing on the output of the HTTP source and then sends data to the log sink.
You can then use the https://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/#__code_source_code_code_sink_code_and_code_processor_code[Processor] convenience class, which has both an inbound channel and an outbound channel.

To do so:

. Visit the link:https://start.spring.io/[Spring Initialzr] site.
.. Create a new Maven project with a *Group* name of `io.spring.stream.sample` and an *Artifact* name of `transformer`.
.. In the Dependencies text box, type `stream` to select the Spring Cloud Stream dependency.
.. In the Dependencies text box, type either `kafka` or `rabbit` and select which middleware you want to use.
.. Click the *Generate Project* button
. Unzip the project and bring the project into your favorite IDE.
. Create a class called Transformer in the `io.spring.stream.sample` package with the following contents:
+
[source,java]
----
package io.spring.stream.sample.transformer;

import org.springframework.cloud.stream.annotation.EnableBinding;
import org.springframework.cloud.stream.annotation.Output;
import org.springframework.cloud.stream.annotation.StreamListener;
import org.springframework.cloud.stream.messaging.Processor;
import org.springframework.messaging.handler.annotation.Payload;

import java.util.HashMap;
import java.util.Map;

@EnableBinding(Processor.class)
public class Transformer {

    @StreamListener(Processor.INPUT)
    @Output(Processor.OUTPUT)
    public Map<String, Object> transform(@Payload Map<String, Object> doc) {
        Map<String, Object> map = new HashMap<>();
        map.put("sensor_id", doc.getOrDefault("id", "-1"));
        map.put("temp_val", doc.getOrDefault("temperature", "-999"));
        return map;
    }
}
----
+
All that this processor is doing is changing the names of the keys in the map and providing a default value if one does not exist.
+
. Open the `TransformerApplicationTests` class (which already exists) and create a simple unit test for the `Transformer` class, as shown in the following example:
+
[source,java]
----
package io.spring.stream.sample;


import org.junit.Test;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringRunner;

import java.util.HashMap;
import java.util.Map;

import static org.assertj.core.api.Assertions.assertThat;
import static org.assertj.core.api.Assertions.entry;

@RunWith(SpringRunner.class)
@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)
public class TransformApplicationTests {

    @Autowired
    private Transformer transformer;

    @Test
    public void simpleTest() {
        Map<String, Object> resultMap = transformer.transform(createInputData());
        assertThat(resultMap).hasSize(2)
                .contains(entry("sensor_id", "1"))
                .contains(entry("temp_val", "100"));
    }

    private Map<String, Object> createInputData() {
        HashMap<String, Object> inputData = new HashMap<>();
        inputData.put("id", "1");
        inputData.put("temperature", "100");
        return inputData;
    }
}
----

Executing `./mvnw clean package` in the root directory of the transformer
project generates the artifact `transformer-0.0.1-SNAPSHOT.jar` under the
`target` directory.

Now you can run all three applications, as shown in the following listing:

[source,bash]
----
java -Dserver.port=8123 \
     -Dhttp.path-pattern=/data \
     -Dspring.cloud.stream.bindings.output.destination=sensorData \
     -jar http-source-rabbit-1.3.1.RELEASE.jar

java -Dserver.port=8090 \
 -Dspring.cloud.stream.bindings.input.destination=sensorData \
 -Dspring.cloud.stream.bindings.output.destination=normalizedSensorData \
 -jar transformer-0.0.1-SNAPSHOT.jar

java -Dlog.level=WARN \
     -Dspring.cloud.stream.bindings.input.destination=normalizedSensorData \
     -jar log-sink-rabbit-1.3.1.RELEASE.jar
----

Now you can post some content to the http source application, as follows:

[source,bash]
----
curl -H "Content-Type: application/json" -X POST -d '{"id":"2","temperature":"200"}' http://localhost:8123/data
----

The preceding `curl` command results in the log sink showing the following output:

[source,bash]
----
2017-03-24 16:09:42.726  WARN 7839 --- [Raj4gYSoR_6YA-1] log-sink                                 : {sensor_id=2, temp_val=200}
----

== Improving the Quality of Service

Without additional configuration, RabbitMQ applications that produce data create a durable topic exchange.
Similarly, a RabbitMQ application that consumes data creates an anonymous auto-delete queue.
This can result in a message not being stored and forwarded by the producer if the producer application started before the consumer application.
Even though the exchange is durable, there needs to be a durable queue bound to the exchange for the message to be stored for later consumption.

To pre-create durable queues and bind them to the exchange, producer applications should set the `spring.cloud.stream.bindings.<channelName>.producer.requiredGroups` property.
The `requiredGroups` property accepts a comma-separated list of groups to which the producer must ensure message delivery even if they start after it has been created.
The consumer applications should then specify the `spring.cloud.stream.bindings.<channelName>.group` property to consume from the durable queue.
The comma-separated list of groups for both properties should generally match.
link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/#consumer-groups[Consumer groups] are also the means by which multiple instances of a consuming application can participate in a competing consumer relationship with other members of the same consumer group.

The following listing shows multiple applications sharing the same groups:

[source,bash]
----
java -Dserver.port=8123 \
     -Dhttp.path-pattern=/data \
     -Dspring.cloud.stream.bindings.output.destination=sensorData \
     -Dspring.cloud.stream.bindings.output.producer.requiredGroups=sensorDataGroup \
     -jar http-source-rabbit-1.2.0.BUILD-SNAPSHOT.jar

java -Dserver.port=8090 \
     -Dspring.cloud.stream.bindings.input.destination=sensorData \
     -Dspring.cloud.stream.bindings.input.group=sensorDataGroup \
     -Dspring.cloud.stream.bindings.output.destination=normalizedSensorData \
     -Dspring.cloud.stream.bindings.output.producer.requiredGroups=normalizedSensorDataGroup \
     -jar transformer-0.0.1-SNAPSHOT.jar

java -Dlog.level=WARN \
     -Dspring.cloud.stream.bindings.input.destination=normalizedSensorData \
     -Dspring.cloud.stream.bindings.input.group=normalizedSensorDataGroup \
     -jar log-sink-rabbit-1.1.1.RELEASE.jar
----

As before, posting data to the `http` source results in the same log message in the sink.

== Mapping Applications onto Data Flow

Spring Cloud Data Flow (SCDF) provides a higher level way to create this group of three Spring Cloud Stream applications by introducing the concept of a stream.
A stream is defined by using Unix-like pipes and a filtering DSL.
Each application is first registered with a simple name, such as `http`, `transformer`, and `log` (for the applications we are using in this example).
The stream DSL to connect these three applications is `http | transformer | log`.

Spring Cloud Data Flow has server and shell components.
Through the shell, you can easily register applications under a name and also create and deploy streams.
You can also use the JavaDSL to perform the same actions.
However, we use the shell for the examples in this chapter.

In the shell application, register the jar files you have on your local machine by using the following commands.
In this example, the `http` and `log` applications are in the `/home/mpollack/temp/dev` directory and the `transformer` application is in the `/home/mpollack/dev-marketing/transformer/target` directory.

The following commands register the three applications:

[source,bash]
----
dataflow:>app register --type source --name http --uri file://home/mpollack/temp/dev/http-source-rabbit-1.2.0.BUILD-SNAPSHOT.jar

dataflow:>app register --type processor --name transformer --uri file://home/mpollack/dev-marketing/transformer/target/transformer-0.0.1-SNAPSHOT.jar

dataflow:>app register --type sink --name log --uri file://home/mpollack/temp/dev/log-sink-rabbit-1.1.1.RELEASE.jar
----

Now you can create a stream definition and deploy it with the following command:

[source,bash]
----
stream create --name httpIngest --definition "http --server.port=8123 --path-pattern=/data | transformer --server.port=8090 | log --level=WARN" --deploy

----

Then, in the shell, you can query for the list of streams, as shown (with output) in the following listing:

[source,bash,options="nowrap"]
----
dataflow:>stream list
╔═══════════╤════════════════════════════════════════════════════════════════════════════════════════════════╤═════════╗
║Stream Name│                                       Stream Definition                                        │ Status  ║
╠═══════════╪════════════════════════════════════════════════════════════════════════════════════════════════╪═════════╣
║httpIngest │http --server.port=8123 --path-pattern=/data | transformer --server.port=8090 | log --level=WARN│Deploying║
╚═══════════╧════════════════════════════════════════════════════════════════════════════════════════════════╧═════════╝

----

Eventually, you can see the status column say `Deployed`.

In the server log, you can see output similar to the following:

----
2017-03-24 17:12:44.071  INFO 9829 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app httpIngest.log instance 0
   Logs will be in /tmp/spring-cloud-dataflow-4401025649434774446/httpIngest-1490389964038/httpIngest.log
2017-03-24 17:12:44.153  INFO 9829 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app httpIngest.transformer instance 0
   Logs will be in /tmp/spring-cloud-dataflow-4401025649434774446/httpIngest-1490389964143/httpIngest.transformer
2017-03-24 17:12:44.285  INFO 9829 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app httpIngest.http instance 0
   Logs will be in /tmp/spring-cloud-dataflow-4401025649434774446/httpIngest-1490389964264/httpIngest.http
----

You can go to each directory to see the logs of each application.
In the RabbitMQ management console, you can see two exchanges and two durable queues.

The SCDF server has configured the input and output destinations, through the `requiredGroups` and  `group` properties, for each application, as was done explicitly in the previous example.

Now you can post some content to the http source application, as follows:

[source,bash]
----
curl -H "Content-Type: application/json" -X POST -d '{"id":"1","temperature":"100"}' http://localhost:8123/data
----

Using the `tail` command on the stdout_0.log file for the log sink then shows output similar to the following listing:

[source,bash]
----
2017-03-24 17:29:55.280  WARN 11302 --- [er.httpIngest-1] log-sink                                 : {sensor_id=4, temp_val=400}
----

If you access the Boot actuator endpoint for the applications, you can see the conventions that SCDF has made for the destination names, the consumer groups, and the requiredGroups configuration properties, as shown in the following listing:

[source,bash]
----
# for the http source
"spring.cloud.stream.bindings.output.producer.requiredGroups": "httpIngest",
"spring.cloud.stream.bindings.output.destination": "httpIngest.http",
"spring.cloud.application.group": "httpIngest",


# For the transformer
"spring.cloud.stream.bindings.input.group": "httpIngest",
"spring.cloud.stream.bindings.output.producer.requiredGroups": "httpIngest",


"spring.cloud.stream.bindings.output.destination": "httpIngest.transformer",
"spring.cloud.stream.bindings.input.destination": "httpIngest.http",
"spring.cloud.application.group": "httpIngest",

# for the log sink
"spring.cloud.stream.bindings.input.group": "httpIngest",
"spring.cloud.stream.bindings.input.destination": "httpIngest.transformer",
"spring.cloud.application.group": "httpIngest",
----
