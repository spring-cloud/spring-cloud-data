[[spring-cloud-dataflow-task]]
= Tasks

[partintro]
--
This section goes into more detail about how you can orchestrate https://cloud.spring.io/spring-cloud-task/[Spring Cloud Task] applications on Spring Cloud Data Flow.

If you are just starting out with Spring Cloud Data Flow, you should probably read the Getting Started guide for  "`<<getting-started-local.adoc#getting-started-local, Local>>`" , "`<<getting-started-cloudfoundry.adoc#getting-started-cloudfoundry, Cloud Foundry>>`", "`<<getting-started-kubernetes.adoc#getting-started-kubernetes, Kubernetes>>`" before diving into this section.
--

[[spring-cloud-dataflow-task-intro]]
== Introduction

A task application is short lived, meaning it stops running on purpose, and can be executed on demand or scheduled for execution.
A use case might be to scrape a web page and write to the database.
The http://cloud.spring.io/spring-cloud-task/[Spring Cloud Task] framework is based on Spring Boot and adds the capability for Boot applications to record the lifecycle events of a short lived application such as when it starts, when it ends and the exit status.
The https://docs.spring.io/spring-cloud-task/docs/{spring-cloud-task-version}/reference/htmlsingle/#features-task-execution-details[TaskExecution] documentation shows which information is stored in the database.
The entry point for code execution in a Spring Cloud Task application is most often an implementation of Boot's CommandLineRunner interface as shown in this https://docs.spring.io/spring-cloud-task/docs/{spring-cloud-task-version}/reference/htmlsingle/#getting-started-writing-the-code[example].

The Spring Batch project is probably what comes to mind for Spring developers writing short lived applications.
Spring Batch provides a much richer set of functionality than Spring Cloud Task and is recommended when processing large volumes of data.
A use case might be to read many CSV files, transform each row of data, and write each transformed row to a database.
Spring Batch provides its own database schema with a much more rich https://docs.spring.io/spring-batch/{spring-batch-version}/reference/html/schema-appendix.html#metaDataSchema[set of information] about the execution of a Spring Batch job.
Spring Cloud Task is integrated with Spring Batch so that if a Spring Cloud Task application defined a Spring Batch `Job`, a link between the Spring Cloud Task and Spring Cloud Batch execution tables is created.

When running Data Flow on your local machine, Tasks are launched in a separate JVM.
When running on Cloud Foundry, tasks are launched using https://docs.cloudfoundry.org/devguide/using-tasks.html[Cloud Foundry's Task] functional and when running on Kubernetes, task are launched using the `Job` kind type.


== The Lifecycle of a Task

Before we dive deeper into the details of creating Tasks, we need to understand the typical lifecycle for tasks in the context of Spring Cloud Data Flow:

. <<spring-cloud-dataflow-create-task-apps>>
. <<spring-cloud-dataflow-register-task-apps>>
. <<spring-cloud-dataflow-create-task-definition>>
. <<spring-cloud-dataflow-task-launch>>
. <<spring-cloud-dataflow-task-review-executions>>
. <<spring-cloud-dataflow-task-definition-destroying>>

[[spring-cloud-dataflow-create-task-apps]]
=== Creating a Task Application

While Spring Cloud Task does provide a number of out-of-the-box applications (at https://github.com/spring-cloud-task-app-starters[spring-cloud-task-app-starters]), most task applications require custom development.
  To create a custom task application:

.  Use the http://start.spring.io[Spring Initializer] to create a new project, making sure to select the following starters:
.. `Cloud Task`: This dependency is the `spring-cloud-starter-task`.
.. `JDBC`: This dependency is the `spring-jdbc` starter.
. Within your new project, create a new class to serve as your main class, as follows:
+
[source,java]
----
@EnableTask
@SpringBootApplication
public class MyTask {

    public static void main(String[] args) {
		SpringApplication.run(MyTask.class, args);
	}
}
----
+
. With this class, you need one or more `CommandLineRunner` or `ApplicationRunner` implementations within your application.  You can either implement your own or use the ones provided by Spring Boot (there is one for running batch jobs, for example).
. Packaging your application with Spring Boot into an Ã¼ber jar is done through the standard {spring-boot-docs-reference}/html/getting-started-first-application.html#getting-started-first-application-executable-jar[Spring Boot conventions].
The packaged application can be registered and deployed as noted below.



==== Task Database Configuration

CAUTION: When launching a task application, be sure that the database driver that is being used by Spring Cloud Data Flow is also a dependency on the task application.
For example, if your Spring Cloud Data Flow is set to use Postgresql, be sure that the task application also has Postgresql as a dependency.

TIP: When you run tasks externally (that is, from the command line) and you want Spring Cloud Data Flow to show the TaskExecutions in its UI, be sure that common datasource settings are shared among the both.
By default, Spring Cloud Task uses a local H2 instance, and the execution is recorded to the database used by Spring Cloud Data Flow.



[[spring-cloud-dataflow-register-task-apps]]
=== Registering a Task Application

You can register a Task App with the App Registry by using the Spring Cloud Data Flow Shell `app register` command.
You must provide a unique name and a URI that can be resolved to the app artifact. For the type, specify "task".
The following listing shows three examples:

[source,bash]
----
dataflow:>app register --name task1 --type task --uri maven://com.example:mytask:1.0.2

dataflow:>app register --name task2 --type task --uri file:///Users/example/mytask-1.0.2.jar

dataflow:>app register --name task3 --type task --uri http://example.com/mytask-1.0.2.jar
----

When providing a URI with the `maven` scheme, the format should conform to the following:

`maven://<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>`

If you would like to register multiple apps at one time, you can store them in a properties file where the keys are formatted as `<type>.<name>` and the values are the URIs.
For example, the followinng listing would be a valid properties file:

[source]
task.foo=file:///tmp/foo-1.2.1.BUILD-SNAPSHOT.jar
task.bar=file:///tmp/bar-1.2.1.BUILD-SNAPSHOT.jar


Then you can use the `app import` command and provide the location of the properties file by using the  `--uri` option, as follows:

```
app import --uri file:///tmp/task-apps.properties
```

For convenience, we have the static files with application-URIs (for both maven and docker) available for all the out-of-the-box Task app-starters.
You can point to this file and import all the application-URIs in bulk.
Otherwise, as explained earlier in this chapter, you can register them individually or have your own custom property file with only the required application-URIs in it.
It is recommended, however, to have a "`focused`" list of desired application-URIs in a custom property file.

The following table lists the available static property files:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release
|Maven   | http://bit.ly/Clark-GA-task-applications-maven | http://bit.ly/Clark-BUILD-SNAPSHOT-task-applications-maven
|Docker  | http://bit.ly/Clark-GA-task-applications-docker | http://bit.ly/Clark-BUILD-SNAPSHOT-task-applications-docker
|======================

For example, if you would like to register all out-of-the-box task applications in bulk, you can do so with the following command:

```
dataflow:>app import --uri http://bit.ly/Clark-GA-task-applications-maven
```

You can also pass the `--local` option (which is `TRUE` by default) to indicate whether the properties file location should be resolved within the shell process itself.
If the location should be resolved from the Data Flow Server process, specify `--local false`.

When using either `app register` or `app import`, if a task app is already registered with
the provided name, it is not overridden by default. If you would like to override the
pre-existing task app, then include the `--force` option.

[NOTE]
In some cases, the Resource is resolved on the server side.
In other cases, the URI is passed to a runtime container instance where it is resolved.
Consult the specific documentation of each Data Flow Server for more detail.



[[spring-cloud-dataflow-create-task-definition]]
=== Creating a Task Definition

You can create a task Definition from a task app by providing a definition name as well as
properties that apply to the task execution.  Creating a task definition can be done through
the RESTful API or the shell.  To create a task definition by using the shell, use the
`task create` command to create the task definition, as shown in the following example:

[source]
dataflow:>task create mytask --definition "timestamp --format=\"yyyy\""
Created new task 'mytask'

A listing of the current task definitions can be obtained through the RESTful API or the shell.
To get the task definition list by using the shell, use the `task list` command.

[[spring-cloud-dataflow-task-launch]]
=== Launching a Task
An adhoc task can be launched through the RESTful API or the shell.
To launch an ad-hoc task through the shell, use the `task launch` command, as shown in the following example:

[source]
dataflow:>task launch mytask
 Launched task 'mytask'

When a task is launched, any properties that need to be passed as command line arguments to the task application can be set when launching the task, as follows:
[source]
dataflow:>task launch mytask --arguments "--server.port=8080 --custom=value"

[NOTE]
The arguments need to be passed as `space` delimited values.

Additional properties meant for a `TaskLauncher` itself can be passed in by using a `--properties` option.
The format of this option is a comma-separated string of properties prefixed with `app.<task definition name>.<property>`.
Properties are passed to `TaskLauncher` as application properties.
It is up to an implementation to choose how those are passed into an actual task application.
If the property is prefixed with `deployer` instead of `app`, it is passed to `TaskLauncher` as a deployment property and its meaning may be `TaskLauncher` implementation specific.

`dataflow:>task launch mytask --properties "deployer.timestamp.custom1=value1,app.timestamp.custom2=value2"`

==== Application properties

Each application takes properties to customize its behavior.  As an example, the `timestamp` task `format` setting establishes a output format that is different from the default value.

`dataflow:> task create --definition "timestamp --format=\"yyyy\"" --name printTimeStamp`

This `timestamp` property is actually the same as the `timestamp.format` property specified by the timestamp application.
Data Flow adds the ability to use the shorthand form `format` instead of `timestamp.format`.
One may also specify the longhand version as well, as shown in the following example:

`dataflow:> task create --definition "timestamp --timestamp.format=\"yyyy\"" --name printTimeStamp`

This shorthand behavior is discussed more in the section on <<spring-cloud-dataflow-stream-app-whitelisting>>.
If you have <<spring-cloud-dataflow-stream-app-metadata-artifact, registered application property metadata>> you can use tab completion in the shell after typing `--` to get a list of candidate property names.

The shell provides tab completion for application properties. The shell command `app info --name <appName> --type <appType>` provides additional documentation for all the supported properties.

NOTE: The supported Task `<appType>` is task.


==== Common application properties

In addition to configuration through DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all the task applications that are launched by it.
This can be done by adding properties prefixed with `spring.cloud.dataflow.applicationProperties.task` when starting the server.
When doing so, the server passes all the properties, without the prefix, to the instances it launches.

For example, all the launched applications can be configured to use the properties `prop1` and `prop2` by launching the Data Flow server with the following options:

[source]
--spring.cloud.dataflow.applicationProperties.task.prop1=value1
--spring.cloud.dataflow.applicationProperties.task.prop2=value2

This causes the properties, `prop1=value1` and `prop2=value2`, to be passed to all the launched applications.

[NOTE]
Properties configured by using this mechanism have lower precedence than task deployment properties.
They are overridden if a property with the same key is specified at task launch time (for example, `app.trigger.prop2`
overrides the common property).

[[spring-cloud-dataflow-task-limit-concurrent-executions]]
=== Limit the number concurrent task launches
Spring Cloud Data Flow allows a user establish the maximum number of concurrently running tasks to prevent the saturation of IaaS/hardware resources.
This limit can be configured by setting the `spring.cloud.dataflow.task.maximum-concurrent-tasks` property.   By default it is set to `20`.
If the number of concurrently running tasks is equal or greater than the value set by `spring.cloud.dataflow.task.maximum-concurrent-tasks` the next
task launch request will be declined and a warning message will be returned via the RESTful API, Shell or UI.


[[spring-cloud-dataflow-task-review-executions]]
=== Reviewing Task Executions
Once the task is launched, the state of the task is stored in a relational DB.  The state
includes:

* Task Name
* Start Time
* End Time
* Exit Code
* Exit Message
* Last Updated Time
* Parameters

A user can check the status of their task executions through the RESTful API or the shell.
To display the latest task executions through the shell, use the `task execution list` command.

To get a list of task executions for just one task definition, add `--name` and
the task definition name, for example `task execution list --name foo`.  To retrieve full
details for a task execution use the `task execution status` command with the id of the task execution,
for example `task execution status --id 549`.



[[spring-cloud-dataflow-task-definition-destroying]]
=== Destroying a Task Definition
Destroying a Task Definition removes the definition from the definition repository.
This can be done through the RESTful API or the shell.
To destroy a task through the shell, use the `task destroy` command, as shown in the following example:

[source]
dataflow:>task destroy mytask
 Destroyed task 'mytask'

To destroy all tasks through the shell, use the `task all destroy` command as shown in the following example:

[source]
dataflow:>task all destroy
 Really destroy all tasks? [y, n]: y
 All tasks destroyed

Or use the force command:
[source]
dataflow:>task all destroy --force
 All tasks destroyed

The task execution information for previously launched tasks for the definition remains in the task repository.

NOTE: This does not stop any currently executing tasks for this definition. Instead, it removes the task definition from the database.

NOTE: The `task destroy <task-name>` deletes only the definition and not the task deployed on Cloud Foundry.
The only way to do this now is through the CLI in two steps. First, obtain a list of the apps by using the `cf apps` command.
. Identify the task application to be deleted and run the `cf delete <task-name>` command.



[[spring-cloud-dataflow-validate-task]]
=== Validating a Task

Sometimes the one or more of the apps contained within a task definition contain an invalid URI in its registration.
This can be caused by an invalid URI entered at app registration time or the app was removed from the repository from which it was to be drawn.
To verify that all the apps contained in a task are resolve-able, a user can use the `validate` command.
For example:
[source,bash]
----
dataflow:>task validate time-stamp
ââââââââââââ¤ââââââââââââââââ
âTask Name âTask Definitionâ
â âââââââââââªââââââââââââââââ£
âtime-stampâtimestamp      â
ââââââââââââ§ââââââââââââââââ


time-stamp is a valid task.
âââââââââââââââââ¤ââââââââââââââââââ
â   App Name    âValidation Statusâ
â ââââââââââââââââªââââââââââââââââââ£
âtask:timestamp âvalid            â
âââââââââââââââââ§ââââââââââââââââââ
----
In the example above the user validated their time-stamp task.   As we see `task:timestamp` app is valid.
Now let's see what happens if we have a stream definition with a registered app with an invalid URI.
[source,bash]
----
dataflow:>task validate bad-timestamp
âââââââââââââââ¤ââââââââââââââââ
â  Task Name  âTask Definitionâ
â ââââââââââââââªââââââââââââââââ£
âbad-timestampâbadtimestamp   â
âââââââââââââââ§ââââââââââââââââ


bad-timestamp is an invalid task.
ââââââââââââââââââââ¤ââââââââââââââââââ
â     App Name     âValidation Statusâ
â âââââââââââââââââââªââââââââââââââââââ£
âtask:badtimestamp âinvalid          â
ââââââââââââââââââââ§ââââââââââââââââââ
----
In this case Spring Cloud Data Flow states that the task is invalid because task:badtimestamp has an invalid URI.



[[spring-cloud-dataflow-task-events]]
== Subscribing to Task/Batch Events

You can also tap into various task and batch events when the task is launched.
If the task is enabled to generate task or batch events (with the additional dependencies `spring-cloud-task-stream` and, in the case of Kafka as the binder, `spring-cloud-stream-binder-kafka`), those events are published during the task lifecycle.
By default, the destination names for those published events on the broker (Rabbit, Kafka, and others) are the event names themselves (for instance: `task-events`, `job-execution-events`, and so on).

[source]
dataflow:>task create myTask --definition "myBatchJob"
dataflow:>stream create task-event-subscriber1 --definition ":task-events > log" --deploy
dataflow:>task launch myTask

You can control the destination name for those events by specifying explicit names when launching the task, as follows:

[source]
dataflow:>stream create task-event-subscriber2 --definition ":myTaskEvents > log" --deploy
dataflow:>task launch myTask --properties "app.myBatchJob.spring.cloud.stream.bindings.task-events.destination=myTaskEvents"

The following table lists the default task and batch event and destination names on the broker:

.Task and Batch Event Destinations

[cols="2*"]
|===

|*Event*|*Destination*

|Task events
|`task-events`
|Job Execution events  |`job-execution-events`
|Step Execution events|`step-execution-events`
|Item Read events|`item-read-events`
|Item Process events|`item-process-events`
|Item Write events|`item-write-events`
|Skip events|`skip-events`
|===

[[spring-cloud-dataflow-composed-tasks]]
== Composed Tasks

Spring Cloud Data Flow lets a user create a directed graph where each node of the graph is a task application.
This is done by using the DSL for composed tasks.
A composed task can be created through the RESTful API, the Spring Cloud Data Flow Shell, or the Spring Cloud Data Flow UI.

=== Configuring the Composed Task Runner

Composed tasks are executed through a task application called the https://github.com/spring-cloud-task-app-starters/composed-task-runner[Composed Task Runner].



==== Registering the Composed Task Runner

By default, the Composed Task Runner application is not registered with Spring Cloud Data Flow.
Consequently, to launch composed tasks, we must first register the Composed
Task Runner as an application with Spring Cloud Data Flow, as follows:

`app register --name composed-task-runner --type task --uri maven://org.springframework.cloud.task.app:composedtaskrunner-task:<DESIRED_VERSION>`

You can also configure Spring Cloud Data Flow to use a different task definition name for the composed task runner.
This can be done by setting the `spring.cloud.dataflow.task.composedTaskRunnerName` property to the name of your choice.
You can then register the composed task runner application with the name you set by using that property.



==== Configuring the Composed Task Runner

The Composed Task Runner application has a `dataflow.server.uri` property that is used for validation and for launching child tasks.
This defaults to `http://localhost:9393`. If you run a distributed Spring Cloud Data Flow server, as you would if you deploy the server on Cloud Foundry, YARN, or Kubernetes, you need to provide the URI that can be used to access the server.
You can either provide this `dataflow.server.uri` property for the Composed Task Runner application when launching a composed task or you can provide a `spring.cloud.dataflow.server.uri` property for the Spring Cloud Data Flow server when it is started.
For the latter case, the `dataflow.server.uri` Composed Task Runner application property is automatically set when a composed task is launched.

In some cases, you may wish to execute an instance of the Composed Task Runner through the Task Launcher sink.
In that case, you must configure the Composed Task Runner to use the same datasource that the Spring Cloud Data Flow instance is using.
The datasource properties are set with the `TaskLaunchRequest` through the use of the `commandlineArguments` or the `environmentProperties` switches.
This is because the Composed Task Runner monitors the `task_executions` table to check the status of the tasks that it is running.
Using information from the table, it determines how it should navigate the graph.

===== Configuration Options

The ComposedTaskRunner task has the following options:

* *increment-instance-enabled*
Allows a single ComposedTaskRunner instance to be re-executed without changing the parameters. Default is false which means a ComposedTaskRunner instance can only be executed once with a given set of parameters, if true it can be re-executed. (Boolean, default: false).
ComposedTaskRunner is built using https://github.com/spring-projects/spring-batch[Spring Batch] and thus upon a successful execution the batch job is considered complete.
To launch the same ComposedTaskRunner definition multiple times you must set the `increment-instance-enabled` property to true or change the parameters for the definition for each launch.

* *interval-time-between-checks*
The amount of time in millis that the ComposedTaskRunner will wait between checks of the database to see if a task has completed. (Integer, default: 10000).
ComposedTaskRunner uses the datastore to determine the status of each child tasks.  This interval indicates to ComposedTaskRunner how often it should check the status its child tasks.

* *max-wait-time*
The maximum amount of time in millis that a individual step can run before the execution of the Composed task is failed (Integer, default: 0).
Determines the maximum time each child task is allowed to run before the CTR will terminate with a failure.  The default of `0` indicates no timeout.

* *split-thread-allow-core-thread-timeout*
Specifies whether to allow split core threads to timeout. Default is false; (Boolean, default: false)
Sets the policy governing whether core threads may timeout and terminate if no tasks arrive within the keep-alive time, being replaced if needed when new tasks arrive.

* *split-thread-core-pool-size*
Split's core pool size. Default is 1; (Integer, default: 1)
Each child task contained in a split requires a thread in order to execute.   So for example a definition like: `<AAA || BBB || CCC> && <DDD || EEE>` would require a split-thread-core-pool-size of 3.
This is because the largest split contains 3 child tasks.   A count of 2 would mean that `AAA` and `BBB` would run in parallel but CCC would wait until either `AAA` or `BBB` to finish in order to run.
Then `DDD` and `EEE` would run in parallel.

* *split-thread-keep-alive-seconds*
Split's thread keep alive seconds. Default is 60. (Integer, default: 60)
If the pool currently has more than corePoolSize threads, excess threads will be terminated if they have been idle for more than the keepAliveTime.

* *split-thread-max-pool-size*
Split's maximum pool size. Default is {@code Integer.MAX_VALUE} (Integer, default: <none>).
Establish the maximum number of threads allowed for the thread pool.

* *split-thread-queue-capacity*
Capacity for Split's BlockingQueue. Default is {@code Integer.MAX_VALUE}. (Integer, default: <none>)
** If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing.
** If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
** If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.

* *split-thread-wait-for-tasks-to-complete-on-shutdown*
Whether to wait for scheduled tasks to complete on shutdown, not interrupting running tasks and executing all tasks in the queue. Default is false; (Boolean, default: false)

Note
when using the options above as environment variables, convert to uppercase, remove the dash character and replace with the underscore character. For example: increment-instance-enabled would be INCREMENT_INSTANCE_ENABLED.


=== The Lifecycle of a Composed Task

The lifecycle of a composed task has three parts:

* <<spring-cloud-data-flow-composed-task-creating>>
* <<spring-cloud-data-flow-composed-task-stopping>>
* <<spring-cloud-data-flow-composed-task-restarting>>



[[spring-cloud-data-flow-composed-task-creating]]
==== Creating a Composed Task

The DSL for the composed tasks is used when creating a task definition through the task create command, as shown in the following example:

[source]
dataflow:> app register --name timestamp --type task --uri maven://org.springframework.cloud.task.app:timestamp-task:<DESIRED_VERSION>
dataflow:> app register --name mytaskapp --type task --uri file:///home/tasks/mytask.jar
dataflow:> task create my-composed-task --definition "mytaskapp && timestamp"
dataflow:> task launch my-composed-task

In the preceding example, we assume that the applications to be used by our composed task have not been registered yet.
Consequently, in the first two steps, we register two task applications.
We then create our composed task definition by using the `task create` command.
The composed task DSL in the preceding example, when launched, runs mytaskapp and then runs the timestamp application.

But before we launch the `my-composed-task` definition, we can view what Spring Cloud Data Flow generated for us.
This can be done by executing the task list command, as shown (including its output) in the following example:

[source,bash,options="nowrap"]
----
dataflow:>task list
ââââââââââââââââââââââââââââ¤âââââââââââââââââââââââ¤ââââââââââââ
â        Task Name         â   Task Definition    âTask Statusâ
â âââââââââââââââââââââââââââªâââââââââââââââââââââââªââââââââââââ£
âmy-composed-task          âmytaskapp && timestampâunknown    â
âmy-composed-task-mytaskappâmytaskapp             âunknown    â
âmy-composed-task-timestampâtimestamp             âunknown    â
ââââââââââââââââââââââââââââ§âââââââââââââââââââââââ§ââââââââââââ
----

In the example, Spring Cloud Data Flow created three task definitions, one for each of the applications that makes up our composed task (`my-composed-task-mytaskapp` and `my-composed-task-timestamp`) as well as the composed task (`my-composed-task`) definition.
We also see that each of the generated names for the child tasks is made up of the name of the composed task and the name of the application, separated by a dash `-` (as in _my-composed-task_ `-` _mytaskapp_).



===== Task Application Parameters

The task applications that make up the composed task definition can also contain parameters, as shown in the following example:

`dataflow:> task create my-composed-task --definition "mytaskapp --displayMessage=hello && timestamp --format=YYYY"`



==== Launching a Composed Task
Launching a composed task is done the same way as launching a stand-alone task, as follows:

`task launch my-composed-task`

Once the task is launched, and assuming all the tasks complete successfully, you can see three task executions when executing a `task execution list`, as shown in the following example:

[source,bash,options="nowrap"]
----
dataflow:>task execution list
ââââââââââââââââââââââââââââ¤ââââ¤âââââââââââââââââââââââââââââ¤âââââââââââââââââââââââââââââ¤ââââââââââ
â        Task Name         âID â         Start Time         â          End Time          âExit Codeâ
â âââââââââââââââââââââââââââªââââªâââââââââââââââââââââââââââââªâââââââââââââââââââââââââââââªââââââââââ£
âmy-composed-task-timestampâ713âWed Apr 12 16:43:07 EDT 2017âWed Apr 12 16:43:07 EDT 2017â0        â
âmy-composed-task-mytaskappâ712âWed Apr 12 16:42:57 EDT 2017âWed Apr 12 16:42:57 EDT 2017â0        â
âmy-composed-task          â711âWed Apr 12 16:42:55 EDT 2017âWed Apr 12 16:43:15 EDT 2017â0        â
ââââââââââââââââââââââââââââ§ââââ§âââââââââââââââââââââââââââââ§âââââââââââââââââââââââââââââ§ââââââââââ
----

In the preceding example, we see that `my-compose-task` launched and that it also launched the other tasks in sequential order.
All of them executed successfully with `Exit Code` as `0`.

===== Passing properties to the child tasks

To set the properties for child tasks in a composed task graph at task launch time,
you would use the following format of `app.<composed task definition name>.<child task app name>.<property>`.
Using the following Composed Task definition as an example:

[source,bash]
----
dataflow:> task create my-composed-task --definition "mytaskapp  && mytimestamp"
----
To have mytaskapp display 'HELLO' and set the mytimestamp timestamp format to 'YYYY' for the Composed Task definition, you would use the following task launch format:
[source,bash]
----
task launch my-composed-task --properties "app.my-composed-task.mytaskapp.displayMessage=HELLO,app.my-composed-task.mytimestamp.timestamp.format=YYYY"
----

Similar to application properties, the `deployer` properties can also be set for child tasks using the format format of `deployer.<composed task definition name>.<child task app name>.<deployer-property>`.

[source,bash]
----
task launch my-composed-task --properties "deployer.my-composed-task.mytaskapp.memory=2048m,app.my-composed-task.mytimestamp.timestamp.format=HH:mm:ss"
Launched task 'a1'
----

===== Passing arguments to the composed task runner

Command line arguments for the composed task runner can be passed using `--arguments` option.

For example:

[source,bash]
----
dataflow:>task create my-composed-task --definition "<aaa: timestamp || bbb: timestamp>"
Created new task 'my-composed-task'

dataflow:>task launch my-composed-task --arguments "--increment-instance-enabled=true --max-wait-time=50000 --split-thread-core-pool-size=4" --properties "app.my-composed-task.bbb.timestamp.format=dd/MM/yyyy HH:mm:ss"
Launched task 'my-composed-task'
----

===== Exit Statuses

The following list shows how the Exit Status is set for each step (task) contained in the composed task following each step execution:

* If the `TaskExecution` has an `ExitMessage`, that is used as the `ExitStatus`.
* If no `ExitMessage` is present and the `ExitCode` is set to zero, then the `ExitStatus` for the step is `COMPLETED`.
* If no `ExitMessage` is present and the `ExitCode` is set to any non-zero number, the `ExitStatus` for the step is `FAILED`.



==== Destroying a Composed Task

The command used to destroy a stand-alone task is the same as the command used to destroy a composed task.
The only difference is that destroying a composed task also destroys the child tasks associated with it.
The following example shows the task list before and after using the `destroy` command:

[source,bash,options="nowrap"]
----
dataflow:>task list
ââââââââââââââââââââââââââââ¤âââââââââââââââââââââââ¤ââââââââââââ
â        Task Name         â   Task Definition    âTask Statusâ
â âââââââââââââââââââââââââââªâââââââââââââââââââââââªââââââââââââ£
âmy-composed-task          âmytaskapp && timestampâCOMPLETED  â
âmy-composed-task-mytaskappâmytaskapp             âCOMPLETED  â
âmy-composed-task-timestampâtimestamp             âCOMPLETED  â
ââââââââââââââââââââââââââââ§âââââââââââââââââââââââ§ââââââââââââ
...
dataflow:>task destroy my-composed-task
dataflow:>task list
âââââââââââ¤ââââââââââââââââ¤ââââââââââââ
âTask NameâTask DefinitionâTask Statusâ
âââââââââââ§ââââââââââââââââ§ââââââââââââ
----



[[spring-cloud-data-flow-composed-task-stopping]]
==== Stopping a Composed Task
In cases where a composed task execution needs to be stopped, you can do so through the:

* RESTful API
* Spring Cloud Data Flow Dashboard

To stop a composed task through the dashboard, select the Jobs tab and click the Stop button next to the job execution that you want to stop.

The composed task run is stopped when the currently running child task completes.
The step associated with the child task that was running at the time that the composed task was stopped is marked as `STOPPED` as well as the composed task job execution.



[[spring-cloud-data-flow-composed-task-restarting]]
==== Restarting a Composed Task
In cases where a composed task fails during execution and the status of the composed task is `FAILED`, the task can be restarted.
You can do so through the:

* RESTful API
* The shell
* Spring Cloud Data Flow Dashboard


To restart a composed task through the shell, launch the task with the same parameters.
To restart a composed task through the dashboard, select the Jobs tab and click the Restart button next to the job execution that you want to restart.

NOTE: Restarting a Composed Task job that has been stopped (through the Spring Cloud Data Flow Dashboard or RESTful API) relaunches the `STOPPED` child task and then launches the remaining (unlaunched) child tasks in the specified order.



== Composed Tasks DSL

Composed tasks can be run in three ways:

* <<spring-cloud-data-flow-conditional-execution>>
* <<spring-cloud-data-flow-transitional-execution>>
* <<spring-cloud-data-flow-split-execution>>



[[spring-cloud-data-flow-conditional-execution]]
=== Conditional Execution

Conditional execution is expressed by using a double ampersand symbol (`&&`).
This lets each task in the sequence be launched only if the previous task
successfully completed, as shown in the following example:

`task create my-composed-task --definition "task1 && task2"`

When the composed task called `my-composed-task` is launched, it launches the task called `task1` and, if it completes successfully, then the task called `task2` is launched.
If `task1` fails, then `task2` does not launch.

You can also use the Spring Cloud Data Flow Dashboard to create your conditional execution, by using the designer to drag and drop applications that are required and connecting them together to create your directed graph, as shown in the following image:

.Conditional Execution
image::{dataflow-asciidoc}/images/dataflow-ctr-conditional-execution.png[Composed Task Conditional Execution, scaledwidth="50%"]

The preceding diagram is a screen capture of the directed graph as it being created by using the Spring Cloud Data Flow Dashboard.
You can see that are four components in the diagram that comprise a conditional execution:

* Start icon: All directed graphs start from this symbol.
There is only one.
* Task icon: Represents each task in the directed graph.
* End icon: Represents the termination of a directed graph.
* Solid line arrow: Represents the flow conditional execution flow between:
** Two applications.
** The start control node and an application.
** An application and the end control node.
* End icon: All directed graphs end at this symbol.

TIP: You can view a diagram of your directed graph by clicking the Detail button next to the composed task definition on the Definitions tab.



[[spring-cloud-data-flow-transitional-execution]]
=== Transitional Execution

The DSL supports fine- grained control over the transitions taken during the execution of the directed graph.
Transitions are specified by providing a condition for equality based on the exit status of the previous task.
A task transition is represented by the following symbol `-&gt;`.


==== Basic Transition

A basic transition would look like the following:

```
task create my-transition-composed-task --definition "foo 'FAILED' -> bar 'COMPLETED' -> baz"
```

In the preceding example, `foo` would launch, and, if it had an exit status of `FAILED`, the `bar` task would launch.
If the exit status of `foo` was `COMPLETED`, `baz` would launch.
All other statuses returned by `foo` have no effect, and the task would terminate normally.

Using the Spring Cloud Data Flow Dashboard to create the same " `basic transition` " would resemble the following image:

.Basic Transition
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-basic.png[Composed Task Basic Transition, scaledwidth="50%"]

The preceding diagram is a screen capture of the directed graph as it being created in the Spring Cloud Data Flow Dashboard.
Notice that there are two different types of connectors:

* Dashed line: Represents transitions from the application to one of the possible destination applications.
* Solid line: Connects applications in a conditional execution or a connection between the application and a control node (start or end).

To create a transitional connector:

. When creating a transition, link the application to each possible destination by using the connector.
. Once complete, go to each connection and select it by clicking it.
. A bolt icon appears.
. Click that icon.
. Enter the exit status required for that connector.
. The solid line for that connector turns to a dashed line.



==== Transition With a Wildcard

Wildcards are supported for transitions by the DSL, as shown in the following:

```
task create my-transition-composed-task --definition "foo 'FAILED' -> bar '*' -> baz"
```

In the preceding example, `foo` would launch, and, if it had an exit status of `FAILED`, the `bar` task would launch.
For any exit status of `foo` other than `FAILED`, `baz` would launch.

Using the Spring Cloud Data Flow Dashboard to create the same "`transition with wildcard`" would resemble the following image:

.Basic Transition With Wildcard
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-basic-wildcard.png[Composed Task Basic Transition with Wildcard, scaledwidth="50%"]



==== Transition With a Following Conditional Execution

A transition can be followed by a conditional execution so long as the wildcard
is not used, as shown in the following example:

```
task create my-transition-conditional-execution-task --definition "foo 'FAILED' -> bar 'UNKNOWN' -> baz && qux && quux"
```

In the preceding example, `foo` would launch, and, if it had an exit status of `FAILED`, the `bar` task would launch.
If `foo` had an exit status of `UNKNOWN`, `baz` would launch.
For any exit status of `foo` other than `FAILED` or `UNKNOWN`, `qux` would launch and, upon successful completion, `quux` would launch.

Using the Spring Cloud Data Flow Dashboard to create the same "`transition with conditional execution`" would resemble the following image:

.Transition With Conditional Execution
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-conditional-execution.png[Composed Task Transition with Conditional Execution, scaledwidth="50%"]

NOTE: In this diagram we see the dashed line (transition) connecting the `foo` application to the target applications, but a solid line connecting the conditional executions between `foo`, `qux`, and  `quux`.



[[spring-cloud-data-flow-split-execution]]
=== Split Execution

Splits allow multiple tasks within a composed task to be run in parallel.
It is denoted by using angle brackets (`<>`) to group tasks and flows that are to be run in parallel.
These tasks and flows are separated by the double pipe `||` symbol, as shown in the following example:

`task create my-split-task --definition "<foo || bar || baz>"`

The preceding example above launches tasks `foo`, `bar` and `baz` in parallel.

Using the Spring Cloud Data Flow Dashboard to create the same "`split execution`" would resemble the following image:

.Split
image::{dataflow-asciidoc}/images/dataflow-ctr-split.png[Composed Task Split, scaledwidth="50%"]

With the task DSL, a user may also execute multiple split groups in succession, as shown in the following example:

`task create my-split-task --definition "<foo || bar || baz> && <qux || quux>"'

In the preceding example, tasks `foo`, `bar`, and `baz` are launched in parallel.
Once they all complete, then tasks `qux` and `quux` are launched in parallel.
Once they complete, the composed task ends.
However, if `foo`, `bar`, or `baz` fails, the split containing `qux` and `quux` does not launch.

Using the Spring Cloud Data Flow Dashboard to create the same "`split with multiple groups`" would resemble the following image:

.Split as a part of a conditional execution
image::{dataflow-asciidoc}/images/dataflow-ctr-multiple-splits.png[Composed Task Split, scaledwidth="50%"]

Notice that there is a `SYNC` control node that is inserted by the designer when
connecting two consecutive splits.

NOTE: Tasks that are used in a split should not set the their `ExitMessage`.   Setting the `ExitMessage` is only to be used
with  <<spring-cloud-data-flow-transitional-execution, transitions>>.

==== Split Containing Conditional Execution

A split can also have a conditional execution within the angle brackets, as shown in the following example:

`task create my-split-task --definition "<foo && bar || baz>"`

In the preceding example, we see that `foo` and `baz` are launched in parallel.
However, `bar` does not launch until `foo` completes successfully.

Using the Spring Cloud Data Flow Dashboard to create the same " `split containing conditional execution` " resembles the following image:

.Split with conditional execution
image::{dataflow-asciidoc}/images/dataflow-ctr-split-contains-conditional.png[Composed Task Split With Conditional Execution, scaledwidth="50%"]

==== Establishing the proper thread count for splits

Each child task contained in a split requires a thread in order to execute.  To set this properly you want to look at your graph and count the split that has the largest number of child tasks, this will be the number of threads you will need to utilize.
To set the thread count use the split-thread-core-pool-size property (defaults to 1).   So for example a definition like: `<AAA || BBB || CCC> && <DDD || EEE>` would require a split-thread-core-pool-size of 3.
This is because the largest split contains 3 child tasks.   A count of 2 would mean that `AAA` and `BBB` would run in parallel but CCC would wait until either `AAA` or `BBB` to finish in order to run.
Then `DDD` and `EEE` would run in parallel.

[[spring-cloud-dataflow-launch-tasks-from-stream]]
== Launching Tasks from a Stream

You can launch a task from a stream by using one of the available `task-launcher` sinks. Currently the platforms supported by the `task-launcher` sinks are:

* https://github.com/spring-cloud-stream-app-starters/tasklauncher-local[local],
* https://github.com/spring-cloud-stream-app-starters/tasklauncher-cloudfoundry[Cloud Foundry]
* https://github.com/spring-cloud-stream-app-starters/tasklauncher-kubernetes[Kubernetes]
* https://github.com/spring-cloud-stream-app-starters/tasklauncher-yarn[Yarn]

CAUTION: `task-launcher-local` is meant for development purposes only.

A `task-launcher` sink expects a message containing a https://github.com/spring-cloud/spring-cloud-task/blob/master/spring-cloud-task-stream/src/main/java/org/springframework/cloud/task/launcher/TaskLaunchRequest.java[TaskLaunchRequest] object in its payload.
From the `TaskLaunchRequest` object, the `task-launcher` obtains the URI of the artifact to be launched, as well as the environment properties, command line arguments, deployment properties, and application name to be used by the task.

The https://github.com/spring-cloud-stream-app-starters/tasklauncher-local/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-sink-task-launcher-local/README.adoc[task-launcher-local] can be added to the available sinks by executing the app register command, as follows (for the Rabbit Binder, in this case):

`app register --name task-launcher-local --type sink --uri maven://org.springframework.cloud.stream.app:task-launcher-local-sink-rabbit:jar:1.2.0.RELEASE`

In the case of a Maven-based task that is to be launched, the `task-launcher` application is responsible for downloading the artifact.
You *must* configure the `task-launcher` with the appropriate configuration of https://github.com/spring-cloud/spring-cloud-deployer/blob/master/spring-cloud-deployer-resource-maven/src/main/java/org/springframework/cloud/deployer/resource/maven/MavenProperties.java[Maven Properties], such as `--maven.remote-repositories.repo1.url=http://repo.spring.io/libs-milestone"` to resolve artifacts (in this case against a milestone repo).  Note that this repostory can be different than the one used to register the `task-launcher` application itself.



=== TriggerTask

One way to launch a task with the `task-launcher` is to use the https://github.com/spring-cloud-stream-app-starters/triggertask/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-source-triggertask/README.adoc[triggertask] source.
The `triggertask` source emits a message with a `TaskLaunchRequest` object that contains the required launch information.
The `triggertask` can be added to the available sources by running the app register command, as follows (for the Rabbit Binder, in this case):

`app register --type source --name triggertask --uri maven://org.springframework.cloud.stream.app:triggertask-source-rabbit:1.2.0.RELEASE`

For example, to launch the timestamp task once every 60 seconds, the stream implementation would be as follows:

[source]
stream create foo --definition "triggertask --triggertask.uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE --trigger.fixed-delay=60 --triggertask.environment-properties='spring.datasource.url=jdbc:h2:tcp://localhost:19092/mem:dataflow,spring.datasource.username=sa' | task-launcher-local --maven.remote-repositories.repo1.url=http://repo.spring.io/libs-release" --deploy

If you run `runtime apps`, you can find the log file for the task launcher sink.
By using the `tail` command on that file, you can find the log file for the launched tasks.
Setting of `triggertask.environment-properties` establishes the Data Flow Server's H2 Database as the database where the task executions will be recorded.
You can then see the list of task executions by using the shell command `task execution list`, as shown (with its output) in the following example:

[source,bash,options="nowrap"]
----
dataflow:>task execution list
ââââââââââââââââââââââ¤âââ¤âââââââââââââââââââââââââââââ¤âââââââââââââââââââââââââââââ¤ââââââââââ
â     Task Name      âIDâ         Start Time         â          End Time          âExit Codeâ
â âââââââââââââââââââââªâââªâââââââââââââââââââââââââââââªâââââââââââââââââââââââââââââªââââââââââ£
âtimestamp-task_26176â4 âTue May 02 12:13:49 EDT 2017âTue May 02 12:13:49 EDT 2017â0        â
âtimestamp-task_32996â3 âTue May 02 12:12:49 EDT 2017âTue May 02 12:12:49 EDT 2017â0        â
âtimestamp-task_58971â2 âTue May 02 12:11:50 EDT 2017âTue May 02 12:11:50 EDT 2017â0        â
âtimestamp-task_13467â1 âTue May 02 12:10:50 EDT 2017âTue May 02 12:10:50 EDT 2017â0        â
ââââââââââââââââââââââ§âââ§âââââââââââââââââââââââââââââ§âââââââââââââââââââââââââââââ§ââââââââââ
----



=== TaskLaunchRequest-transform

Another way to start a task with the `task-launcher` would be to create a stream by using the
https://github.com/spring-cloud-stream-app-starters/tasklaunchrequest-transform[Tasklaunchrequest-transform] processor to translate a message payload to a `TaskLaunchRequest`.

The `tasklaunchrequest-transform` can be added to the available processors by executing the app register command, as follows (for the Rabbit Binder, in this case):

`app register --type processor --name tasklaunchrequest-transform --uri maven://org.springframework.cloud.stream.app:tasklaunchrequest-transform-processor-rabbit:1.2.0.RELEASE`

The following example shows the creation of a task that includes the `tasklaunchrequest-transform`:

`stream create task-stream --definition "http --port=9000 | tasklaunchrequest-transform --uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE | task-launcher-local --maven.remote-repositories.repo1.url=http://repo.spring.io/libs-release"`



=== Launching a Composed Task From a Stream

A composed task can be launched with one of the `task-launcher` sinks as discussed <<spring-cloud-dataflow-launch-tasks-from-stream, here>>.
Since we use the `ComposedTaskRunner` directly, we need to set up the task definitions it uses prior to the creation of the composed task launching stream.
Suppose we wanted to create the following composed task definition: `AAA && BBB`.
The first step would be to create the task definitions, as shown in the following example:

[source]
----
task create AAA --definition "timestamp"
task create BBB --definition "timestamp"
----
Now that the task definitions we need for composed task definition are ready, we need to create a stream that launches `ComposedTaskRunner`.
So, in this case, we create a stream with

* A trigger that emits a message once every 30 seconds
* A transformer that creates a `TaskLaunchRequest` for each message received
* A `task-launcher-local` sink that launches a the `ComposedTaskRunner` on our local machine

The stream should resemble the following:

[source]
----
stream create ctr-stream --definition "time --fixed-delay=30 | tasklaunchrequest-transform --uri=maven://org.springframework.cloud.task.app:composedtaskrunner-task:<current release> --command-line-arguments='--graph=AAA&&BBB --increment-instance-enabled=true --spring.datasource.url=...' | task-launcher-local"
----
In the preceding example, we see that the `tasklaunchrequest-transform` is establishing two primary components:

* *uri*: The URI of the `ComposedTaskRunner` that is used
* *command-line-arguments*: To configure the `ComposedTaskRunner`

For now, we focus on the configuration that is required to launch the `ComposedTaskRunner`:

* *graph*: this is the graph that is to be executed by the `ComposedTaskRunner`.
In this case it is `AAA&&BBB`.
* *increment-instance-enabled*: This lets each execution of `ComposedTaskRunner` be unique.
`ComposedTaskRunner` is built by using http://projects.spring.io/spring-batch/[Spring Batch].
Thus, we want a new Job Instance for each launch of the `ComposedTaskRunner`.
To do this, we set `increment-instance-enabled` to be `true`.
* *spring.datasource.**: The datasource that is used by Spring Cloud
Data Flow, which lets the user track the tasks launched by the
`ComposedTaskRunner` and the state of the job execution.
Also, this is so that the `ComposedTaskRunner` can track the state of the tasks it launched and update its state.

NOTE: Releases of `ComposedTaskRunner` can be found
https://github.com/spring-cloud-task-app-starters/composed-task-runner/releases[here].

[[sharing-spring-cloud-dataflows-datastore-with-tasks]]
== Sharing Spring Cloud Data Flow's Datastore with Tasks
As discussed in the <<spring-cloud-dataflow-task, Tasks>> documentation Spring
Cloud Data Flow allows a user to view Spring Cloud Task App executions. So in
this section we will discuss what is required by a Task Application and Spring
Cloud Data Flow to share the task execution information.

[a-common-datastore-dependency]
=== A Common DataStore Dependency
Spring Cloud Data Flow supports many databases out-of-the-box,
so all the user typically has to do is declare the `spring_datasource_*` environment variables
to establish what data store Spring Cloud Data Flow will need.
So whatever database you decide to use for Spring Cloud Data Flow make sure that the your task also
includes that database dependency in its `pom.xml` or `gradle.build` file.  If the database dependency
that is used by Spring Cloud Data Flow is not present in the Task Application, the task will fail
and the task execution will not be recorded.

[a-common-datastore]
=== A Common Data Store
Spring Cloud Data Flow and your task application must access the same datastore instance.
This is so that the task executions recorded by the task application can be read by Spring Cloud Data Flow to list them in the Shell and Dashboard views.
Also the task app must have read  & write privileges to the task data tables that are used by Spring Cloud Data Flow.

Given the understanding of Datasource dependency between Task apps and Spring Cloud Data Flow, let's review how to apply them in various Task orchestration scenarios.

[datasource-simple-task-launch]
==== Simple Task Launch
When launching a task from Spring Cloud Data Flow, Data Flow adds its datasource
properties (`spring.datasource.url`, `spring.datasource.driverClassName`, `spring.datasource.username`, `spring.datasource.password`)
to the app properties of the task being launched.  Thus a task application
will record its task execution information to the Spring Cloud Data Flow repository.

[datasource-task-launcher-sink]
==== Task Launcher Sink
The https://github.com/spring-cloud-stream-app-starters/tasklauncher-local[Task Launcher Sink] allows tasks to be launched via a stream as discussed <<spring-cloud-dataflow-launch-tasks-from-stream, here>>.
Since tasks launched by the Task Launcher Sink may not want their task executions
recorded to the same datastore as Spring Cloud Data Flow,
each https://docs.spring.io/spring-cloud-task/docs/current/apidocs/org/springframework/cloud/task/launcher/TaskLaunchRequest.html[TaskLaunchRequest]
received by the Task Launcher Sink must have the required datasource information established as app properties or command line arguments.
Both https://github.com/spring-cloud-stream-app-starters/tasklaunchrequest-transform/blob/master/spring-cloud-starter-stream-processor-tasklaunchrequest-transform/README.adoc[TaskLaunchRequest-Transform]
and https://github.com/spring-cloud-stream-app-starters/triggertask/blob/master/spring-cloud-starter-stream-source-triggertask/README.adoc[TriggerTask Source] are examples
of how a source and a processor allow a user to set the datasource properties via the app properties or command line arguments.

==== Composed Task Runner
Spring Cloud Data Flow allows a user to create a directed graph where each node
of the graph is a task application and this is done via the
https://github.com/spring-cloud-task-app-starters/composed-task-runner/blob/master/spring-cloud-starter-task-composedtaskrunner/README.adoc[Composed Task Runner].
In this case the rules that applied to a <<datasource-simple-task-launch, Simple Task Launch>>
or <<datasource-task-launcher-sink, Task Launcher Sink>> apply to the composed task runner as well.
All child apps must also have access to the datastore that is being used by the composed task runner
Also, All child apps must have the same database dependency as the composed task runner enumerated in their `pom.xml` or `gradle.build` file.

==== Launching a task externally from Spring Cloud Data Flow
Users may wish to launch Spring Cloud Task applications via another method (scheduler for example) but still track the task execution via Spring Cloud Data Flow.
This can be done so long as the task applications observe the rules specified <<a-common-datastore-dependency, here>> and <<a-common-datastore, here>>.

NOTE: If a user wishes to use Spring Cloud Data Flow to view their
https://projects.spring.io/spring-batch/[Spring Batch] jobs, the user must make sure that
their batch application use the `@EnableTask` annotation and follow the rules enumerated <<a-common-datastore-dependency, here>> and <<a-common-datastore, here>>.
More information is available https://github.com/spring-projects/spring-batch-admin/blob/master/MIGRATION.md[here].

[[spring-cloud-dataflow-schedule-launch-tasks]]
== Scheduling Tasks

Spring Cloud Data Flow lets a user schedule the execution of tasks via a cron expression.
A schedule can be created through the RESTful API or the Spring Cloud Data Flow UI.

=== The Scheduler

Spring Cloud Data Flow will schedule the execution of its tasks via a scheduling agent that is available on the cloud platform.
When using the Cloud Foundry platform Spring Cloud Data Flow will use the https://www.cloudfoundry.org/the-foundry/scheduler/[PCF Scheduler].
When using Kubernetes, a https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/[CronJob] will be used.

.Architectural Overview
image::{dataflow-asciidoc}/images/dataflow-scheduling-architecture.png[Scheduler Architecture Overview, scaledwidth="50%"]

=== Enabling Scheduling

By default the Spring Cloud Data Flow leaves the scheduling feature disabled.  To enable the scheduling feature the following feature properties must be set to `true`:

* `spring.cloud.dataflow.features.schedules-enabled`
* `spring.cloud.dataflow.features.tasks-enabled`

=== The Lifecycle of a Schedule

The lifecycle of a schedule has 2 parts:

* <<spring-cloud-data-flow-schedule-scheduling>>
* <<spring-cloud-data-flow-schedule-unscheduling>>

[[spring-cloud-data-flow-schedule-scheduling]]
==== Scheduling a Task Execution

You can schedule a task execution via the:

* RESTful API
* Spring Cloud Data Flow Dashboard

To schedule a task from the UI click the Tasks tab at the top of the screen, this will take you to the Task Definitions screen.   Then from the Task Definition that you wish to schedule click the "clock" icon associated with task definition you wish to schedule.
This will lead you to a `Create Schedule(s)` screen, where you will create a unique name for the schedule and enter the associated cron expression.
Keep in mind you can always create multiple schedules for a single task definition.

[[spring-cloud-data-flow-schedule-unscheduling]]
==== Deleting a Schedule

You can delete a schedule via the:

* RESTful API
* Spring Cloud Data Flow Dashboard

To delete a schedule through the dashboard, select the Schedule tab under Tasks tab and click the `garbage can` icon next to the schedule you wish to delete.

NOTE: Any currently running tasks that were run by the scheduling agent will not be stopped if the schedule is deleted.   It only prevents future executions.
