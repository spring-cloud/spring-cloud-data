[[spring-cloud-task-overview]]
= Tasks

[partintro]
--
This section goes into more detail about how you can work with
http://cloud.spring.io/spring-cloud-task/[Spring Cloud Tasks]. It covers topics such as
creating and running task applications.

If you're just starting out with Spring Cloud Data Flow, you should probably read the
_<<getting-started.adoc#getting-started, Getting Started>>_ guide before diving into
this section.
--

[[spring-cloud-dataflow-task-intro]]
== Introducing Spring Cloud Task
A task executes a process on demand.  In this case a task is a
http://projects.spring.io/spring-boot/[Spring Boot] application that is annotated with
`@EnableTask`.  Hence a user launches a task that performs a certain process, and once
complete the task ends. An example of a task would be a boot application that exports
data from a JDBC repository to an HDFS instance.  Tasks record the start time and the end
time as well as the boot exit code in a relational database. The task implementation is
based on the http://cloud.spring.io/spring-cloud-task/[Spring Cloud Task] project.

== The Lifecycle of a task
Before we dive deeper into the details of creating Tasks, we need to understand the
typical lifecycle for tasks in the context of Spring Cloud Data Flow:

1. Register a Task App
2. Create a Task Definition
3. Launch a Task
4. Task Execution
5. Destroy a Task Definition

=== Register a Task App
Register a Task App with the App Registry using the Spring Cloud Data Flow Shell
`app register` command. You must provide a unique name and a URI that can be
resolved to the app artifact. For the type, specify "task". Here are a few examples:

```
dataflow:>app register --name task1 --type task --uri maven://com.example:mytask:1.0.2

dataflow:>app register --name task2 --type task --uri file:///Users/example/mytask-1.0.2.jar

dataflow:>app register --name task3 --type task --uri http://example.com/mytask-1.0.2.jar
```

When providing a URI with the `maven` scheme, the format should conform to the following:

```
maven://<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>
```

If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as `<type>.<name>` and the values are the URIs. For example, this
would be a valid properties file:

```
task.foo=file:///tmp/foo.jar
task.bar=file:///tmp/bar.jar
```

Then use the `app import` command and provide the location of the properties file via `--uri`:

```
app import --uri file:///tmp/task-apps.properties
```

You can also pass the `--local` option (which is TRUE by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify `--local false`. 

When using either `app register` or `app import`, if a task app is already registered with
the provided name, it will not be overridden by default. If you would like to override the
pre-existing task app, then include the `--force` option.

[NOTE]
In some cases the Resource is resolved on the server side, whereas in others the
URI will be passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.


=== Create a Task Definition
Create a Task Definition from a Task App by providing a definition name as well as
properties that apply to the task execution.  Creating a task definition can be done via
the restful API or the shell.  To create a task definition using the shell, use the
`task create` command to create the task definition.  For example:

```
dataflow:>task create mytask --definition "timestamp --format=\"yyyy\""
 Created new task 'mytask'
```

A listing of the current task definitions can be obtained via the restful API or the
shell.  To get the task definition list using the shell, use the `task list` command.

=== Launch an ad-hoc Task
An adhoc task can be launched via the restful API or via the shell.  To launch an ad-hoc
task via the shell use the `task launch` command.  For Example:

```
dataflow:>task launch mytask
 Launched task 'mytask'
```

=== Task Execution
Once the task is launched the state of the task is stored in a relational DB.  The state
includes:

* Task Name
* Start Time
* End Time
* Exit Code
* Exit Message
* Last Updated Time
* Parameters

A user can check the status of their task executions via the restful API or by the shell.
To display the latest task executions via the shell use the `task execution list` command.

To get a list of task executions for just one task definition, add `--name` and
the task definition name, for example `task execution list --name foo`.  To retrieve full
details for a task execution use the `task display` command with the id of the task execution
, for example `task display --id 549`.

=== Destroy a Task Definition
Destroying a Task Definition will remove the definition from the definition repository.
This can be done via the restful API or via the shell.  To destroy a task via the shell
use the `task destroy` command. For Example:

```
dataflow:>task destroy mytask
 Destroyed task 'mytask'
```

The task execution information for previously launched tasks for the definition will
remain in the task repository.

*Note:* This will not stop any currently executing tasks for this definition, this just
removes the definition.

[[spring-cloud-dataflow-task-repository]]
== Task Repository

Out of the box Spring Cloud Data Flow offers an embedded instance of the H2 database.
The H2 is good for development purposes but is not recommended for production use.

=== Configuring the Task Execution Repository
To add a driver for the database that will store the Task Execution information, a
dependency for the driver will need to be added to a maven pom file and the
Spring Cloud Data Flow will need to be rebuilt.  Since Spring Cloud Data Flow is comprised of an SPI for
each environment it supports, please review the SPI's documentation on which POM should be
updated to add the dependency and how to build.  This document will cover how to setup the
dependency for local SPI.

==== Local

1. Open the spring-cloud-dataflow-server-local/pom.xml in your IDE.
2. In the `dependencies` section add the dependency for the database driver required.  In
the sample below postgresql has been chosen.
```
<dependencies>
...
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>postgresql</artifactId>
    </dependency>
...
</dependencies>
```
[start=3]
1. Save the changed pom.xml
2. Build the application as described here: <<appendix-building.adoc#building, Building Spring Cloud Data Flow>>

=== Datasource

To configure the datasource Add the following properties to the dataflow-server.yml or via
environment variables:

a. spring.datasource.url
b. spring.datasource.username
c. spring.datasource.password
d. spring.datasource.driver-class-name

For example adding postgres would look something like this:

* Environment variables:
```
export spring_datasource_url=jdbc:postgresql://localhost:5432/mydb
export spring_datasource_username=myuser
export spring_datasource_password=mypass
export spring_datasource_driver-class-name="org.postgresql.Driver"
```
* dataflow-server.yml
```
spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypass
    driver-class-name:org.postgresql.Driver
```

[[spring-cloud-dataflow-task-events]]
== Subscribing to Task/Batch events

You can also tap into various task/batch events when the task is launched.
If the task is enabled to generate task and/or batch events (with the additional dependencies `spring-cloud-task-stream` and `spring-cloud-stream-binder-kafka`, in the case of Kafka as the binder), those events are published during the task lifecycle. 
By default, the destination names for those published events on the broker (rabbit, kafka etc.,) are the event names themselves (for instance: `task-events`, `job-execution-events` etc.,).

```
dataflow:>task create myTask --definition â€œmyBatchJob"
dataflow:>task launch myTask
dataflow:>stream create task-event-subscriber1 --definition ":task-events > log" --deploy
```

You can control the destination name for those events by specifying explicit names when launching the task such as:

```
dataflow:>task launch myTask --properties "spring.cloud.stream.bindings.task-events.destination=myTaskEvents"
dataflow:>stream create task-event-subscriber2 --definition ":myTaskEvents > log" --deploy
```

The default Task/Batch event and destination names on the broker are enumerated below:

.Task/Batch Event Destinations

[cols="2*"]
|===

|*Event*|*Destination*

|Task events
|`task-events`
|Job Execution events  |`job-execution-events`
|Step Execution events|`step-execution-events`
|Item Read events|`item-read-events`
|Item Process events|`item-process-events`
|Item Write events|`item-write-events`
|Skip events|`skip-events`
|===
