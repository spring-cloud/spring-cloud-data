[[streams-monitoring-local]]
== Stream Monitoring - Local

[[streams-monitoring-local-prometheus]]
=== Prometheus

NOTE: The <<getting-started-local-deploying-spring-cloud-dataflow-docker,Data Flow docker-compose >> comes already configured with `Prometheus` and `Grafana` for monitoring.
When installing Data Flow manually (e.g. without `docker-compose`) follow the instructions below to configure `Prometheus` and `Grafana` in your Data Flow installations.

https://prometheus.io/[Prometheus] is a popular pull based time series database that pulls the metrics from the target applications from preconfigured endpoints.
Prometheus needs to be regularly provided with the URLs of the target applications to monitor.
 This task is delegated to a component called Service Discovery, which usually a platform specific implementation.
The Data Flow server provides a standalone Service Discovery service that uses the Data Flow `/runtime/apps` REST endpoint to compute the URL locations of the deployed application's Prometheus endpoints.

To enable Micrometer's Prometheus meter registry for <<applications,Spring Cloud Stream application starters>>, set the following properties.

[source,bash]
----
management.metrics.export.prometheus.enabled=true
management.endpoints.web.exposure.include=prometheus
----

Disable the application's security which allows for a simple Prometheus configuration to scrape monitoring information by setting the following property.

[source,bash]
----
spring.cloud.streamapp.security.enabled=false
----

The following steps will start up Prometheus, Grafana, and the local service discovery application.

NOTE: It is recommended that you upgrade to the link:https://docs.docker.com/compose/install/[latest version] of Docker before running the `docker-compose` command. We have tested it against Docker Engine version `18.09.2`

Clone the Data Flow github repository for tagged release and change to the `prometheus/docker` folder:
[source,bash]
----
cd ./src/grafana/prometheus/docker
----

Set the SCDF_HOST_IP environment variable to the IP address of your local host.  Use the real IP address and not the localhost/127.0.0.1.
You can use `ifconfig` to find out your IP address.
[source,bash]
----
export SCDF_HOST_IP=<YOUR locahost IP address>
----
In many cases the provided `find_host_ip.sh` script will give you the IP address.
[source,bash]
----
source ./find_host_ip.sh
----

Start Prometheus, Grafana + Service-Discovery using `docker-compose`.
[source,bash]
----
docker-compose up -d --build
----

Check the containers have started:
[source,bash]
----
docker ps
CONTAINER ID IMAGE              ...  PORTS                    NAMES
2b8b6a442365 tzolov/spring-...  ...  0.0.0.0:8181->8181/tcp   service-discovery
bff63c4902d5 docker_prometheus  ...  0.0.0.0:9090->9090/tcp   prometheus
40190da6aa4b docker_grafana     .... 0.0.0.0:3000->3000/tcp   grafana
----

To validate the setup, you can login into those containers using the following commands.
[source,bash]
----
docker exec -it service-discovery /bin/sh
docker exec -it prometheus /bin/sh
docker exec -it grafana /bin/bash
----
Then on the prometheus and service-discovery containers you can check the content of the targets.json file like this: cat /tmp/scdf-targets/targets.json

You can reach the Prometheus UI on `http://localhost:9090/graph` and `http://localhost:9090/targets`

The Grafana dashboard can be reached at `http://localhost:3000` with credentials user: admin, password: admin.
It comes with two provisioned dashboards

. Streams: `http://localhost:3000/d/scdf-streams/streams?refresh=10s`
. Applications: `http://localhost:3000/d/scdf-applications/applications?refresh=10s`

Start the Skipper server.  Then start the Data Flow server with the following properties:
[source,bash]
----
--spring.cloud.dataflow.applicationProperties.stream.management.metrics.export.prometheus.enabled=true
--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.streamapp.security.enabled=false
--spring.cloud.dataflow.applicationProperties.stream.management.endpoints.web.exposure.include=prometheus,info,health
--spring.cloud.dataflow.grafana-info.url=http://localhost:3000
----

Now if you deploy a simple stream that uses Kafka, such as
[source,bash]
----
dataflow:>app import --uri http://bit.ly/Einstein-SR2-stream-applications-kafka-maven --force
dataflow:>stream create stream2 --definition "time --fixed-delay=10 --time-unit=MILLISECONDS | filter --expression=payload.contains('3') | log" --deploy
----

You should see dashboards similar to these.

image::{dataflow-asciidoc}/images/grafana-prometheus-scdf-applications-dashboard.png[Grafana Prometheus Dashboard, scaledwidth="80%"]

You can destroy all containers with
[source,bash]
----
docker-compose down
----

[[streams-monitoring-local-influx]]
=== InfluxDB

NOTE: The <<getting-started-local-deploying-spring-cloud-dataflow-docker,Data Flow docker-compose >> is pre-configured with `Prometheus` for monitoring.
Follow the <<getting-started-local-customizing-spring-cloud-dataflow-docker-influxdb,customization instructions>> to replace `Prometheus` with `InfluxDB` inside the docker-compose script.
When installing Data Flow manually (e.g. without `docker-compose`) follow the instructions below to configure InfluxDB and Grafana in your Data Flow installations.

https://github.com/influxdata/influxdb[InfluxDB] is a popular open-source push based time series database.
It supports downsampling, automatically expiring and deleting unwanted data, as well as backup and restore. Analysis of data is done via a  https://docs.influxdata.com/influxdb/v1.5/query_language/[SQL-like query] language.

To enable Micrometer's Influx meter registry for <<applications,Spring Cloud Stream application starters>>, set the following property.

[source,bash]
----
management.metrics.export.influx.enabled=true
----

In the docker setup provided below the InfluxDB server runs on localhost:8086.
If you use a different InfluxDB server, setting <<spring-cloud-dataflow-global-properties>> for Influx is a convenient way to have all deployed applications configured to send metrics to Influx.  The property to set is `management.metrics.export.influx.uri`.
Alternatively you can pass this as a deployment property `app.*.management.metrics.export.influx.uri={influxdb-server-url}` when deploying a stream.
The https://micrometer.io/docs/registry/influx[Micrometer influx] documentation shows the full list of Spring Boot properties to configure sending metrics to Influx.

The following steps will start up Influx and Grafana.

Clone the Data Flow github repository for tagged release and change to the `influxdb/docker` folder:
[source,bash]
----
cd ./src/grafana/influxdb/docker
----

Start Influx and Grafna using `docker-compose`.
[source,bash]
----
docker-compose up -d --build
----

Check the containers have started:
[source,bash]
----
docker ps
CONTAINER ID        IMAGE               PORTS                    NAMES
1b7633c63ba1        docker_influxdb     0.0.0.0:8086->8086/tcp   influxdb
2f42e88f0606        docker_grafana      0.0.0.0:3000->3000/tcp   grafana
----

To validate the setup, you can login into those containers using the following commands.
[source,bash]
----
docker exec -it influxdb /bin/sh
docker exec -it grafana /bin/bash
----

and check the content of InfluxDB
[source,bash]
----
root:/# influx
> show databases
> use myinfluxdb
> show measurements
> select * from spring_integration_send limit 10
----

Grafana dashboard can be reached at http://localhost:3000 with credentials user: `admin`, password: `admin`.
It comes with 2 provisioned dashboards.

. Streams: http://localhost:3000/d/scdf-streams/streams?refresh=10s
. Applications: http://localhost:3000/d/scdf-applications/applications?refresh=10s

Start the Skipper server.  Then start the Data Flow server with the following properties:
[source,bash]
----
--spring.cloud.dataflow.applicationProperties.stream.management.metrics.export.influx.enabled=true
--spring.cloud.dataflow.applicationProperties.stream.management.metrics.export.influx.db=myinfluxdb
--spring.cloud.dataflow.applicationProperties.stream.management.metrics.export.influx.uri=http://localhost:8086
--spring.cloud.dataflow.grafana-info.url=http://localhost:3000
----

Now if you deploy a simple stream that uses Kafka, such as
[source,bash]
----
dataflow:>app import --uri http://bit.ly/Einstein-SR2-stream-applications-kafka-maven --force

dataflow:>stream create stream2 --definition "time --fixed-delay=10 --time-unit=MILLISECONDS | filter --expression=payload.contains('3') | log" --deploy
----

You should see dashboards similar to these.

image::{dataflow-asciidoc}/images/grafana-influxdb-scdf-streams-dashboard.png[Grafana InfluxDB Dashboard, scaledwidth="80%"]

You can destroy all containers with
[source,bash]
----
docker-compose down
----
