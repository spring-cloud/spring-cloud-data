[appendix]
[[howto]]
== "`How-to`" guides

This section provides answers to some common '`how do I do that...`' type of questions that often arise when using Spring Cloud Data Flow.

If you are having a specific problem that we do not cover here, you might want to check out https://stackoverflow.com/tags/spring-cloud-dataflow[stackoverflow.com] to see if someone has already provided an answer.
That is also a great place to ask new questions (please use the `spring-cloud-dataflow` tag).

We are also more than happy to extend this section. If you want to add a "`how-to`", you can send us a {github-code}[pull request].



=== Configure Maven Properties

You can set the maven properties such as local maven repository location, remote maven repositories, authentication credentials, and proxy server properties through command line properties when starting the Data Flow server.
Alternatively, you can set the properites using `SPRING_APPLICATION_JSON` environment property for the Data Flow server.

The remote maven repositories need to be configured explicitly if the apps are resolved using maven repository, except for a `local` Data Flow server.
The other Data Flow server implementations (that use maven resources for app artifacts resolution) have no default value for remote repositories.
The `local` server has `https://repo.spring.io/libs-snapshot` as the default remote repository.

To pass the properties as commandline options, run the server with a command similar to the following:

[source,bash]
----
$ java -jar <dataflow-server>.jar --maven.localRepository=mylocal
--maven.remote-repositories.repo1.url=https://repo1
--maven.remote-repositories.repo1.auth.username=repo1user
--maven.remote-repositories.repo1.auth.password=repo1pass
--maven.remote-repositories.repo2.url=https://repo2 --maven.proxy.host=proxyhost
--maven.proxy.port=9018 --maven.proxy.auth.username=proxyuser
--maven.proxy.auth.password=proxypass
----

You can also use the `SPRING_APPLICATION_JSON` environment property:

[source,json]
----
export SPRING_APPLICATION_JSON='{ "maven": { "local-repository": "local","remote-repositories": { "repo1": { "url": "https://repo1", "auth": { "username": "repo1user", "password": "repo1pass" } },
"repo2": { "url": "https://repo2" } }, "proxy": { "host": "proxyhost", "port": 9018, "auth": { "username": "proxyuser", "password": "proxypass" } } } }'
----

Here is the same content in nicely formatted JSON:

[source,json]
----
SPRING_APPLICATION_JSON='{
  "maven": {
    "local-repository": "local",
    "remote-repositories": {
      "repo1": {
        "url": "https://repo1",
        "auth": {
          "username": "repo1user",
          "password": "repo1pass"
        }
      },
      "repo2": {
        "url": "https://repo2"
      }
    },
    "proxy": {
      "host": "proxyhost",
      "port": 9018,
      "auth": {
        "username": "proxyuser",
        "password": "proxypass"
      }
    }
  }
}'
----

NOTE: Depending on the Spring Cloud Data Flow server implementation, you may have to pass the environment properties by using the platform specific environment-setting capabilities. For instance, in Cloud Foundry, you would pass them as `cf set-env SPRING_APPLICATION_JSON`.



=== Logging

Spring Cloud Data Flow is built upon several Spring projects, but ultimately the dataflow-server is a Spring Boot app, so the logging techniques that apply to any link:{spring-boot-docs-reference}/html/howto-logging.html#howto-logging[Spring Boot] application are applicable here as well.

While troubleshooting, the two primary areas where enabling the DEBUG logs could be useful are

* <<troubleshooting-deployment-logs>>
* <<troubleshooting-application-logs>>



[[troubleshooting-deployment-logs]]
==== Deployment Logs

Spring Cloud Data Flow builds upon link:https://github.com/spring-cloud/spring-cloud-deployer[Spring Cloud Deployer] SPI, and the platform-specific dataflow server uses the respective link:https://github.com/spring-cloud?utf8=%E2%9C%93&q=spring-cloud-deployer[SPI implementations].
Specifically, if we were to troubleshoot deployment specific issues, such as network errors, it would be useful to enable the DEBUG logs at the underlying deployer and the libraries used by it.

To enable DEBUG logs for the link:https://github.com/spring-cloud/spring-cloud-deployer-local[local-deployer], start the server as follows:

[source,bash]
----
$ java -jar <dataflow-server>.jar --logging.level.org.springframework.cloud.deployer.spi.local=DEBUG
----

(where `org.springframework.cloud.deployer.spi.local` is the global package for everything local-deployer
related.)

To enable DEBUG logs for the link:https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry[cloudfoundry-deployer], set the following environment variable and, after restaging the dataflow server, you can see more logs around request and response and see detailed stack traces for failures.
The cloudfoundry deployer uses link:https://github.com/cloudfoundry/cf-java-client[cf-java-client], so you must also enable DEBUG logs for this library.

[source,bash]
----
$ cf set-env dataflow-server JAVA_OPTS '-Dlogging.level.cloudfoundry-client=DEBUG'
$ cf restage dataflow-server
----

(where `cloudfoundry-client` is the global package for everything `cf-java-client` related.)

To review Reactor logs, which are used by the `cf-java-client`, then the following commad would be helpful:

[source,bash]
----
$ cf set-env dataflow-server JAVA_OPTS '-Dlogging.level.cloudfoundry-client=DEBUG -Dlogging.level.reactor.ipc.netty=DEBUG'
$ cf restage dataflow-server
----

(where `reactor.ipc.netty` is the global package for everything `reactor-netty` related.)

NOTE: Similar to the `local-deployer` and `cloudfoundry-deployer` options as discussed above, there are equivalent settings available for Kubernetes.
See the respective link:https://github.com/spring-cloud?utf8=%E2%9C%93&q=spring-cloud-deployer[SPI implementations] for more detail about the packages to configure for logging.



[[troubleshooting-application-logs]]
==== Application Logs

The streaming applications in Spring Cloud Data Flow are Spring Cloud Stream applications, which are in turn based on Spring Boot.  They can be independently setup with logging configurations.

For instance, if you must troubleshoot the `header` and `payload` specifics that are being passed around source, processor, and sink channels, you should deploy the stream with the following options:

[source,bash]
----
dataflow:>stream create foo --definition "http --logging.level.org.springframework.integration=DEBUG | transform --logging.level.org.springframework.integration=DEBUG | log --logging.level.org.springframework.integration=DEBUG" --deploy
----

(where `org.springframework.integration` is the global package for everything Spring Integration related,
which is responsible for messaging channels.)

These properties can also be specified with `deployment` properties when deploying the stream, as follows:

[source,bash]
----
dataflow:>stream deploy foo --properties "app.*.logging.level.org.springframework.integration=DEBUG"
----

==== Remote Debugging

The Data Flow local server lets you debug the deployed applications.
This is accomplished by enabling the remote debugging feature of the JVM through deployment properites, as shown in the following example:

[source,bash]
----
stream deploy --name mystream --properties "deployer.fooApp.local.debugPort=9999"
----

The preceding example starts the `fooApp` application in debug mode, allowing a remote debugger to be attached on port 9999.
By default, the application starts in a ’suspend’ mode and waits for the remote debug session to be attached (started). Otherwise, you can  provide an additional `debugSuspend` property with value `n`.

Also, when there is more then one instance of the application, the debug port for each instance is the value of `debugPort` + instanceId.

NOTE: Unlike other properties you must NOT use a wildcard for the application name, since each application must use a unique debug port.



==== Log Redirect

Given that each application is a separate process that maintains its own set of logs, accessing individual logs could be a bit inconvenient, especially in the early stages of development, when logs are accessed more often.
Since it is also a common pattern to rely on a local SCDF Server that deploys each application as a local JVM process, you can redirect the stdout and stdin from the deployed applications to the parent process.
Thus, with a local SCDF Server, the application logs appear in the logs of the running local SCDF Server.

Typically when you deploy the stream, you see something resembling the following in the server logs:

[source,bash]
----
017-06-28 09:50:16.372  INFO 41161 --- [nio-9393-exec-7] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId mystream.myapp instance 0.
   Logs will be in /var/folders/l2/63gcnd9d7g5dxxpjbgr0trpw0000gn/T/spring-cloud-dataflow-5939494818997196225/mystream-1498661416369/mystream.myapp
----

However, by setting `local.inheritLogging=true` as a deployment property, you can see the following:
[source,bash]
----
017-06-28 09:50:16.372  INFO 41161 --- [nio-9393-exec-7] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId mystream.myapp instance 0.
   Logs will be inherited.
----

After that, the application logs appear alongside the server logs, as shown in the following example:

[source,bash]
----
stream deploy --name mystream --properties "deployer.*.local.inheritLogging=true”
----

The preceding stream definition enables log redirection for each application in the stream.
The following stream definition enables log redirection for only the application named ‘my app’.

[source,bash]
----
stream deploy --name mystream --properties "deployer.myapp.local.inheritLogging=true”
----

NOTE: Log redirect is only supported with link:https://github.com/spring-cloud/spring-cloud-deployer-local[local-deployer].



[[faqs]]
=== Frequently Asked Questions
In this section, we review the frequently asked questions in Spring Cloud Data Flow.

==== Advanced SpEL Expressions

One of the powerful features of SpEL expressions is https://docs.spring.io/spring/docs/current/spring-framework-reference/html/expressions.html#expressions-ref-functions[functions].
If the appropriate libraries are in the classpath, Spring Integration provides the `jsonPath()` and `xpath()` https://docs.spring.io/spring-integration/reference/html/spel.html#spel-functions[SpEL-functions].
All the provided Spring Cloud Stream application starters are have with the `json-path` and `spring-integration-xml` in their uber-jar.
Consequently, we can use those SpEL-functions in Spring Cloud Data Flow streams whenever expressions are possible.
For example, we can transform JSON-aware `payload` from the HTTP request by using a few `jsonPath()` expressions, as follows:

[source,bash]
----
dataflow:>stream create jsonPathTransform --definition "http | transform --expression=#jsonPath(payload,'$.price') | log" --deploy
...
dataflow:> http post --target http://localhost:8080 --data {"symbol":"SCDF","price":72.04}
dataflow:> http post --target http://localhost:8080 --data {"symbol":"SCDF","price":72.06}
dataflow:> http post --target http://localhost:8080 --data {"symbol":"SCDF","price":72.08}
----

In the preceding example, we apply `jsonPath` for the incoming payload to extract only the `price` field value.
Similar syntax can be used with `splitter` or `filter` `expression` options.
Actually, any available SpEL-based option has access to the built-in SpEL-functions.
For example, we can extract some value from JSON data to calculate the `partitionKey` before sending output to the Binder, as follows:

[source,bash]
----
dataflow:>stream deploy foo --properties "deployer.transform.count=2,app.transform.producer.partitionKeyExpression=#jsonPath(payload,'$.symbol')"
----
The same syntax can be applied for `xpath()` SpEL-function when you deal with XML data.
Any other custom SpEL-function can also be used.
However, for this purpose, you should build a library with a `@Configuration` class containing an appropriate `SpelFunctionFactoryBean` `@Bean` definition.
The target Spring Cloud Stream application starter should be repackaged to supply such a custom extension with a built-in Spring Boot `@ComponentScan` mechanism or auto-configuration hook.



[[dataflow-jdbc-sink]]
==== How to Use JDBC-sink?

The JDBC-sink can be used to insert message payload data into a relational database table.
By default, it inserts the entire payload into a table named after the `jdbc.table-name` property.
If it is not set, by default, the application expects to use a table with a name of `messages`.
To alter this behavior, the JDBC sink accepts link:https://docs.spring.io/spring-cloud-stream-app-starters/docs/current/reference/html/spring-cloud-stream-modules-sinks.html#spring-cloud-stream-modules-jdbc-sink[several options] that you can pass by using the --param=value notation in the stream or change globally.
The JDBC sink has a `jdbc.initialize` property that, if set to `true`, results in the sink creating a table based on the specified configuration when it starts.
If that initialize property is `false`, which is the default, you must make sure that the table to use is already available.

A stream definition using `jdbc` sink and relying on all defaults with MySQL as the backing database looks
like the following example:

[source,bash]
----
dataflow:>stream create --name mydata --definition "time | jdbc --spring.datasource.url=jdbc:mysql://localhost:3306/test --spring.datasource.username=root --spring.datasource.password=root --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver" --deploy
----

In the preceding example, the system time is persisted in MySQL for every second.
For this to work, you must have the following table in the MySQL database:

[source,sql]
----
CREATE TABLE test.messages
(
  payload varchar(255)
);
----

[source,bash]
----
mysql> desc test.messages;
+---------+--------------+------+-----+---------+-------+
| Field   | Type         | Null | Key | Default | Extra |
+---------+--------------+------+-----+---------+-------+
| payload | varchar(255) | YES  |     | NULL    |       |
+---------+--------------+------+-----+---------+-------+
1 row in set (0.00 sec)
----

[source,bash]
----
mysql> select * from test.messages;
+-------------------+
| payload           |
+-------------------+
| 04/25/17 09:10:04 |
| 04/25/17 09:10:06 |
| 04/25/17 09:10:07 |
| 04/25/17 09:10:08 |
| 04/25/17 09:10:09 |
.............
.............
.............
----



[[dataflow-multiple-brokers]]
==== How to Use Multiple Message-binders?

For situations where the data is consumed and processed between two different message brokers, Spring Cloud Data Flow provides easy-to-override global configurations, an out-of-the-box link:https://github.com/spring-cloud-stream-app-starters/bridge[`bridge-processor`], and DSL primitives to build these type of topologies.

Assume that data is queueing up in RabbitMQ (for example, queue = `myRabbit`) and the requirement is to consume all the payloads and publish them to Apache Kafka (for example, topic = `myKafka`), as the destination for downstream processing.

In that case, you should follow the global application of <<streams.adoc#spring-cloud-dataflow-global-properties, configurations>> to define multiple binder configurations, as shown in the following configuration

[source,properties]
----
# Apache Kafka Global Configurations (that is, identified by "kafka1")
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.kafka1.type=kafka
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.kafka1.environment.spring.cloud.stream.kafka.binder.brokers=localhost:9092
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.kafka1.environment.spring.cloud.stream.kafka.binder.zkNodes=localhost:2181

# RabbitMQ Global Configurations (that is, identified by "rabbit1")
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.rabbit1.type=rabbit
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.rabbit1.environment.spring.rabbitmq.host=localhost
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.rabbit1.environment.spring.rabbitmq.port=5672
----

NOTE: In the preceding example, both message brokers are running locally and are reachable at `localhost` through their respective ports.

These properties can be supplied in a `.properties` file that is accessible to the server directly or through `config-server`, as follows:

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar --spring.config.additional-location=<PATH-TO-FILE>/foo.properties
----

Spring Cloud Data Flow internally uses `bridge-processor` to directly connect different named channel destinations.
Since we are publishing and subscribing from two different messaging systems, you must build the `bridge-processor` with both RabbitMQ and Apache Kafka binders in the classpath.
To do that, head over to https://start-scs.cfapps.io/ and select `Bridge Processor`, `Kafka binder starter`, and `Rabbit binder starter` as the dependencies and follow the patching procedure described in the link:{scs-app-starters-docs}/_introduction.html#customizing-binder[reference guide].
Specifically, for the `bridge-processor`, you must import the `BridgeProcessorConfiguration` provided by the starter.

Once you have the necessary adjustments, you can build the application. The following example registers the name of the
application as `multiBinderBridge`:

[source,bash]
----
dataflow:>app register --type processor --name multiBinderBridge --uri file:///<PATH-TO-FILE>/multipleBinderBridge-0.0.1-SNAPSHOT.jar
----

It is time to create a stream definition with the newly registered processor application, as follows:

[source,bash]
----
dataflow:>stream create fooRabbitToBarKafka --definition ":fooRabbit > multiBinderBridge --spring.cloud.stream.bindings.input.binder=rabbit1 --spring.cloud.stream.bindings.output.binder=kafka1 > :barKafka" --deploy
----

NOTE: Since we want to consume messages from RabbitMQ (identified by `rabbit1`) and then publish the payload to Apache Kafka (identified by `kafka1`), we are supplying them as the `input` and `output` channel settings respectively.

NOTE: The stream consumes events from the `myRabbit` queue in RabbitMQ and sends the data to the  `myKafka` topic in Apache Kafka.
